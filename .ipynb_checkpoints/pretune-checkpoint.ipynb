{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389fb1c4-f0da-4107-86ee-47b9c4aaa5d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: mace_run_train \\\n",
      "    --name \\\n",
      "    mace_T3_finetune_h200_cn10 \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/binary_elem_test.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/binary_elem_val.extxyz \\\n",
      "    --batch_size \\\n",
      "    2 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --valid_fraction \\\n",
      "    0.1 \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --forces_weight \\\n",
      "    50 \\\n",
      "    --energy_weight \\\n",
      "    75 \\\n",
      "    --lr \\\n",
      "    0.001 \\\n",
      "    --scheduler_patience \\\n",
      "    4 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    130 \\\n",
      "    --E0s \\\n",
      "    average \\\n",
      "    --seed \\\n",
      "    21 \\\n",
      "    --patience \\\n",
      "    5 \\\n",
      "    --restart_latest\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-14 03:33:08.799 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-08-14 03:33:08.799 INFO: MACE version: 0.3.14\n",
      "2025-08-14 03:33:09.350 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py:152: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-14 03:33:09.877 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-08-14 03:33:09.878 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-08-14 03:33:09.878 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-08-14 03:33:09.878 INFO: Using heads: ['Default']\n",
      "2025-08-14 03:33:09.878 INFO: Using the key specifications to parse data:\n",
      "2025-08-14 03:33:09.878 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-14 03:33:09.878 INFO: =============    Processing head Default     ===========\n",
      "2025-08-14 03:33:10.107 INFO: Training set 1/1 [energy: 1200, stress: 0, virials: 0, dipole components: 0, head: 1200, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1200, charges: 0]\n",
      "2025-08-14 03:33:10.110 INFO: Total Training set [energy: 1200, stress: 0, virials: 0, dipole components: 0, head: 1200, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1200, charges: 0]\n",
      "2025-08-14 03:33:10.430 INFO: Validation set 1/1 [energy: 1800, stress: 0, virials: 0, dipole components: 0, head: 1800, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1800, charges: 0]\n",
      "2025-08-14 03:33:10.435 INFO: Total Validation set [energy: 1800, stress: 0, virials: 0, dipole components: 0, head: 1800, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1800, charges: 0]\n",
      "2025-08-14 03:33:10.435 INFO: Total number of configurations: train=1200, valid=1800, tests=[],\n",
      "2025-08-14 03:33:10.437 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-08-14 03:33:10.437 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-08-14 03:33:10.437 INFO: Computing average Atomic Energies using least squares regression\n",
      "2025-08-14 03:33:10.442 INFO: Atomic Energies used (z: eV) for head Default: {3: 0.12686486466246572, 8: -0.5040028186029172, 40: 2.637859408854715, 57: 0.7164206613193861}\n",
      "2025-08-14 03:33:10.443 INFO: Processing datasets for head 'Default'\n",
      "2025-08-14 03:33:10.821 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-08-14 03:33:11.568 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-08-14 03:33:11.569 INFO: Combined validation datasets for Default\n",
      "2025-08-14 03:33:11.569 INFO: Head 'Default' training dataset size: 1200\n",
      "2025-08-14 03:33:11.569 INFO: Computing average number of neighbors\n",
      "2025-08-14 03:33:12.043 INFO: Average number of neighbors: 70.09490333919156\n",
      "2025-08-14 03:33:12.043 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-08-14 03:33:12.043 INFO: ===========MODEL DETAILS===========\n",
      "2025-08-14 03:33:12.530 INFO: Loading FOUNDATION model\n",
      "2025-08-14 03:33:12.530 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-08-14 03:33:12.531 INFO: Model configuration extracted from foundation model\n",
      "2025-08-14 03:33:12.531 INFO: Using weighted loss function for fine-tuning\n",
      "2025-08-14 03:33:12.531 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-08-14 03:33:12.531 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-08-14 03:33:12.531 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-08-14 03:33:12.531 INFO: Distance transform for radial basis functions: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/checkpoint.py:187: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-14 03:33:13.842 INFO: Total number of parameters: 894362\n",
      "2025-08-14 03:33:13.842 INFO: \n",
      "2025-08-14 03:33:13.842 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-08-14 03:33:13.842 INFO: Using ADAM as parameter optimizer\n",
      "2025-08-14 03:33:13.842 INFO: Batch size: 2\n",
      "2025-08-14 03:33:13.842 INFO: Number of gradient updates: 78000\n",
      "2025-08-14 03:33:13.842 INFO: Learning rate: 0.001, weight decay: 5e-07\n",
      "2025-08-14 03:33:13.843 INFO: WeightedEnergyForcesLoss(energy_weight=75.000, forces_weight=50.000)\n",
      "2025-08-14 03:33:13.851 WARNING: No SWA checkpoint found, while SWA is enabled. Compare the swa_start parameter and the latest checkpoint.\n",
      "2025-08-14 03:33:13.851 INFO: Loading checkpoint: ./checkpoints/mace_T3_finetune_h200_cn10_run-21_epoch-107.pt\n",
      "2025-08-14 03:33:14.381 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-08-14 03:33:14.381 INFO: \n",
      "2025-08-14 03:33:14.381 INFO: ===========TRAINING===========\n",
      "2025-08-14 03:33:14.381 INFO: Started training, reporting errors on validation set\n",
      "2025-08-14 03:33:14.381 INFO: Loss metrics on validation set\n"
     ]
    }
   ],
   "source": [
    "# Now universal MACE finetuning on T2\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    os.environ[\"PYTHONPATH\"] = \"/home/phanim/harshitrawat/mace/mace\"\n",
    "\n",
    "    cmd = [\n",
    "        \"mace_run_train\",\n",
    "        \"--name\",              \"mace_T3_finetune_h200_cn10\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "\n",
    "        \"--train_file\",        \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz\",\n",
    "        \"--train_file\",        \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/binary_elem_test.extxyz\",\n",
    "        \n",
    "        \"--valid_file\",        \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz\",\n",
    "        \"--valid_file\",        \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/binary_elem_val.extxyz\",\n",
    "\n",
    "        \"--batch_size\",        \"2\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "        \"--valid_fraction\",    \"0.1\",\n",
    "\n",
    "        \"--device\",            \"cuda\",\n",
    "\n",
    "        # === Loss function weights ===\n",
    "        \"--forces_weight\",     \"50\",         # Increased force weight to balance energy better\n",
    "        \"--energy_weight\",     \"75\",         # Reduced from 100 → avoid dominance + stabilize energy RMSE\n",
    "\n",
    "        # === Learning setup ===\n",
    "        \"--lr\",                \"0.001\",      # Explicit learning rate (0.0001 is too low → stagnation)\n",
    "        \"--scheduler_patience\",\"4\",          # Reduce LR if val loss doesn’t improve in 3 epochs\n",
    "        #\"--clip_grad\",         \"10\",        # Avoid exploding gradients — essential when energy_weight is high\n",
    "        #\"--weight_decay\",      \"1e-8\",       # Mild regularization to prevent overfitting\n",
    "\n",
    "        # === EMA helps smooth loss curve ===\n",
    "        #\"--ema_decay\",         \"0.999\",     # Smooths validation loss and helps final convergence\n",
    "\n",
    "        # === Domain + training settings ===\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"130\",\n",
    "        \"--E0s\",               \"average\",    # Still allowed — could optionally be replaced by manual E0s\n",
    "        \"--seed\",              \"21\",\n",
    "        \"--patience\",     \"5\",\n",
    "\n",
    "        \"--restart_latest\",                   # Resumes from checkpoint if available\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09d6dbf7-7ef9-4c30-b4a8-e11d17ff2703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[✓] Saved 3000 structures to /home/phanim/harshitrawat/summer/T1_T2_T3_data/binary_elem.extxyz\n"
     ]
    }
   ],
   "source": [
    "from ase.io import read, write\n",
    "import numpy as np\n",
    "import json, os\n",
    "\n",
    "LABEL_PATH = \"/home/phanim/harshitrawat/summer/md/mdlabels_it2.jsonl\"\n",
    "CIF_DIR = \"/home/phanim/harshitrawat/summer/md/mdcifs_it2\"\n",
    "OUT_XYZ = \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/binary_elem.extxyz\"\n",
    "\n",
    "# Load labels from JSONL using only basename for matching\n",
    "with open(LABEL_PATH) as f:\n",
    "    label_data = {\n",
    "        os.path.basename(entry[\"snapshot_file\"]): entry\n",
    "        for entry in map(json.loads, f)\n",
    "    }\n",
    "\n",
    "frames = []\n",
    "skipped = 0\n",
    "\n",
    "for fname in sorted(os.listdir(CIF_DIR)):\n",
    "    if not fname.endswith(\".cif\"):\n",
    "        continue\n",
    "    if fname not in label_data:\n",
    "        print(f\"[WARN] No label found for: {fname}\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    cif_path = os.path.join(CIF_DIR, fname)\n",
    "    try:\n",
    "        atoms = read(cif_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to read {fname}: {e}\")\n",
    "        continue\n",
    "\n",
    "    meta = label_data[fname]\n",
    "\n",
    "    # Set energy and forces\n",
    "    atoms.info[\"REF_energy\"] = meta[\"energy_eV\"]\n",
    "    atoms.arrays[\"REF_forces\"] = np.array(meta[\"forces_per_atom_eV_per_A\"])\n",
    "    atoms.info[\"snapshot_file\"] = fname  # traceability\n",
    "\n",
    "    frames.append(atoms)\n",
    "\n",
    "# Write the final .extxyz file\n",
    "write(OUT_XYZ, frames)\n",
    "print(f\"\\n[✓] Saved {len(frames)} structures to {OUT_XYZ}\")\n",
    "if skipped > 0:\n",
    "    print(f\"[i] Skipped {skipped} CIFs due to missing labels.\")\n",
    "\n",
    "# Optional: Check for label entries with no matching CIFs\n",
    "label_fnames = set(label_data.keys())\n",
    "cif_fnames = set(os.listdir(CIF_DIR))\n",
    "unused_labels = label_fnames - cif_fnames\n",
    "if unused_labels:\n",
    "    print(f\"[INFO] {len(unused_labels)} labels had no matching CIF file:\")\n",
    "    for name in sorted(unused_labels)[:5]:\n",
    "        print(\"  \", name)\n",
    "    if len(unused_labels) > 5:\n",
    "        print(\"  ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e787206b-c593-4d4f-8506-453f14cb2b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 47f7ac20-5f9d-577f-87ae-0d21207606bf__T360K__step1000.cif\n",
      "Energy: 10.264266014099121\n",
      "Forces shape: (3, 3)\n",
      "Formula: Li2O\n"
     ]
    }
   ],
   "source": [
    "# Preview a labeled structure\n",
    "from ase.io import read\n",
    "atoms = read(OUT_XYZ, index=0)\n",
    "print(\"File:\", atoms.info[\"snapshot_file\"])\n",
    "print(\"Energy:\", atoms.info[\"REF_energy\"])\n",
    "print(\"Forces shape:\", atoms.arrays[\"REF_forces\"].shape)\n",
    "print(\"Formula:\", atoms.get_chemical_formula())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee3af870-a023-4602-a3fd-09837d31f0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Saved 1800 to binary_elem_val.extxyz\n",
      "[✓] Saved 1200 to binary_elem_test.extxyz\n"
     ]
    }
   ],
   "source": [
    "from ase.io import read, write\n",
    "import numpy as np\n",
    "\n",
    "IN_FILE = \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/binary_elem.extxyz\"\n",
    "OUT_VAL = \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/binary_elem_val.extxyz\"\n",
    "OUT_TEST = \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/binary_elem_test.extxyz\"\n",
    "\n",
    "# Load all structures\n",
    "frames = read(IN_FILE, \":\")\n",
    "\n",
    "# Shuffle with fixed seed\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(frames))\n",
    "\n",
    "# 90% val, 10% test\n",
    "split_at = int(0.6 * len(frames))\n",
    "val_indices = indices[:split_at]\n",
    "test_indices = indices[split_at:]\n",
    "\n",
    "val_frames = [frames[i] for i in val_indices]\n",
    "test_frames = [frames[i] for i in test_indices]\n",
    "\n",
    "# Save them\n",
    "write(OUT_VAL, val_frames, format=\"extxyz\", write_info=True, write_results=True)\n",
    "write(OUT_TEST, test_frames, format=\"extxyz\", write_info=True, write_results=True)\n",
    "\n",
    "print(f\"[✓] Saved {len(val_frames)} to binary_elem_val.extxyz\")\n",
    "print(f\"[✓] Saved {len(test_frames)} to binary_elem_test.extxyz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892082aa-3ef5-4f56-b203-68910a0a1751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU mace_0.3.8)",
   "language": "python",
   "name": "mace_0.3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
