{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b7e138",
   "metadata": {},
   "source": [
    "# OOD Latent Space Analysis (Ensemble Uncertainty)\n",
    "\n",
    "This notebook analyzes the MACE latent embeddings to identify Out-of-Distribution (OOD) atomic environments using **Ensemble Uncertainty**.\n",
    "\n",
    "## Methodology\n",
    "We use an ensemble of 4 models ($w_1, w_2, w_3, w_4$) for each training set (T1, T2).\n",
    "\n",
    "### Metrics\n",
    "1.  **Ensemble Mean ($\\bar{z}$):** The average embedding vector. We use this \"denoised\" representation for Density Estimation (GMM).\n",
    "2.  **Ensemble Variance ($\\sigma^2$):** The average squared Euclidean distance from the mean. This is a direct measure of **Epistemic Uncertainty** (Model Disagreement).\n",
    "\n",
    "$$ \\sigma^2_i = \\frac{1}{4} \\sum_{k=1}^4 ||z_{i,k} - \\bar{z}_i||^2 $$\n",
    "\n",
    "### Hypothesis\n",
    "*   **In-Distribution (Reference):** Low Variance, High Density.\n",
    "*   **Out-of-Distribution (Test):** High Variance (Disagreement) OR Low Density.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde33e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ase.io import iread\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fed4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_DIR = \"/home/phanim/harshitrawat/summer/embeddings_results\"\n",
    "\n",
    "files = {\n",
    "    # MACE T1 (trained on T1)\n",
    "    \"T1_w1_on_T1\": \"embeddings_MACE_T1_w1_on_T1.extxyz\",\n",
    "    \"T1_w2_on_T1\": \"embeddings_MACE_T1_w2_on_T1.extxyz\",\n",
    "    \"T1_w3_on_T1\": \"embeddings_MACE_T1_w3_on_T1.extxyz\",\n",
    "    \"T1_w4_on_T1\": \"embeddings_MACE_T1_w4_on_T1.extxyz\",\n",
    "    \n",
    "    \"T1_w1_on_T2\": \"embeddings_MACE_T1_w1_on_T2.extxyz\",\n",
    "    \"T1_w2_on_T2\": \"embeddings_MACE_T1_w2_on_T2.extxyz\",\n",
    "    \"T1_w3_on_T2\": \"embeddings_MACE_T1_w3_on_T2.extxyz\",\n",
    "    \"T1_w4_on_T2\": \"embeddings_MACE_T1_w4_on_T2.extxyz\",\n",
    "\n",
    "    # MACE T2 (trained on T2)\n",
    "    \"T2_w1_on_T2\": \"embeddings_MACE_T2_w1_on_T2.extxyz\",\n",
    "    \"T2_w2_on_T2\": \"embeddings_MACE_T2_w2_on_T2.extxyz\",\n",
    "    \"T2_w3_on_T2\": \"embeddings_MACE_T2_w3_on_T2.extxyz\",\n",
    "    \"T2_w4_on_T2\": \"embeddings_MACE_T2_w4_on_T2.extxyz\",\n",
    "\n",
    "    \"T2_w1_on_T1\": \"embeddings_MACE_T2_w1_on_T1.extxyz\",\n",
    "    \"T2_w2_on_T1\": \"embeddings_MACE_T2_w2_on_T1.extxyz\",\n",
    "    \"T2_w3_on_T1\": \"embeddings_MACE_T2_w3_on_T1.extxyz\",\n",
    "    \"T2_w4_on_T1\": \"embeddings_MACE_T2_w4_on_T1.extxyz\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff928b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_ensemble(filenames, limit=None, sample_rate=1, print_interval=1000):\n",
    "    paths = [os.path.join(BASE_DIR, f) for f in filenames]\n",
    "    for p in paths:\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"Error: File not found: {p}\")\n",
    "            return None, None\n",
    "            \n",
    "    print(f\"Loading Ensemble: {filenames[0]} (and 3 others)...\")\n",
    "    \n",
    "    mean_list = []\n",
    "    var_list = []\n",
    "    \n",
    "    # Open 4 generators\n",
    "    gens = [iread(p, index=\":\") for p in paths]\n",
    "    \n",
    "    # Iterate simultaneously\n",
    "    try:\n",
    "        for i, atoms_tuple in enumerate(zip(*gens)):\n",
    "            if limit and i >= limit:\n",
    "                break\n",
    "            \n",
    "            if i % sample_rate == 0:\n",
    "                # Extract latents: Shape (4, N_atoms, 128)\n",
    "                latents = []\n",
    "                valid = True\n",
    "                for atoms in atoms_tuple:\n",
    "                    if \"mace_latent\" not in atoms.arrays:\n",
    "                        valid = False\n",
    "                        break\n",
    "                    latents.append(atoms.arrays[\"mace_latent\"])\n",
    "                \n",
    "                if not valid:\n",
    "                    continue\n",
    "                    \n",
    "                # Stack: (4, N, 128)\n",
    "                stack = np.stack(latents)\n",
    "                \n",
    "                # Compute Mean: (N, 128)\n",
    "                mean = np.mean(stack, axis=0)\n",
    "                \n",
    "                # Compute Variance (Scalar per atom): (N,)\n",
    "                # Sum of squared Euclidean distance from mean, averaged over ensemble\n",
    "                diff = stack - mean\n",
    "                norm_sq = np.sum(diff**2, axis=2) # (4, N)\n",
    "                variance = np.mean(norm_sq, axis=0) # (N,)\n",
    "                \n",
    "                mean_list.append(mean)\n",
    "                var_list.append(variance)\n",
    "            \n",
    "            if i > 0 and i % print_interval == 0:\n",
    "                print(f\"Processed {i} frames...\", end='\\r')\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading stream: {e}\")\n",
    "        return None, None\n",
    "        \n",
    "    print(f\"Done. Loaded {len(mean_list)} chunks.\")\n",
    "    \n",
    "    if not mean_list:\n",
    "        return np.array([]), np.array([])\n",
    "        \n",
    "    X_mean = np.concatenate(mean_list, axis=0)\n",
    "    X_var = np.concatenate(var_list, axis=0)\n",
    "    \n",
    "    print(f\"Total Atoms: {len(X_mean)}\")\n",
    "    return X_mean, X_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1104020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_gmm(ref_data, n_components=5):\n",
    "    print(\"Fitting GMM on Reference Data...\")\n",
    "    pca = PCA(n_components=16)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    ref_scaled = scaler.fit_transform(ref_data)\n",
    "    ref_pca = pca.fit_transform(ref_scaled)\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)\n",
    "    gmm.fit(ref_pca)\n",
    "    \n",
    "    return gmm, pca, scaler\n",
    "\n",
    "def score_gmm(model_tuple, data):\n",
    "    gmm, pca, scaler = model_tuple\n",
    "    data_scaled = scaler.transform(data)\n",
    "    data_pca = pca.transform(data_scaled)\n",
    "    # Negative log likelihood (Higher = OOD)\n",
    "    return -gmm.score_samples(data_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc05b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ANALYSIS 1: MACE_T1 Ensemble\n",
    "# Trained on T1. Reference = T1. Test = T2.\n",
    "\n",
    "SAMPLE_RATE = 10 # Adjust for memory\n",
    "\n",
    "files_ref = [files[f\"T1_w{i}_on_T1\"] for i in range(1, 5)]\n",
    "files_test = [files[f\"T1_w{i}_on_T2\"] for i in range(1, 5)]\n",
    "\n",
    "print(\"--- Loading REFERENCE (T1 on T1) ---\")\n",
    "X_mean_ref, X_var_ref = load_ensemble(files_ref, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "print(\"--- Loading TEST (T1 on T2) ---\")\n",
    "X_mean_test, X_var_test = load_ensemble(files_test, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "if X_mean_ref is not None:\n",
    "    # 1. Uncertainty (Variance) Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.kdeplot(X_var_ref, fill=True, label=\"Reference (T1)\", clip=(0, None))\n",
    "    sns.kdeplot(X_var_test, fill=True, label=\"Test (T2)\", clip=(0, None))\n",
    "    plt.title(\"Model Uncertainty (Ensemble Variance) - MACE T1\")\n",
    "    plt.xlabel(\"Variance (Uncertainty)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Density (GMM) Plot\n",
    "    models = fit_gmm(X_mean_ref)\n",
    "    gmm_scores_ref = score_gmm(models, X_mean_ref)\n",
    "    gmm_scores_test = score_gmm(models, X_mean_test)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.kdeplot(gmm_scores_ref, fill=True, label=\"Reference (T1)\")\n",
    "    sns.kdeplot(gmm_scores_test, fill=True, label=\"Test (T2)\")\n",
    "    plt.title(\"Density OOD Score (GMM NLL) - MACE T1\")\n",
    "    plt.xlabel(\"Negative Log-Likelihood\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ANALYSIS 2: MACE_T2 Ensemble\n",
    "# Trained on T2. Reference = T2. Test = T1.\n",
    "\n",
    "files_ref_2 = [files[f\"T2_w{i}_on_T2\"] for i in range(1, 5)]\n",
    "files_test_2 = [files[f\"T2_w{i}_on_T1\"] for i in range(1, 5)]\n",
    "\n",
    "print(\"--- Loading REFERENCE (T2 on T2) ---\")\n",
    "X_mean_ref_2, X_var_ref_2 = load_ensemble(files_ref_2, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "print(\"--- Loading TEST (T2 on T1) ---\")\n",
    "X_mean_test_2, X_var_test_2 = load_ensemble(files_test_2, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "if X_mean_ref_2 is not None:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.kdeplot(X_var_ref_2, fill=True, label=\"Reference (T2)\", clip=(0, None))\n",
    "    sns.kdeplot(X_var_test_2, fill=True, label=\"Test (T1)\", clip=(0, None))\n",
    "    plt.title(\"Model Uncertainty (Ensemble Variance) - MACE T2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    models_2 = fit_gmm(X_mean_ref_2)\n",
    "    scores_ref_2 = score_gmm(models_2, X_mean_ref_2)\n",
    "    scores_test_2 = score_gmm(models_2, X_mean_test_2)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.kdeplot(scores_ref_2, fill=True, label=\"Reference (T2)\")\n",
    "    sns.kdeplot(scores_test_2, fill=True, label=\"Test (T1)\")\n",
    "    plt.title(\"Density OOD Score (GMM NLL) - MACE T2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_diversity(mean_embeddings, variances, top_n=1000, n_clusters=5):\n",
    "    print(f\"\\n--- Diversity Analysis (Top {top_n} Uncertain Atoms) ---\")\n",
    "    \n",
    "    # 1. Select High Variance Atoms\n",
    "    # Note: This is per-atom. For structures, we'd need structure indices. \n",
    "    # Here we analyze which *types* of atomic environments are uncertain.\n",
    "    indices = np.argsort(variances)[-top_n:]\n",
    "    X_ood = mean_embeddings[indices]\n",
    "    \n",
    "    # 2. Cluster them\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_ood)\n",
    "    \n",
    "    # 3. Visualize with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_ood)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', alpha=0.6)\n",
    "    plt.colorbar(scatter, label=\"Cluster ID\")\n",
    "    plt.title(f\"Clustering of Top {top_n} Uncertain Environments\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Cluster Counts:\")\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    for u, c in zip(unique, counts):\n",
    "        print(f\"Cluster {u}: {c} atoms\")\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Run on T2 Test Data\n",
    "if 'X_mean_test' in locals() and X_mean_test is not None:\n",
    "    analyze_diversity(X_mean_test, X_var_test, top_n=2000, n_clusters=5)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
