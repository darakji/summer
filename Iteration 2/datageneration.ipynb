{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523487fa-d8a9-48de-b99b-95187ba9b347",
   "metadata": {},
   "source": [
    "First, minimal, single‑purpose script that just pulls bulk binary structures from Materials Project and saves only CIFs (+ a tiny Excel index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd264ac-817b-433b-b652-a3dba20b41d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li-O: 3 structures\n",
      "Li-La: 0 structures\n",
      "Li-Zr: 0 structures\n",
      "La-O: 1 structures\n",
      "Zr-O: 3 structures\n",
      "La-Zr: 0 structures\n",
      "\n",
      "Saved 7 CIFs to /home/phanim/harshitrawat/summer/binaries_bulk\n",
      "Index: /home/phanim/harshitrawat/summer/binaries_bulk/index.xlsx\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pymatgen.ext.matproj import MPRester\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "from ase.io import write\n",
    "\n",
    "# ====== CONFIG ======\n",
    "API_KEY = \"j3J85pX4nLw6asHG9E2lbbCHEKDKgrjc\"  # put your MP API key here\n",
    "OUT_ROOT = Path(\"/home/phanim/harshitrawat/summer/binaries_bulk\")\n",
    "PAIRS = [(\"Li\",\"O\"), (\"Li\",\"La\"), (\"Li\",\"Zr\"), (\"La\",\"O\"), (\"Zr\",\"O\"), (\"La\",\"Zr\")]\n",
    "E_ABOVE_HULL_MAX = 0.03\n",
    "MAX_DOCS_PER_PAIR = None\n",
    "SAVE_CONVENTIONAL = False\n",
    "INDEX_XLSX = OUT_ROOT / \"index.xlsx\"\n",
    "# ====================\n",
    "\n",
    "def safe(s): return s.replace(\" \", \"\")\n",
    "\n",
    "def main():\n",
    "    OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    rows = []\n",
    "    with MPRester(API_KEY) as mpr:\n",
    "        for a,b in PAIRS:\n",
    "            pair = f\"{a}-{b}\"\n",
    "            pair_dir = OUT_ROOT / f\"pair_{a}_{b}\" / \"cifs\"\n",
    "            pair_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # replace the summary_search call with this\n",
    "            docs = mpr.summary_search(\n",
    "                chemsys=pair,\n",
    "                nelements=2,\n",
    "                is_stable=True,  # optional prefilter; keeps results small\n",
    "                _fields=[\n",
    "                    \"material_id\", \"formula_pretty\", \"chemsys\", \"is_stable\",\n",
    "                    \"energy_above_hull\", \"structure\"\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # client-side filter on energy_above_hull\n",
    "            docs = [d for d in docs\n",
    "                    if (d.get(\"energy_above_hull\") is not None)\n",
    "                    and (float(d[\"energy_above_hull\"]) <= E_ABOVE_HULL_MAX)]\n",
    "\n",
    "            if MAX_DOCS_PER_PAIR:\n",
    "                docs = docs[:MAX_DOCS_PER_PAIR]\n",
    "\n",
    "            print(f\"{pair}: {len(docs)} structures\")\n",
    "            for d in docs:\n",
    "                mpid = d[\"material_id\"]\n",
    "                formula = safe(d[\"formula_pretty\"])\n",
    "                pm = d[\"structure\"]\n",
    "                if SAVE_CONVENTIONAL:\n",
    "                    try:\n",
    "                        pm = pm.get_conventional_standard_structure()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                cif_path = pair_dir / f\"mpid-{mpid}_{formula}__bulk.cif\"\n",
    "                write(cif_path, AseAtomsAdaptor.get_atoms(pm), format=\"cif\")\n",
    "\n",
    "                rows.append({\n",
    "                    \"mpid\": mpid,\n",
    "                    \"formula\": formula,\n",
    "                    \"chemsys\": d[\"chemsys\"],\n",
    "                    \"is_stable\": bool(d[\"is_stable\"]),\n",
    "                    \"energy_above_hull_eV_per_atom\": float(d.get(\"energy_above_hull\", 0.0) or 0.0),\n",
    "                    \"n_atoms\": len(pm),\n",
    "                    \"cif\": str(cif_path),\n",
    "                })\n",
    "\n",
    "    if rows:\n",
    "        pd.DataFrame(rows).sort_values([\"chemsys\",\"formula\",\"mpid\"]).to_excel(INDEX_XLSX, index=False)\n",
    "        print(f\"\\nSaved {len(rows)} CIFs to {OUT_ROOT}\\nIndex: {INDEX_XLSX}\")\n",
    "    else:\n",
    "        print(\"No structures found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5a0cbc3-5072-4ef6-bd74-e2a0c1f4fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we download elemental bulk structures from MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46869030-7e30-4fd0-beb7-3ec2dae8d558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li: 1 structure(s)\n",
      "La: 1 structure(s)\n",
      "Zr: 1 structure(s)\n",
      "O: 1 structure(s)\n",
      "\n",
      "Saved 4 elemental bulk CIFs to /home/phanim/harshitrawat/summer/elementals_bulk\n",
      "Index: /home/phanim/harshitrawat/summer/elementals_bulk/index.xlsx\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pymatgen.ext.matproj import MPRester\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "from ase.io import write\n",
    "\n",
    "# ====== CONFIG ======\n",
    "API_KEY = \"j3J85pX4nLw6asHG9E2lbbCHEKDKgrjc\"  # Put your MP API key here\n",
    "OUT_ROOT = Path(\"/home/phanim/harshitrawat/summer/elementals_bulk\")\n",
    "ELEMENTS = [\"Li\", \"La\", \"Zr\", \"O\"]\n",
    "MAX_DOCS_PER_EL = 1       # Usually 1 stable phase per element is enough\n",
    "SAVE_CONVENTIONAL = False\n",
    "INDEX_XLSX = OUT_ROOT / \"index.xlsx\"\n",
    "# ====================\n",
    "\n",
    "def safe(s): return s.replace(\" \", \"\")\n",
    "\n",
    "def main():\n",
    "    OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    rows = []\n",
    "\n",
    "    with MPRester(API_KEY) as mpr:\n",
    "        for el in ELEMENTS:\n",
    "            el_dir = OUT_ROOT / f\"element_{el}\" / \"cifs\"\n",
    "            el_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            docs = mpr.summary_search(\n",
    "                chemsys=el,\n",
    "                nelements=1,\n",
    "                is_stable=True,\n",
    "                _fields=[\"material_id\",\"formula_pretty\",\"chemsys\",\"is_stable\",\"energy_above_hull\",\"structure\"],\n",
    "            )\n",
    "\n",
    "            # Sort by energy_above_hull and take top N\n",
    "            docs = sorted(docs, key=lambda x: float(x.get(\"energy_above_hull\", 0.0) or 0.0))\n",
    "            if MAX_DOCS_PER_EL:\n",
    "                docs = docs[:MAX_DOCS_PER_EL]\n",
    "\n",
    "            print(f\"{el}: {len(docs)} structure(s)\")\n",
    "\n",
    "            for d in docs:\n",
    "                mpid = d[\"material_id\"]\n",
    "                formula = safe(d[\"formula_pretty\"])\n",
    "                pm = d[\"structure\"]\n",
    "                if SAVE_CONVENTIONAL:\n",
    "                    try:\n",
    "                        pm = pm.get_conventional_standard_structure()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                cif_path = el_dir / f\"mpid-{mpid}_{formula}__bulk.cif\"\n",
    "                write(cif_path, AseAtomsAdaptor.get_atoms(pm), format=\"cif\")\n",
    "\n",
    "                rows.append({\n",
    "                    \"element\": el,\n",
    "                    \"mpid\": mpid,\n",
    "                    \"formula\": formula,\n",
    "                    \"chemsys\": d[\"chemsys\"],\n",
    "                    \"is_stable\": bool(d[\"is_stable\"]),\n",
    "                    \"energy_above_hull_eV_per_atom\": float(d.get(\"energy_above_hull\", 0.0) or 0.0),\n",
    "                    \"n_atoms\": len(pm),\n",
    "                    \"cif\": str(cif_path),\n",
    "                })\n",
    "\n",
    "    if rows:\n",
    "        pd.DataFrame(rows).sort_values([\"element\",\"mpid\"]).to_excel(INDEX_XLSX, index=False)\n",
    "        print(f\"\\nSaved {len(rows)} elemental bulk CIFs to {OUT_ROOT}\\nIndex: {INDEX_XLSX}\")\n",
    "    else:\n",
    "        print(\"No structures found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6203dd4",
   "metadata": {},
   "source": [
    "# Now we do MD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c556d2-ba9e-4d8b-a05a-31c791344e48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds: 11 | snaps/temp ≈ 160 | ≈320 per seed | target=3000\n",
      "CHGNet v0.3.0 initialized with 412,525 parameters\n",
      "CHGNet will run on cuda\n",
      "[1/11] MD on /home/phanim/harshitrawat/summer/elementals_bulk/element_O/cifs/mpid-mp-12957_O2__bulk.cif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_190337/3389124539.py:167: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"created_at_iso\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - mpid-mp-12957_O2__bulk.cif @ 360K: saved 160 so far (quota left 2840)\n",
      "  - mpid-mp-12957_O2__bulk.cif @ 480K: saved 320 so far (quota left 2680)\n",
      "[2/11] MD on /home/phanim/harshitrawat/summer/elementals_bulk/element_Li/cifs/mpid-mp-1018134_Li__bulk.cif\n",
      "  - mpid-mp-1018134_Li__bulk.cif @ 360K: saved 160 so far (quota left 2520)\n",
      "  - mpid-mp-1018134_Li__bulk.cif @ 480K: saved 320 so far (quota left 2360)\n",
      "[3/11] MD on /home/phanim/harshitrawat/summer/elementals_bulk/element_La/cifs/mpid-mp-26_La__bulk.cif\n",
      "  - mpid-mp-26_La__bulk.cif @ 360K: saved 160 so far (quota left 2200)\n",
      "  - mpid-mp-26_La__bulk.cif @ 480K: saved 320 so far (quota left 2040)\n",
      "[4/11] MD on /home/phanim/harshitrawat/summer/binaries_bulk/pair_Li_O/cifs/mpid-mp-841_Li2O2__bulk.cif\n",
      "  - mpid-mp-841_Li2O2__bulk.cif @ 360K: saved 160 so far (quota left 1880)\n",
      "  - mpid-mp-841_Li2O2__bulk.cif @ 480K: saved 320 so far (quota left 1720)\n",
      "[5/11] MD on /home/phanim/harshitrawat/summer/binaries_bulk/pair_Zr_O/cifs/mpid-mp-14024_Zr3O__bulk.cif\n",
      "  - mpid-mp-14024_Zr3O__bulk.cif @ 360K: saved 160 so far (quota left 1560)\n",
      "  - mpid-mp-14024_Zr3O__bulk.cif @ 480K: saved 320 so far (quota left 1400)\n",
      "[6/11] MD on /home/phanim/harshitrawat/summer/elementals_bulk/element_Zr/cifs/mpid-mp-131_Zr__bulk.cif\n",
      "  - mpid-mp-131_Zr__bulk.cif @ 360K: saved 160 so far (quota left 1240)\n",
      "  - mpid-mp-131_Zr__bulk.cif @ 480K: saved 320 so far (quota left 1080)\n",
      "[7/11] MD on /home/phanim/harshitrawat/summer/binaries_bulk/pair_Li_O/cifs/mpid-mp-1960_Li2O__bulk.cif\n",
      "  - mpid-mp-1960_Li2O__bulk.cif @ 360K: saved 160 so far (quota left 920)\n",
      "  - mpid-mp-1960_Li2O__bulk.cif @ 480K: saved 320 so far (quota left 760)\n",
      "[8/11] MD on /home/phanim/harshitrawat/summer/binaries_bulk/pair_Zr_O/cifs/mpid-mp-2858_ZrO2__bulk.cif\n",
      "  - mpid-mp-2858_ZrO2__bulk.cif @ 360K: saved 160 so far (quota left 600)\n",
      "  - mpid-mp-2858_ZrO2__bulk.cif @ 480K: saved 320 so far (quota left 440)\n",
      "[9/11] MD on /home/phanim/harshitrawat/summer/binaries_bulk/pair_Li_O/cifs/mpid-mp-1235059_LiO8__bulk.cif\n",
      "  - mpid-mp-1235059_LiO8__bulk.cif @ 360K: saved 160 so far (quota left 280)\n",
      "  - mpid-mp-1235059_LiO8__bulk.cif @ 480K: saved 320 so far (quota left 120)\n",
      "[10/11] MD on /home/phanim/harshitrawat/summer/binaries_bulk/pair_Zr_O/cifs/mpid-mp-1215377_Zr4O__bulk.cif\n",
      "  - mpid-mp-1215377_Zr4O__bulk.cif @ 360K: saved 120 so far (quota left 0)\n",
      "\n",
      "MD finished in 246.6 min. Saved 3000 new snapshots (total 3000).\n",
      "Traj dir:   /home/phanim/harshitrawat/summer/md/mdtraj_it2\n",
      "Snapshots:  /home/phanim/harshitrawat/summer/md/mdcifs_it2\n",
      "Metadata:   /home/phanim/harshitrawat/summer/md/mdinfo_it2.xlsx\n",
      "JSONL:      /home/phanim/harshitrawat/summer/md/mdinfo_it2.jsonl\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# run_md_binaries_and_elements_it2.py\n",
    "import os, sys, json, time, re, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ase.io import read, write, Trajectory\n",
    "from ase.md.langevin import Langevin\n",
    "from ase.md.velocitydistribution import MaxwellBoltzmannDistribution, Stationary\n",
    "from ase import units\n",
    "from math import isfinite\n",
    "\n",
    "# ---- Try both CHGNet calculator import paths ----\n",
    "try:\n",
    "    from chgnet.model.dynamics import CHGNetCalculator\n",
    "except Exception:\n",
    "    from chgnet.model.ase import CHGNetCalculator  # older versions\n",
    "\n",
    "# ======= CONFIG =======\n",
    "BINARIES_ROOT   = Path(\"/home/phanim/harshitrawat/summer/binaries_bulk\")\n",
    "ELEMENTALS_ROOT = Path(\"/home/phanim/harshitrawat/summer/elementals_bulk\")\n",
    "\n",
    "OUT_MD_ROOT   = Path(\"/home/phanim/harshitrawat/summer/md\")\n",
    "MD_TRAJ_DIR   = OUT_MD_ROOT / \"mdtraj_it2\"\n",
    "MD_CIFS_DIR   = OUT_MD_ROOT / \"mdcifs_it2\"\n",
    "MD_META_JSONL = OUT_MD_ROOT / \"mdinfo_it2.jsonl\"\n",
    "MD_META_XLSX  = OUT_MD_ROOT / \"mdinfo_it2.xlsx\"\n",
    "\n",
    "TEMPS_K = [360, 480]\n",
    "\n",
    "STEPS = 1800             # total steps per temperature\n",
    "DT_FS = 1.0              # timestep (fs)\n",
    "WARMUP_STEPS = 200       # don't sample during warmup\n",
    "SNAPSHOT_STRIDE = 10     # keep every Nth step after warmup\n",
    "FRICTION_FS_INV = 0.02   # Langevin gamma (fs^-1)\n",
    "\n",
    "TARGET_SNAPSHOTS = 3000  # total snapshots target (across temps & seeds)\n",
    "RNG_SEED = 123\n",
    "DEVICE = \"cuda\"          # \"cuda\" or \"cpu\"\n",
    "# ======================\n",
    "\n",
    "random.seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "OUT_MD_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "MD_TRAJ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MD_CIFS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "UUID_RE = re.compile(r\"([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})\", re.I)\n",
    "\n",
    "def find_seed_cifs():\n",
    "    bins = sorted(BINARIES_ROOT.glob(\"pair_*/*/*.cif\"))\n",
    "    elems = sorted(ELEMENTALS_ROOT.glob(\"element_*/*/*.cif\"))\n",
    "    seeds = bins + elems\n",
    "    if not seeds:\n",
    "        print(\"No CIFs found. Check BINARIES_ROOT/ELEMENTALS_ROOT.\")\n",
    "        sys.exit(2)\n",
    "    return seeds\n",
    "\n",
    "def extract_uuid(path_str):\n",
    "    m = UUID_RE.search(path_str)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    import uuid\n",
    "    return str(uuid.uuid5(uuid.NAMESPACE_URL, f\"seed|{Path(path_str).resolve()}\"))\n",
    "\n",
    "def estimate_snaps_per_temp(steps, warmup, stride):\n",
    "    usable = max(0, steps - warmup)\n",
    "    return usable // stride\n",
    "\n",
    "def init_calc():\n",
    "    calc = CHGNetCalculator(use_device=DEVICE, model_name=None)\n",
    "    try:\n",
    "        dev = getattr(calc.model, \"device\", None)\n",
    "        if dev is not None:\n",
    "            print(f\"[CHGNet] device = {dev}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return calc\n",
    "\n",
    "def safe_energy_forces(atoms):\n",
    "    e = float(atoms.get_potential_energy())\n",
    "    f = atoms.get_forces()\n",
    "    if not isfinite(e) or np.isnan(f).any() or np.isinf(f).any():\n",
    "        raise RuntimeError(\"Non-finite energy/forces encountered\")\n",
    "    return e, f\n",
    "\n",
    "def run_md_on_seed(cif_path, calc, temps, steps, dt_fs, warmup, stride, meta_rows, remaining_quota):\n",
    "    \"\"\"Runs MD for this seed across temps; returns snapshots_saved.\"\"\"\n",
    "    uid = extract_uuid(str(cif_path))\n",
    "    atoms0 = read(cif_path)\n",
    "    atoms0.calc = calc\n",
    "\n",
    "    saved = 0\n",
    "    for T in temps:\n",
    "        if remaining_quota <= 0:\n",
    "            break\n",
    "\n",
    "        A = atoms0.copy()\n",
    "        A.calc = calc  # IMPORTANT: calculator isn't copied by ASE\n",
    "\n",
    "        # fresh velocities per temperature (Kelvin), remove drift\n",
    "        MaxwellBoltzmannDistribution(A, T)\n",
    "        Stationary(A)\n",
    "\n",
    "        # reproducible RNG per (uuid, T)\n",
    "        rng_seed = abs(hash((uid, int(T), RNG_SEED))) % (1 << 32)\n",
    "        rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "        # ASE Langevin signature: (atoms, timestep, temperature_K, friction, ...)\n",
    "        try:\n",
    "            dyn = Langevin(A, dt_fs * units.fs, T, FRICTION_FS_INV / units.fs, rng=rng)\n",
    "        except TypeError:\n",
    "            # older ASE without rng=\n",
    "            dyn = Langevin(A, dt_fs * units.fs, T, FRICTION_FS_INV / units.fs)\n",
    "\n",
    "        traj_path = MD_TRAJ_DIR / f\"{uid}__T{int(T)}K.traj\"\n",
    "        steps_done = 0\n",
    "\n",
    "        with Trajectory(traj_path, \"w\", A, properties=[\"energy\", \"forces\"]) as traj:\n",
    "            for _ in range(steps):\n",
    "                dyn.run(1)\n",
    "                steps_done += 1\n",
    "\n",
    "                # throttle traj I/O\n",
    "                if steps_done > warmup and ((steps_done - warmup) % stride == 0):\n",
    "                    traj.write(A)\n",
    "                elif steps_done == warmup:\n",
    "                    traj.write(A)\n",
    "\n",
    "                if steps_done <= warmup or (steps_done - warmup) % stride != 0:\n",
    "                    continue\n",
    "                if remaining_quota <= 0:\n",
    "                    break\n",
    "\n",
    "                # Save snapshot + metadata\n",
    "                try:\n",
    "                    e, f = safe_energy_forces(A)\n",
    "                except Exception as ex:\n",
    "                    print(f\"    !! NaN detected @ {T}K step {steps_done}: {ex}. Skipping this temperature.\")\n",
    "                    break\n",
    "\n",
    "                snap_name = f\"{uid}__T{int(T)}K__step{steps_done}.cif\"\n",
    "                snap_path = MD_CIFS_DIR / snap_name\n",
    "                write(snap_path, A, format=\"cif\")\n",
    "\n",
    "                fmag = np.linalg.norm(f, axis=1)\n",
    "                fmax = float(fmag.max())\n",
    "                frms = float(np.sqrt((f**2).sum() / f.shape[0]))\n",
    "\n",
    "                row = {\n",
    "                    \"uuid\": uid,\n",
    "                    \"source_file\": str(cif_path),\n",
    "                    \"snapshot_file\": str(snap_path),\n",
    "                    \"traj_file\": str(traj_path),\n",
    "                    \"temperature_K\": int(T),\n",
    "                    \"step\": int(steps_done),\n",
    "                    \"dt_fs\": float(dt_fs),\n",
    "                    \"thermostat\": \"Langevin\",\n",
    "                    \"friction_fs_inv\": float(FRICTION_FS_INV),\n",
    "                    \"energy_eV\": e,\n",
    "                    \"force_rms_eV_per_A\": frms,\n",
    "                    \"fmax_eV_per_A\": fmax,\n",
    "                    \"n_atoms\": len(A),\n",
    "                    \"created_at_iso\": datetime.utcnow().isoformat() + \"Z\",\n",
    "                    # keep full forces in JSONL (Excel will omit)\n",
    "                    \"forces_eV_per_A\": f.tolist(),\n",
    "                }\n",
    "                meta_rows.append(row)\n",
    "                saved += 1\n",
    "                remaining_quota -= 1\n",
    "\n",
    "        print(f\"  - {Path(cif_path).name} @ {T}K: saved {saved} so far (quota left {remaining_quota})\")\n",
    "        if remaining_quota <= 0:\n",
    "            break\n",
    "\n",
    "    return saved\n",
    "\n",
    "def main():\n",
    "    seeds = find_seed_cifs()\n",
    "    random.shuffle(seeds)\n",
    "\n",
    "    snaps_per_temp = estimate_snaps_per_temp(STEPS, WARMUP_STEPS, SNAPSHOT_STRIDE)\n",
    "    est_per_seed = snaps_per_temp * len(TEMPS_K)\n",
    "    print(f\"Seeds: {len(seeds)} | snaps/temp ≈ {snaps_per_temp} | ≈{est_per_seed} per seed | target={TARGET_SNAPSHOTS}\")\n",
    "\n",
    "    # resume: count existing JSONL rows if present\n",
    "    existing_rows = 0\n",
    "    if MD_META_JSONL.exists():\n",
    "        try:\n",
    "            with open(MD_META_JSONL) as f:\n",
    "                existing_rows = sum(1 for _ in f)\n",
    "            if existing_rows >= TARGET_SNAPSHOTS:\n",
    "                print(f\"Found {existing_rows} existing rows in JSONL; target met. Exiting.\")\n",
    "                return\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    calc = init_calc()\n",
    "    meta_rows = []\n",
    "    remaining = TARGET_SNAPSHOTS - existing_rows\n",
    "\n",
    "    t0 = time.time()\n",
    "    for i, cif in enumerate(seeds, 1):\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "        print(f\"[{i}/{len(seeds)}] MD on {cif}\")\n",
    "        try:\n",
    "            saved = run_md_on_seed(\n",
    "                cif_path=cif,\n",
    "                calc=calc,\n",
    "                temps=TEMPS_K,\n",
    "                steps=STEPS,\n",
    "                dt_fs=DT_FS,\n",
    "                warmup=WARMUP_STEPS,\n",
    "                stride=SNAPSHOT_STRIDE,\n",
    "                meta_rows=meta_rows,\n",
    "                remaining_quota=remaining,\n",
    "            )\n",
    "            remaining -= saved\n",
    "        except Exception as e:\n",
    "            print(f\"!! Failed {cif}: {e}\")\n",
    "\n",
    "    # Append to JSONL (resume-friendly)\n",
    "    if meta_rows:\n",
    "        with open(MD_META_JSONL, \"a\") as f:\n",
    "            for r in meta_rows:\n",
    "                f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "        # Excel: drop giant forces column\n",
    "        df = pd.DataFrame(meta_rows)\n",
    "        if \"forces_eV_per_A\" in df.columns:\n",
    "            df = df.drop(columns=[\"forces_eV_per_A\"])\n",
    "        df.sort_values([\"temperature_K\",\"uuid\",\"step\"], inplace=True)\n",
    "        df.to_excel(MD_META_XLSX, index=False)\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    total = existing_rows + len(meta_rows)\n",
    "    print(f\"\\nMD finished in {dt/60:.1f} min. Saved {len(meta_rows)} new snapshots (total {total}).\")\n",
    "    print(f\"Traj dir:   {MD_TRAJ_DIR}\")\n",
    "    print(f\"Snapshots:  {MD_CIFS_DIR}\")\n",
    "    print(f\"Metadata:   {MD_META_XLSX}\")\n",
    "    print(f\"JSONL:      {MD_META_JSONL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18c108cd-09df-4c4f-b315-f893046c74d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHGNet v0.3.0 initialized with 412,525 parameters\n",
      "CHGNet will run on cuda\n",
      "[CHGNet] device = unknown\n",
      "Found 3000 snapshots → labeling to /home/phanim/harshitrawat/summer/md/labels_fresh_it2/run_20250811_121909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_190337/1542158822.py:106: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"created_at_iso\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/3000] 47f7ac20-5f9d-577f-87ae-0d21207606bf__T360K__step390.cif (n=3, E=-7.346784 eV, fmax=101.292)\n",
      "[200/3000] 47f7ac20-5f9d-577f-87ae-0d21207606bf__T480K__step1390.cif (n=3, E=-6.970852 eV, fmax=16.672)\n",
      "[300/3000] 47f7ac20-5f9d-577f-87ae-0d21207606bf__T480K__step790.cif (n=3, E=-13.155951 eV, fmax=1.973)\n",
      "[400/3000] 4bdcb7da-d4f6-58d7-9481-3d71d389469d__T360K__step1790.cif (n=8, E=5.275574 eV, fmax=86.150)\n",
      "[500/3000] 4bdcb7da-d4f6-58d7-9481-3d71d389469d__T480K__step1190.cif (n=8, E=-24.523636 eV, fmax=26.406)\n",
      "[600/3000] 4bdcb7da-d4f6-58d7-9481-3d71d389469d__T480K__step590.cif (n=8, E=-24.549477 eV, fmax=39.591)\n",
      "[700/3000] 68d75590-319e-5978-aab1-dd84a8cea44d__T360K__step1590.cif (n=8, E=5.874779 eV, fmax=10.296)\n",
      "[800/3000] 68d75590-319e-5978-aab1-dd84a8cea44d__T360K__step990.cif (n=8, E=36.176064 eV, fmax=34.752)\n",
      "[900/3000] 68d75590-319e-5978-aab1-dd84a8cea44d__T480K__step390.cif (n=8, E=33.053322 eV, fmax=39.865)\n",
      "[1000/3000] 849d67b4-45cb-55e3-b935-93e0e31cb2bf__T360K__step1390.cif (n=9, E=-35.273280 eV, fmax=7.002)\n",
      "[1100/3000] 849d67b4-45cb-55e3-b935-93e0e31cb2bf__T360K__step790.cif (n=9, E=-20.841850 eV, fmax=48.698)\n",
      "[1200/3000] 849d67b4-45cb-55e3-b935-93e0e31cb2bf__T480K__step1790.cif (n=9, E=16.296171 eV, fmax=26.878)\n",
      "[1300/3000] 8e295137-042d-5319-86f1-4fcbabc1af8d__T360K__step1190.cif (n=12, E=-3.585400 eV, fmax=203.022)\n",
      "[1400/3000] 8e295137-042d-5319-86f1-4fcbabc1af8d__T360K__step590.cif (n=12, E=-22.965494 eV, fmax=154.554)\n",
      "[1500/3000] 8e295137-042d-5319-86f1-4fcbabc1af8d__T480K__step1590.cif (n=12, E=37.252396 eV, fmax=28.084)\n",
      "[1600/3000] 8e295137-042d-5319-86f1-4fcbabc1af8d__T480K__step990.cif (n=12, E=18.957172 eV, fmax=451.180)\n",
      "[1700/3000] 9f0596f2-fc2c-553d-9e2e-c168c9fc7679__T360K__step390.cif (n=2, E=-12.807711 eV, fmax=16.698)\n",
      "[1800/3000] 9f0596f2-fc2c-553d-9e2e-c168c9fc7679__T480K__step1390.cif (n=2, E=-16.751335 eV, fmax=1.120)\n",
      "[1900/3000] 9f0596f2-fc2c-553d-9e2e-c168c9fc7679__T480K__step790.cif (n=2, E=7.552092 eV, fmax=31.519)\n",
      "[2000/3000] cc5b0d58-07f3-585b-936c-5650cebf554a__T360K__step1790.cif (n=4, E=23.034210 eV, fmax=36.122)\n",
      "[2100/3000] cc5b0d58-07f3-585b-936c-5650cebf554a__T480K__step1190.cif (n=4, E=-2.503952 eV, fmax=30.240)\n",
      "[2200/3000] cc5b0d58-07f3-585b-936c-5650cebf554a__T480K__step590.cif (n=4, E=2.675869 eV, fmax=55.211)\n",
      "[2300/3000] d1cbe841-9b8d-5001-a42c-39a39bc59bdf__T360K__step1590.cif (n=8, E=-4.848892 eV, fmax=36.216)\n",
      "[2400/3000] d1cbe841-9b8d-5001-a42c-39a39bc59bdf__T360K__step990.cif (n=8, E=13.268383 eV, fmax=209.514)\n",
      "[2500/3000] d1cbe841-9b8d-5001-a42c-39a39bc59bdf__T480K__step390.cif (n=8, E=1.469410 eV, fmax=38.218)\n",
      "[2600/3000] e11a2cea-be8e-5208-afab-b83cb6d81f1d__T360K__step1390.cif (n=15, E=52.739997 eV, fmax=41.497)\n",
      "[2700/3000] ff27315f-1198-5125-9ebb-9288c2acd021__T360K__step1190.cif (n=3, E=-5.540750 eV, fmax=0.159)\n",
      "[2800/3000] ff27315f-1198-5125-9ebb-9288c2acd021__T360K__step590.cif (n=3, E=-2.844213 eV, fmax=4.038)\n",
      "[2900/3000] ff27315f-1198-5125-9ebb-9288c2acd021__T480K__step1590.cif (n=3, E=-1.687534 eV, fmax=3.816)\n",
      "[3000/3000] ff27315f-1198-5125-9ebb-9288c2acd021__T480K__step990.cif (n=3, E=1.123659 eV, fmax=20.722)\n",
      "\n",
      "Done in 1.1 min.\n",
      "JSONL: /home/phanim/harshitrawat/summer/md/labels_fresh_it2/run_20250811_121909/labels.jsonl\n",
      "Excel: /home/phanim/harshitrawat/summer/md/labels_fresh_it2/run_20250811_121909/labels.xlsx\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# relabel_md_snapshots_fresh_simple.py\n",
    "import re, json, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from math import isfinite\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ase.io import read\n",
    "\n",
    "# CHGNet import\n",
    "try:\n",
    "    from chgnet.model.dynamics import CHGNetCalculator\n",
    "except Exception:\n",
    "    from chgnet.model.ase import CHGNetCalculator\n",
    "\n",
    "# ==== CONFIG (edit here) ====\n",
    "INPUT_DIRS = [\n",
    "    \"/home/phanim/harshitrawat/summer/md/mdcifs_it2\"\n",
    "]\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_FLUSH_EVERY = 200\n",
    "PRINT_EVERY = 100\n",
    "OUT_ROOT = Path(\"/home/phanim/harshitrawat/summer/md/labels_fresh_it2\")\n",
    "TIMESTAMP_OUTPUT = True  # if False, overwrite \"latest\"\n",
    "# ============================\n",
    "\n",
    "FNAME_RE = re.compile(\n",
    "    r\"(?P<uuid>[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})__T(?P<T>\\d+)K__step(?P<step>\\d+)\\.cif\",\n",
    "    re.I,\n",
    ")\n",
    "\n",
    "def parse_from_name(p):\n",
    "    m = FNAME_RE.search(p.name)\n",
    "    if not m:\n",
    "        return None, None, None\n",
    "    return m.group(\"uuid\"), int(m.group(\"T\")), int(m.group(\"step\"))\n",
    "\n",
    "def safe_eval(atoms, name):\n",
    "    e = float(atoms.get_potential_energy())\n",
    "    f = atoms.get_forces()\n",
    "    n = len(atoms)\n",
    "    if f is None or f.shape != (n, 3):\n",
    "        raise RuntimeError(f\"{name}: forces.shape={None if f is None else f.shape}, expected ({n}, 3)\")\n",
    "    if (not isfinite(e)) or np.isnan(f).any() or np.isinf(f).any():\n",
    "        raise RuntimeError(f\"{name}: non-finite energy/forces\")\n",
    "    return e, f\n",
    "\n",
    "def collect_inputs(dirs):\n",
    "    files = []\n",
    "    for d in dirs:\n",
    "        d = Path(d)\n",
    "        if not d.exists():\n",
    "            print(f\"[warn] Missing dir: {d}\")\n",
    "            continue\n",
    "        files.extend(sorted(d.glob(\"*.cif\")))\n",
    "    return [p.resolve() for p in files]\n",
    "\n",
    "# ==== main ====\n",
    "snaps = collect_inputs(INPUT_DIRS)\n",
    "if not snaps:\n",
    "    raise SystemExit(\"No CIFs found.\")\n",
    "\n",
    "if TIMESTAMP_OUTPUT:\n",
    "    stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = OUT_ROOT / f\"run_{stamp}\"\n",
    "else:\n",
    "    out_dir = OUT_ROOT / \"latest\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_JSONL = out_dir / \"labels.jsonl\"\n",
    "OUT_EXCEL = out_dir / \"labels.xlsx\"\n",
    "\n",
    "calc = CHGNetCalculator(use_device=DEVICE, model_name=None)\n",
    "print(f\"[CHGNet] device = {getattr(calc.model, 'device', 'unknown')}\")\n",
    "\n",
    "buffer = []\n",
    "excel_rows = []\n",
    "t0 = time.time()\n",
    "total = len(snaps)\n",
    "print(f\"Found {total} snapshots → labeling to {out_dir}\")\n",
    "\n",
    "for i, p in enumerate(snaps, 1):\n",
    "    try:\n",
    "        atoms = read(p)\n",
    "        atoms.calc = calc\n",
    "\n",
    "        energy_eV, forces = safe_eval(atoms, p.name)\n",
    "        n = len(atoms)\n",
    "        fmag = np.linalg.norm(forces, axis=1)\n",
    "        fmax = float(fmag.max())\n",
    "        frms = float(np.sqrt((forces**2).sum() / n))\n",
    "        uid, T, step = parse_from_name(p)\n",
    "\n",
    "        buffer.append({\n",
    "            \"snapshot_file\": str(p),\n",
    "            \"uuid\": uid,\n",
    "            \"temperature_K\": T,\n",
    "            \"step\": step,\n",
    "            \"n_atoms\": n,\n",
    "            \"energy_eV\": float(energy_eV),\n",
    "            \"forces_per_atom_eV_per_A\": forces.tolist(),\n",
    "            \"force_rms_eV_per_A\": frms,\n",
    "            \"fmax_eV_per_A\": fmax,\n",
    "            \"created_at_iso\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        })\n",
    "\n",
    "        excel_rows.append({\n",
    "            \"snapshot_file\": str(p),\n",
    "            \"uuid\": uid,\n",
    "            \"temperature_K\": T,\n",
    "            \"step\": step,\n",
    "            \"n_atoms\": n,\n",
    "            \"energy_eV\": float(energy_eV),\n",
    "            \"force_rms_eV_per_A\": frms,\n",
    "            \"fmax_eV_per_A\": fmax,\n",
    "        })\n",
    "\n",
    "        if len(buffer) >= BATCH_FLUSH_EVERY:\n",
    "            with open(OUT_JSONL, \"a\") as f:\n",
    "                for r in buffer:\n",
    "                    f.write(json.dumps(r) + \"\\n\")\n",
    "            buffer.clear()\n",
    "\n",
    "        if (i % PRINT_EVERY == 0) or (i == total):\n",
    "            print(f\"[{i}/{total}] {p.name} (n={n}, E={energy_eV:.6f} eV, fmax={fmax:.3f})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!! Failed {p}: {e}\")\n",
    "\n",
    "# final flush\n",
    "if buffer:\n",
    "    with open(OUT_JSONL, \"a\") as f:\n",
    "        for r in buffer:\n",
    "            f.write(json.dumps(r) + \"\\n\")\n",
    "buffer.clear()\n",
    "\n",
    "if excel_rows:\n",
    "    df = pd.DataFrame(excel_rows)\n",
    "    sort_cols = [c for c in [\"temperature_K\", \"uuid\", \"step\"] if c in df.columns]\n",
    "    df.sort_values(sort_cols, inplace=True, na_position=\"last\")\n",
    "    df.to_excel(OUT_EXCEL, index=False)\n",
    "\n",
    "print(f\"\\nDone in {(time.time()-t0)/60:.1f} min.\")\n",
    "print(f\"JSONL: {OUT_JSONL}\")\n",
    "print(f\"Excel: {OUT_EXCEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af256a9-9923-4824-9991-93c5a3a6b25c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INDEX] workers=16\n",
      "[INDEX] basenames=23308 total_paths=23308\n",
      "[LABEL] files=9 workers=16\n",
      "[WARN] Skipping bad JSONL line 1 in /home/phanim/harshitrawat/summer/T1_T2_T3_data/ood_per_model_dynamic.json: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "[WARN] Skipping bad JSONL line 2 in /home/phanim/harshitrawat/summer/T1_T2_T3_data/ood_per_model_dynamic.json: Extra data: line 1 column 8 (char 7)\n",
      "[WARN] Skipping bad JSONL line 3 in /home/phanim/harshitrawat/summer/T1_T2_T3_data/ood_per_model_dynamic.json: Extra data: line 1 column 10 (char 9)\n",
      "[WARN] Skipping bad JSONL line 1 in /home/phanim/harshitrawat/summer/T1_T2_T3_data/ood_ensemble_dynamic_k2.json: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "[WARN] Skipping bad JSONL line 2 in /home/phanim/harshitrawat/summer/T1_T2_T3_data/ood_ensemble_dynamic_k2.json: Extra data: line 1 column 11 (char 10)\n",
      "[WARN] Skipping bad JSONL line 3 in /home/phanim/harshitrawat/summer/T1_T2_T3_data/ood_ensemble_dynamic_k2.json: Extra data: line 1 column 11 (char 10)\n",
      "[WARN] Skipping bad JSONL line 1 in /home/phanim/harshitrawat/summer/T1_T2_T3_data/ood_ensemble_dynamic_k2.json: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "[WARN] Skipping bad JSONL line 2 in /home/phanim/harshitrawat/summer/T1_T2_T3_data/ood_ensemble_dynamic_k2.json: Extra data: line 1 column 11 (char 10)\n",
      "[WARN] Skipping bad JSONL line 3 in /home/phanim/harshitrawat/summer/T1_T2_T3_data/ood_ensemble_dynamic_k2.json: Extra data: line 1 column 11 (char 10)\n",
      "[WARN] Skipping bad JSONL line 1 in /home/phanim/harshitrawat/summer/T1_T2_T3_data/ood_per_model_dynamic.json: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "[WARN] Skipping bad JSONL line 2 in /home/phanim/harshitrawat/summer/T1_T2_T3_data/ood_per_model_dynamic.json: Extra data: line 1 column 8 (char 7)\n",
      "[WARN] Skipping bad JSONL line 3 in /home/phanim/harshitrawat/summer/T1_T2_T3_data/ood_per_model_dynamic.json: Extra data: line 1 column 10 (char 9)\n",
      "[labels] Indexed 11654 unique CIFs (CHGNet=18696, MD_JSONL=0, issues=8654)\n",
      "[SPLIT] T1=5986 Val=640 T2=5028\n",
      "[CURATE] workers=24\n",
      "[CURATE] keep: T1=5986 Val=640 T2=5028 | drop_total=0\n",
      "[WRITE] workers=48 chunk=600\n",
      "[T1] writing 5986 frames → 10 chunks (chunk=600, workers=48)\n",
      "[T1] progress: chunk 7/10\n",
      "[T1] progress: chunk 8/10\n",
      "[T1] progress: chunk 4/10\n",
      "[T1] progress: chunk 1/10\n",
      "[T1] progress: chunk 3/10\n",
      "[T1] progress: chunk 10/10\n",
      "[T1] progress: chunk 9/10\n",
      "[T1] progress: chunk 6/10\n",
      "[T1] progress: chunk 5/10\n",
      "[T1] progress: chunk 2/10\n",
      "[T1] DONE: wrote 5986 frames → /home/phanim/harshitrawat/summer/final_work_extxyz/T1.extxyz\n",
      "[val] writing 640 frames → 2 chunks (chunk=600, workers=48)\n",
      "[val] progress: chunk 2/2\n",
      "[val] progress: chunk 1/2\n",
      "[val] DONE: wrote 640 frames → /home/phanim/harshitrawat/summer/final_work_extxyz/val.extxyz\n",
      "[T2] writing 5028 frames → 9 chunks (chunk=600, workers=48)\n",
      "[T2] progress: chunk 5/9\n",
      "[T2] progress: chunk 6/9\n",
      "[T2] progress: chunk 7/9\n",
      "[T2] progress: chunk 9/9\n",
      "[T2] progress: chunk 3/9\n",
      "[T2] progress: chunk 8/9\n",
      "[T2] progress: chunk 2/9\n",
      "[T2] progress: chunk 1/9\n",
      "[T2] progress: chunk 4/9\n",
      "[T2] DONE: wrote 5028 frames → /home/phanim/harshitrawat/summer/final_work_extxyz/T2.extxyz\n",
      "\n",
      "[OK] Wrote:\n",
      " - /home/phanim/harshitrawat/summer/final_work_extxyz/T1.extxyz\n",
      " - /home/phanim/harshitrawat/summer/final_work_extxyz/val.extxyz\n",
      " - /home/phanim/harshitrawat/summer/final_work_extxyz/T2.extxyz\n",
      "\n",
      "[train hint] Use: --energy_key energy --forces_key forces\n"
     ]
    }
   ],
   "source": [
    "# ==== MAX-PARALLEL ZERO-SKIP EXTXYZ BUILDER (Notebook-Ready) ====\n",
    "# Parallelized stages:\n",
    "#  1) CIF indexing (dir walks)             -> ProcessPool\n",
    "#  2) Label file parsing/ingestion         -> ProcessPool\n",
    "#  3) Curation (natoms vs forces)          -> ProcessPool\n",
    "#  4) Sharded EXTXYZ writing               -> ProcessPool\n",
    "#\n",
    "# Outputs: T1.extxyz, val.extxyz, T2.extxyz (every frame has energy & forces)\n",
    "\n",
    "import os, json, gzip, math\n",
    "from typing import List, Dict, Tuple, Optional, Any, Iterable\n",
    "from collections import defaultdict, Counter\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from ase.io import read, write\n",
    "\n",
    "# --------- CONFIG (edit here) ---------\n",
    "BASE = \"/home/phanim/harshitrawat/summer\"\n",
    "\n",
    "CIF_DIRS = [\n",
    "    f\"{BASE}/md/mdcifs\",\n",
    "    f\"{BASE}/md/mdcifs_strained_perturbed\",\n",
    "    f\"{BASE}/md/mdcifs_strained_perturbed_prime\",\n",
    "    f\"{BASE}/md/mdcifs_it2\",\n",
    "]\n",
    "\n",
    "LABEL_INPUTS = [\n",
    "    f\"{BASE}/T1_T2_T3_data\",\n",
    "    f\"{BASE}/md/labels_fresh_it2\",\n",
    "]\n",
    "\n",
    "SPLITS_DIR = f\"{BASE}/md/splits_global_49_7_44/run_20250811_164008\"\n",
    "OUT_DIR    = f\"{BASE}/final_work_extxyz\"\n",
    "\n",
    "# Workers & chunking\n",
    "MAX_WORKERS   = min(48, max(1, (os.cpu_count() or 8)))\n",
    "INDEX_WORKERS = max(1, min(16, MAX_WORKERS // 2))   # dir-walkers\n",
    "LABEL_WORKERS = max(1, min(16, MAX_WORKERS // 2))   # json parsers\n",
    "CURATE_WORKERS= max(1, MAX_WORKERS // 2)            # natoms checks\n",
    "WRITE_WORKERS = max(1, MAX_WORKERS)                 # shard writers\n",
    "CHUNK_SIZE    = 600                                 # frames/shard\n",
    "\n",
    "# Source keys in label dumps\n",
    "ENERGY_SRC_KEY = \"energy_eV\"\n",
    "FORCES_SRC_KEY = \"forces_per_atom_eV_per_A\"\n",
    "\n",
    "# Keep threads sane in notebooks\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "\n",
    "\n",
    "# --------------- UTIL: robust JSON / JSONL reader ---------------\n",
    "def _read_json_or_jsonl(path: Path) -> Iterable[dict]:\n",
    "    opener = gzip.open if path.suffixes[-1:] == ['.gz'] else open\n",
    "    try:\n",
    "        with opener(path, \"rt\") as f:\n",
    "            buf = f.read(2048)\n",
    "            if not buf:\n",
    "                return\n",
    "            f.seek(0)\n",
    "            first = next((c for c in buf if not c.isspace()), \"\")\n",
    "            if first == \"[\":\n",
    "                try:\n",
    "                    arr = json.load(f)\n",
    "                    for rec in arr:\n",
    "                        if isinstance(rec, dict):\n",
    "                            yield rec\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"[WARN] JSON array parse failed in {path}: {e}\")\n",
    "            else:\n",
    "                for ln, line in enumerate(f, 1):\n",
    "                    s = line.strip()\n",
    "                    if not s:\n",
    "                        continue\n",
    "                    try:\n",
    "                        rec = json.loads(s)\n",
    "                        if isinstance(rec, dict):\n",
    "                            yield rec\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        # skip malformed lines, keep going\n",
    "                        if ln <= 3:\n",
    "                            print(f\"[WARN] Skipping bad JSONL line {ln} in {path}: {e}\")\n",
    "                        continue\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Cannot open/parse {path}: {e}\")\n",
    "\n",
    "\n",
    "# --------------- PARALLEL CIF INDEXING ---------------\n",
    "def _walk_one_dir(d: str) -> Dict[str, List[str]]:\n",
    "    idx = defaultdict(list)\n",
    "    if not os.path.isdir(d):\n",
    "        return idx\n",
    "    for root, _, files in os.walk(d):\n",
    "        for fn in files:\n",
    "            low = fn.lower()\n",
    "            if low.endswith(\".cif\") or low.endswith(\".cif.gz\"):\n",
    "                ap = os.path.abspath(os.path.join(root, fn))\n",
    "                idx[fn].append(ap)\n",
    "                if low.endswith(\".cif.gz\"):\n",
    "                    idx[fn[:-3]].append(ap)      # also index \".cif\"\n",
    "                if low.endswith(\".cif\"):\n",
    "                    idx[fn + \".gz\"].append(ap)    # also index \".cif.gz\"\n",
    "    return idx\n",
    "\n",
    "def build_cif_index_parallel(dirs: List[str], workers: int) -> Dict[str, List[str]]:\n",
    "    out = defaultdict(list)\n",
    "    with ProcessPoolExecutor(max_workers=workers) as ex:\n",
    "        futs = {ex.submit(_walk_one_dir, d): d for d in dirs}\n",
    "        for fut in as_completed(futs):\n",
    "            sub = fut.result()\n",
    "            for k, v in sub.items():\n",
    "                out[k].extend(v)\n",
    "    # dedup lists\n",
    "    for k, v in out.items():\n",
    "        out[k] = sorted(set(v))\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------- PATH RESOLUTION ---------------\n",
    "def resolve_from_index(name_or_path: str, index: Dict[str, List[str]]) -> Optional[str]:\n",
    "    if os.path.isfile(name_or_path):\n",
    "        return os.path.abspath(name_or_path)\n",
    "    base = os.path.basename(name_or_path)\n",
    "    cands = index.get(base)\n",
    "    if not cands:\n",
    "        # cross-swap .cif <-> .cif.gz\n",
    "        if base.lower().endswith(\".cif\"):\n",
    "            cands = index.get(base + \".gz\")\n",
    "        elif base.lower().endswith(\".cif.gz\"):\n",
    "            cands = index.get(base[:-3])\n",
    "    if not cands:\n",
    "        return None\n",
    "    # prefer shorter path (usually closer) then lexicographic for determinism\n",
    "    return sorted(cands, key=lambda p: (len(p), p))[0]\n",
    "\n",
    "\n",
    "# --------------- PARALLEL LABEL INGEST ---------------\n",
    "def _list_label_files(inputs: List[str]) -> List[Path]:\n",
    "    ALLOWED = (\".json\", \".jsonl\", \".json.gz\", \".jsonl.gz\")\n",
    "    out = []\n",
    "    for item in inputs:\n",
    "        p = Path(item)\n",
    "        if not p.exists(): \n",
    "            continue\n",
    "        if p.is_file() and p.name.endswith(ALLOWED):\n",
    "            out.append(p)\n",
    "        elif p.is_dir():\n",
    "            for q in p.rglob(\"*\"):\n",
    "                if q.is_file() and q.name.endswith(ALLOWED):\n",
    "                    out.append(q)\n",
    "    # de-dup preserving order\n",
    "    seen, uniq = set(), []\n",
    "    for p in out:\n",
    "        if p in seen: continue\n",
    "        seen.add(p); uniq.append(p)\n",
    "    return uniq\n",
    "\n",
    "def _ingest_one_label_file(args) -> Tuple[Dict[str, dict], List[dict], Dict[str,int]]:\n",
    "    fp, index = args\n",
    "    m = {}\n",
    "    issues = []\n",
    "    counts = {\"CHGNet\":0, \"MD_JSONL\":0}\n",
    "    src = \"MD_JSONL\" if \"jsonl\" in fp.suffixes else \"CHGNet\"\n",
    "    for rec in _read_json_or_jsonl(fp):\n",
    "        # unified name fields to resolve\n",
    "        name = rec.get(\"snapshot_file\") or rec.get(\"file\") or rec.get(\"src_path\") or rec.get(\"path\")\n",
    "        if not name:\n",
    "            continue\n",
    "        cif_path = resolve_from_index(name, index)\n",
    "        if not cif_path:\n",
    "            issues.append({\"file\": name, \"why\": \"cif_not_found\", \"source\": fp.name}); continue\n",
    "        e, f = rec.get(ENERGY_SRC_KEY), rec.get(FORCES_SRC_KEY)\n",
    "        if e is None or f is None:\n",
    "            issues.append({\"file\": name, \"why\": \"missing_energy_or_forces\", \"source\": fp.name}); continue\n",
    "        m[cif_path] = {\n",
    "            \"energy_eV\": float(e),\n",
    "            \"forces\": np.asarray(f, dtype=np.float64),\n",
    "            \"meta\": {\"label_source\": src}\n",
    "        }\n",
    "        for k in (\"temperature_K\", \"step\", \"uuid\"):\n",
    "            if k in rec: m[cif_path][\"meta\"][k] = rec[k]\n",
    "        counts[src] += 1\n",
    "    return m, issues, counts\n",
    "\n",
    "def build_label_index_parallel(label_files: List[Path], index: Dict[str, List[str]], workers: int\n",
    "                              ) -> Tuple[Dict[str, dict], List[dict]]:\n",
    "    label_map: Dict[str, dict] = {}\n",
    "    issues_all: List[dict] = []\n",
    "    totals = Counter()\n",
    "    if not label_files:\n",
    "        return {}, [{\"why\":\"no_label_files_found\"}]\n",
    "    # fan out files to workers\n",
    "    with ProcessPoolExecutor(max_workers=workers) as ex:\n",
    "        futs = {ex.submit(_ingest_one_label_file, (fp, index)): fp for fp in label_files}\n",
    "        for fut in as_completed(futs):\n",
    "            m, iss, counts = fut.result()\n",
    "            issues_all.extend(iss)\n",
    "            totals.update(counts)\n",
    "            # newer files should override older — we iterate in as_completed (random),\n",
    "            # so enforce order by filename mtime: rebuild at the end deterministically\n",
    "            label_map.update(m)\n",
    "    # deterministic preference: sort label_files by mtime ascending, then re-apply to ensure newest overrides\n",
    "    for fp in sorted(label_files, key=lambda p: p.stat().st_mtime):\n",
    "        m, _, _ = _ingest_one_label_file((fp, index))\n",
    "        label_map.update(m)\n",
    "    print(f\"[labels] Indexed {len(label_map)} unique CIFs \"\n",
    "          f\"(CHGNet={totals['CHGNet']}, MD_JSONL={totals['MD_JSONL']}, issues={len(issues_all)})\")\n",
    "    return label_map, issues_all\n",
    "\n",
    "\n",
    "# --------------- CURATION (Parallel) ---------------\n",
    "@lru_cache(maxsize=100_000)\n",
    "def _natoms_of(path: str) -> Optional[int]:\n",
    "    try: return len(read(path))\n",
    "    except Exception: return None\n",
    "\n",
    "def _check_one(pair: Tuple[str, int]) -> Tuple[str, bool, Optional[int]]:\n",
    "    p, flen = pair\n",
    "    n = _natoms_of(p)\n",
    "    ok = (n is not None) and (n == flen)\n",
    "    return p, ok, n\n",
    "\n",
    "def curate_split_parallel(entries: List[str], index: Dict[str, List[str]], label_map: Dict[str, dict], workers: int\n",
    "                         ) -> Tuple[List[str], List[dict]]:\n",
    "    resolved = []\n",
    "    dropped = []\n",
    "    for e in entries:\n",
    "        p = resolve_from_index(e, index)\n",
    "        if p is None:\n",
    "            dropped.append({\"file\": e, \"why\": \"cif_not_found_in_index\"}); continue\n",
    "        lab = label_map.get(p)\n",
    "        if lab is None:\n",
    "            dropped.append({\"file\": p, \"why\": \"no_label_found\"}); continue\n",
    "        resolved.append((p, len(lab[\"forces\"])))\n",
    "\n",
    "    curated = []\n",
    "    with ProcessPoolExecutor(max_workers=workers) as ex:\n",
    "        futs = {ex.submit(_check_one, pair): pair for pair in resolved}\n",
    "        for fut in as_completed(futs):\n",
    "            p, flen = futs[fut]\n",
    "            try:\n",
    "                _p, ok, n = fut.result()\n",
    "                if ok: curated.append(p)\n",
    "                else: dropped.append({\"file\": p, \"why\": f\"forces_shape_mismatch:{flen}vs{n}\"})\n",
    "            except Exception as e:\n",
    "                dropped.append({\"file\": p, \"why\": f\"shape_check_error:{e}\"})\n",
    "    return curated, dropped\n",
    "\n",
    "\n",
    "# --------------- SHARDED WRITER (Parallel) ---------------\n",
    "def _config_type_from_name(path: str) -> str:\n",
    "    pl = path.lower()\n",
    "    if \"mdcifs_it2\" in pl: return \"md_it2\"\n",
    "    if \"prime\" in pl:      return \"strain_perturb_prime\"\n",
    "    if \"perturbed\" in pl:  return \"strain_perturb\"\n",
    "    return \"base\"\n",
    "\n",
    "def _write_chunk(args) -> Tuple[str, int]:\n",
    "    split_name, chunk_id, paths, label_map, shard_dir = args\n",
    "    shard_path = Path(shard_dir) / f\"{split_name}.chunk{chunk_id:05d}.extxyz\"\n",
    "    frames = []\n",
    "    for cif_path in paths:\n",
    "        lab = label_map[cif_path]\n",
    "        a = read(cif_path)\n",
    "        a.info[\"energy\"] = float(lab[\"energy_eV\"])                   # eV\n",
    "        a.arrays[\"forces\"] = np.asarray(lab[\"forces\"], np.float64)   # eV/Å\n",
    "        a.info[\"file\"] = os.path.basename(cif_path)\n",
    "        a.info[\"src_path\"] = cif_path\n",
    "        a.info[\"label_source\"] = lab[\"meta\"].get(\"label_source\")\n",
    "        for k in (\"temperature_K\", \"step\", \"uuid\"):\n",
    "            if k in lab[\"meta\"]: a.info[k] = lab[\"meta\"][k]\n",
    "        a.info[\"config_type\"] = _config_type_from_name(cif_path)\n",
    "        frames.append(a)\n",
    "    write(str(shard_path), frames, format=\"extxyz\")\n",
    "    return str(shard_path), len(frames)\n",
    "\n",
    "def write_split_zero_skip_parallel(split_name: str, curated_paths: List[str], label_map: Dict[str, dict],\n",
    "                                   out_dir: Path, workers: int, chunk_size: int) -> int:\n",
    "    shard_dir = out_dir / \"_shards\" / split_name\n",
    "    shard_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    chunks = [curated_paths[i:i+chunk_size] for i in range(0, len(curated_paths), chunk_size)]\n",
    "    print(f\"[{split_name}] writing {len(curated_paths)} frames → {len(chunks)} chunks (chunk={chunk_size}, workers={workers})\")\n",
    "\n",
    "    total, shard_paths = 0, []\n",
    "    with ProcessPoolExecutor(max_workers=workers) as ex:\n",
    "        futs = {ex.submit(_write_chunk, (split_name, cid, ch, label_map, shard_dir)): cid\n",
    "                for cid, ch in enumerate(chunks)}\n",
    "        for fut in as_completed(futs):\n",
    "            cid = futs[fut]\n",
    "            try:\n",
    "                sp, w = fut.result()\n",
    "                shard_paths.append(sp); total += w\n",
    "                if (cid+1) % max(1, math.ceil(len(chunks)/10)) == 0:\n",
    "                    print(f\"[{split_name}] progress: chunk {cid+1}/{len(chunks)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[{split_name}] ERROR chunk {cid}: {e}\")\n",
    "\n",
    "    final_path = out_dir / f\"{split_name}.extxyz\"\n",
    "    if final_path.exists(): final_path.unlink()\n",
    "    with open(final_path, \"ab\") as outf:\n",
    "        for sp in sorted(shard_paths):\n",
    "            try:\n",
    "                with open(sp, \"rb\") as sf:\n",
    "                    outf.write(sf.read())\n",
    "            except Exception as e:\n",
    "                print(f\"[{split_name}] WARN merge failed for {sp}: {e}\")\n",
    "            try:\n",
    "                Path(sp).unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "    print(f\"[{split_name}] DONE: wrote {total} frames → {final_path}\")\n",
    "    return total\n",
    "\n",
    "\n",
    "# ===================== RUN EVERYTHING =====================\n",
    "out_dir = Path(OUT_DIR); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[INDEX] workers={INDEX_WORKERS}\")\n",
    "cif_index = build_cif_index_parallel(CIF_DIRS, workers=INDEX_WORKERS)\n",
    "print(f\"[INDEX] basenames={len(cif_index)} total_paths={sum(len(v) for v in cif_index.values())}\")\n",
    "\n",
    "label_files = _list_label_files(LABEL_INPUTS)\n",
    "print(f\"[LABEL] files={len(label_files)} workers={LABEL_WORKERS}\")\n",
    "label_map, label_issues = build_label_index_parallel(label_files, cif_index, workers=LABEL_WORKERS)\n",
    "\n",
    "# Load split lists\n",
    "with open(Path(SPLITS_DIR)/\"T1.json\")  as f: T1  = json.load(f)\n",
    "with open(Path(SPLITS_DIR)/\"Val.json\") as f: VAL = json.load(f)\n",
    "with open(Path(SPLITS_DIR)/\"T2.json\")  as f: T2  = json.load(f)\n",
    "print(f\"[SPLIT] T1={len(T1)} Val={len(VAL)} T2={len(T2)}\")\n",
    "\n",
    "# Curate (zero-skip guarantee)\n",
    "print(f\"[CURATE] workers={CURATE_WORKERS}\")\n",
    "T1_cur,  T1_drop  = curate_split_parallel(T1,  cif_index, label_map, workers=CURATE_WORKERS)\n",
    "VAL_cur, VAL_drop = curate_split_parallel(VAL, cif_index, label_map, workers=CURATE_WORKERS)\n",
    "T2_cur,  T2_drop  = curate_split_parallel(T2,  cif_index, label_map, workers=CURATE_WORKERS)\n",
    "print(f\"[CURATE] keep: T1={len(T1_cur)} Val={len(VAL_cur)} T2={len(T2_cur)} | drop_total={len(T1_drop)+len(VAL_drop)+len(T2_drop)}\")\n",
    "\n",
    "# Write EXTXYZ (parallel shards)\n",
    "print(f\"[WRITE] workers={WRITE_WORKERS} chunk={CHUNK_SIZE}\")\n",
    "n1 = write_split_zero_skip_parallel(\"T1\",  T1_cur,  label_map, out_dir, workers=WRITE_WORKERS, chunk_size=CHUNK_SIZE)\n",
    "nv = write_split_zero_skip_parallel(\"val\", VAL_cur, label_map, out_dir, workers=WRITE_WORKERS, chunk_size=CHUNK_SIZE)\n",
    "n2 = write_split_zero_skip_parallel(\"T2\",  T2_cur,  label_map, out_dir, workers=WRITE_WORKERS, chunk_size=CHUNK_SIZE)\n",
    "\n",
    "# Manifest + curation report\n",
    "manifest = {\n",
    "    \"outputs\": {\"T1\": str(out_dir/\"T1.extxyz\"), \"val\": str(out_dir/\"val.extxyz\"), \"T2\": str(out_dir/\"T2.extxyz\")},\n",
    "    \"written\": {\"T1\": n1, \"val\": nv, \"T2\": n2},\n",
    "    \"workers\": {\"index\": INDEX_WORKERS, \"label\": LABEL_WORKERS, \"curate\": CURATE_WORKERS, \"write\": WRITE_WORKERS},\n",
    "    \"chunk_size\": CHUNK_SIZE,\n",
    "}\n",
    "(Path(OUT_DIR)/\"manifest_zero_skip_parallel.json\").write_text(json.dumps(manifest, indent=2))\n",
    "(Path(OUT_DIR)/\"curation_report_parallel.json\").write_text(json.dumps({\n",
    "    \"label_issues\": label_issues,\n",
    "    \"drops\": {\n",
    "        \"T1\": T1_drop[:200], \"Val\": VAL_drop[:200], \"T2\": T2_drop[:200],\n",
    "        \"note\": \"Only a 200-sample of drops kept for brevity.\"\n",
    "    }\n",
    "}, indent=2))\n",
    "\n",
    "print(\"\\n[OK] Wrote:\")\n",
    "print(\" -\", manifest[\"outputs\"][\"T1\"])\n",
    "print(\" -\", manifest[\"outputs\"][\"val\"])\n",
    "print(\" -\", manifest[\"outputs\"][\"T2\"])\n",
    "print(\"\\n[train hint] Use: --energy_key energy --forces_key forces\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f8b40b-2814-4815-9f87-714a7e15f7c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: mace_run_train \\\n",
      "    --name \\\n",
      "    mace_T1_singlehead_sanity \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/final_work_extxyz/T1.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/final_work_extxyz/val.extxyz \\\n",
      "    --energy_key \\\n",
      "    energy \\\n",
      "    --forces_key \\\n",
      "    forces \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --batch_size \\\n",
      "    2 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --valid_fraction \\\n",
      "    0.0 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    15 \\\n",
      "    --restart_latest \\\n",
      "    --forces_weight \\\n",
      "    30.0 \\\n",
      "    --energy_weight \\\n",
      "    1.0 \\\n",
      "    --lr \\\n",
      "    5e-3 \\\n",
      "    --weight_decay \\\n",
      "    1e-8 \\\n",
      "    --clip_grad \\\n",
      "    3 \\\n",
      "    --patience \\\n",
      "    7 \\\n",
      "    --E0s \\\n",
      "    average\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 20:40:44.166 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-08-12 20:40:44.167 INFO: MACE version: 0.3.14\n",
      "2025-08-12 20:40:44.798 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py:152: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 20:40:46.420 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-08-12 20:40:46.421 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-08-12 20:40:46.421 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-08-12 20:40:46.421 INFO: Using heads: ['Default']\n",
      "2025-08-12 20:40:46.421 INFO: Using the key specifications to parse data:\n",
      "2025-08-12 20:40:46.421 INFO: Default: KeySpecification(info_keys={'energy': 'energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'forces', 'charges': 'REF_charges'})\n",
      "2025-08-12 20:40:46.421 INFO: =============    Processing head Default     ===========\n",
      "2025-08-12 20:41:04.989 WARNING: Since ASE version 3.23.0b1, using energy_key 'energy' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'energy' to 'REF_energy'. You need to use --energy_key='REF_energy' to specify the chosen key name.\n",
      "2025-08-12 20:41:05.493 WARNING: Since ASE version 3.23.0b1, using forces_key 'forces' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'forces' to 'REF_forces'. You need to use --forces_key='REF_forces' to specify the chosen key name.\n",
      "2025-08-12 20:41:06.579 INFO: Training set 1/1 [energy: 5986, stress: 0, virials: 0, dipole components: 0, head: 5986, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 5986, charges: 0]\n",
      "2025-08-12 20:41:06.597 INFO: Total Training set [energy: 5986, stress: 0, virials: 0, dipole components: 0, head: 5986, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 5986, charges: 0]\n",
      "2025-08-12 20:41:06.765 WARNING: Since ASE version 3.23.0b1, using energy_key 'energy' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'energy' to 'REF_energy'. You need to use --energy_key='REF_energy' to specify the chosen key name.\n",
      "2025-08-12 20:41:06.806 WARNING: Since ASE version 3.23.0b1, using forces_key 'forces' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting 'forces' to 'REF_forces'. You need to use --forces_key='REF_forces' to specify the chosen key name.\n",
      "2025-08-12 20:41:06.857 INFO: Validation set 1/1 [energy: 640, stress: 0, virials: 0, dipole components: 0, head: 640, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 640, charges: 0]\n",
      "2025-08-12 20:41:06.859 INFO: Total Validation set [energy: 640, stress: 0, virials: 0, dipole components: 0, head: 640, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 640, charges: 0]\n",
      "2025-08-12 20:41:06.859 INFO: Total number of configurations: train=5986, valid=640, tests=[],\n",
      "2025-08-12 20:41:07.089 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-08-12 20:41:07.089 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-08-12 20:41:07.089 INFO: Computing average Atomic Energies using least squares regression\n",
      "2025-08-12 20:41:07.124 INFO: Atomic Energies used (z: eV) for head Default: {3: -1.1190404292130913, 8: 0.5759887109338244, 40: 5.377267508500145, 57: -69.0693759587064}\n",
      "2025-08-12 20:41:07.124 INFO: Processing datasets for head 'Default'\n",
      "2025-08-12 20:42:15.815 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-08-12 20:42:16.003 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-08-12 20:42:16.003 INFO: Combined validation datasets for Default\n",
      "2025-08-12 20:42:16.003 INFO: Head 'Default' training dataset size: 5986\n",
      "2025-08-12 20:42:16.003 INFO: Computing average number of neighbors\n",
      "2025-08-12 20:42:23.410 INFO: Average number of neighbors: 70.13425544547107\n",
      "2025-08-12 20:42:23.411 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-08-12 20:42:23.411 INFO: ===========MODEL DETAILS===========\n",
      "2025-08-12 20:42:30.286 INFO: Loading FOUNDATION model\n",
      "2025-08-12 20:42:30.287 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-08-12 20:42:30.287 INFO: Model configuration extracted from foundation model\n",
      "2025-08-12 20:42:30.287 INFO: Using weighted loss function for fine-tuning\n",
      "2025-08-12 20:42:30.287 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-08-12 20:42:30.287 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-08-12 20:42:30.287 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-08-12 20:42:30.287 INFO: Distance transform for radial basis functions: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/checkpoint.py:187: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 20:42:31.594 INFO: Total number of parameters: 894362\n",
      "2025-08-12 20:42:31.595 INFO: \n",
      "2025-08-12 20:42:31.595 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-08-12 20:42:31.595 INFO: Using ADAM as parameter optimizer\n",
      "2025-08-12 20:42:31.595 INFO: Batch size: 2\n",
      "2025-08-12 20:42:31.595 INFO: Number of gradient updates: 44895\n",
      "2025-08-12 20:42:31.595 INFO: Learning rate: 0.005, weight decay: 1e-08\n",
      "2025-08-12 20:42:31.595 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=30.000)\n",
      "2025-08-12 20:42:31.647 WARNING: No SWA checkpoint found, while SWA is enabled. Compare the swa_start parameter and the latest checkpoint.\n",
      "2025-08-12 20:42:31.647 INFO: Loading checkpoint: ./checkpoints/mace_T1_singlehead_sanity_run-123_epoch-4.pt\n",
      "2025-08-12 20:42:31.688 INFO: Using gradient clipping with tolerance=3.000\n",
      "2025-08-12 20:42:31.688 INFO: \n",
      "2025-08-12 20:42:31.688 INFO: ===========TRAINING===========\n",
      "2025-08-12 20:42:31.688 INFO: Started training, reporting errors on validation set\n",
      "2025-08-12 20:42:31.688 INFO: Loss metrics on validation set\n",
      "2025-08-12 20:42:51.796 INFO: Initial: head: Default, loss=1015074.68241551, RMSE_E_per_atom=44005.39 meV, RMSE_F=145037.88 meV / A\n",
      "2025-08-12 21:10:31.706 INFO: Epoch 4: head: Default, loss=6896951.46142288, RMSE_E_per_atom=71244.12 meV, RMSE_F=376636.83 meV / A\n",
      "2025-08-12 21:38:09.378 INFO: Epoch 5: head: Default, loss=1982111.66351262, RMSE_E_per_atom=47153.27 meV, RMSE_F=202452.96 meV / A\n",
      "2025-08-12 22:05:46.985 INFO: Epoch 6: head: Default, loss=734772.88359769, RMSE_E_per_atom=42656.90 meV, RMSE_F=125061.38 meV / A\n",
      "2025-08-12 22:33:27.708 INFO: Epoch 7: head: Default, loss=1938985.16012128, RMSE_E_per_atom=46003.23 meV, RMSE_F=200980.77 meV / A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m     subprocess.run(cmd, check=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m cmd = [\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmace_run_train\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m--name\u001b[39m\u001b[33m\"\u001b[39m,              \u001b[33m\"\u001b[39m\u001b[33mmace_T1_singlehead_sanity\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m--E0s\u001b[39m\u001b[33m\"\u001b[39m,               \u001b[33m\"\u001b[39m\u001b[33maverage\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m ]\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m.join(cmd), file=sys.stderr)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:558\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    560\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:1213\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1211\u001b[39m         stderr = \u001b[38;5;28mself\u001b[39m.stderr.read()\n\u001b[32m   1212\u001b[39m         \u001b[38;5;28mself\u001b[39m.stderr.close()\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1215\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:1276\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1274\u001b[39m     endtime = _time() + timeout\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1278\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1279\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1280\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:2068\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.returncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2067\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2068\u001b[39m (pid, sts) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[32m   2071\u001b[39m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[32m   2072\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pid == \u001b[38;5;28mself\u001b[39m.pid:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:2026\u001b[39m, in \u001b[36mPopen._try_wait\u001b[39m\u001b[34m(self, wait_flags)\u001b[39m\n\u001b[32m   2024\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[32m   2025\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2026\u001b[39m     (pid, sts) = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[32m   2028\u001b[39m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[32m   2029\u001b[39m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[32m   2030\u001b[39m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[32m   2031\u001b[39m     pid = \u001b[38;5;28mself\u001b[39m.pid\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    # Uncomment if you want to force your local clone\n",
    "    os.environ[\"PYTHONPATH\"] = \"/home/phanim/harshitrawat/mace/mace\"\n",
    "\n",
    "    cmd = [\n",
    "        \"mace_run_train\",\n",
    "        \"--name\",              \"mace_T1_singlehead_sanity\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "\n",
    "        \"--train_file\",        \"/home/phanim/harshitrawat/summer/final_work_extxyz/T1.extxyz\",\n",
    "        \"--valid_file\",        \"/home/phanim/harshitrawat/summer/final_work_extxyz/val.extxyz\",\n",
    "\n",
    "        # ⚠️ Adjust these to the actual keys in your extxyz\n",
    "        \"--energy_key\", \"energy\",\n",
    "        \"--forces_key\", \"forces\",\n",
    "\n",
    "        \"--device\",            \"cuda\",\n",
    "        \"--batch_size\",        \"2\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "        \"--valid_fraction\",    \"0.0\",\n",
    "    \n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"15\",\n",
    "        \"--restart_latest\",\n",
    "\n",
    "        # Safer force/energy weighting\n",
    "        \"--forces_weight\",     \"30.0\",\n",
    "        \"--energy_weight\",     \"1.0\",\n",
    "\n",
    "        \"--lr\",                \"5e-3\",\n",
    "        \"--weight_decay\",      \"1e-8\",\n",
    "        \"--clip_grad\",         \"3\",\n",
    "        \"--patience\",          \"7\",\n",
    "\n",
    "        \"--E0s\",               \"average\"\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f79018ec-aae4-48fd-bc26-c7edba69b26d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: mace_run_train \\\n",
      "    --name \\\n",
      "    mace_T1_singlehead_sanity \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/final_work_extxyz/T1_labeled.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/final_work_extxyz/val_labeled.extxyz \\\n",
      "    --energy_key \\\n",
      "    energy_eV \\\n",
      "    --forces_key \\\n",
      "    forces_per_atom_eV_per_A \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --batch_size \\\n",
      "    2 \\\n",
      "    --valid_batch_size \\\n",
      "    2 \\\n",
      "    --valid_fraction \\\n",
      "    0.0 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    8 \\\n",
      "    --restart_latest \\\n",
      "    --forces_weight \\\n",
      "    30.0 \\\n",
      "    --energy_weight \\\n",
      "    1.0 \\\n",
      "    --lr \\\n",
      "    3e-3 \\\n",
      "    --weight_decay \\\n",
      "    1e-8 \\\n",
      "    --clip_grad \\\n",
      "    3 \\\n",
      "    --patience \\\n",
      "    3 \\\n",
      "    --ema_decay \\\n",
      "    0.0 \\\n",
      "    --E0s \\\n",
      "    average\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 23:02:18.200 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-08-12 23:02:18.200 INFO: MACE version: 0.3.14\n",
      "2025-08-12 23:02:18.773 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py:152: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 23:02:19.250 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-08-12 23:02:19.256 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-08-12 23:02:19.256 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-08-12 23:02:19.256 INFO: Using heads: ['Default']\n",
      "2025-08-12 23:02:19.256 INFO: Using the key specifications to parse data:\n",
      "2025-08-12 23:02:19.256 INFO: Default: KeySpecification(info_keys={'energy': 'energy_eV', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'forces_per_atom_eV_per_A', 'charges': 'REF_charges'})\n",
      "2025-08-12 23:02:19.256 INFO: =============    Processing head Default     ===========\n",
      "2025-08-12 23:02:40.690 INFO: Training set 1/1 [energy: 5986, stress: 0, virials: 0, dipole components: 0, head: 5986, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 5986, charges: 0]\n",
      "2025-08-12 23:02:40.707 INFO: Total Training set [energy: 5986, stress: 0, virials: 0, dipole components: 0, head: 5986, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 5986, charges: 0]\n",
      "2025-08-12 23:02:40.896 INFO: Validation set 1/1 [energy: 640, stress: 0, virials: 0, dipole components: 0, head: 640, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 640, charges: 0]\n",
      "2025-08-12 23:02:40.897 INFO: Total Validation set [energy: 640, stress: 0, virials: 0, dipole components: 0, head: 640, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 640, charges: 0]\n",
      "2025-08-12 23:02:40.898 INFO: Total number of configurations: train=5986, valid=640, tests=[],\n",
      "2025-08-12 23:02:41.127 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-08-12 23:02:41.127 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-08-12 23:02:41.127 INFO: Computing average Atomic Energies using least squares regression\n",
      "2025-08-12 23:02:41.162 INFO: Atomic Energies used (z: eV) for head Default: {3: -2623.992273374467, 8: 8028.192724893185, 40: 3170.708610087706, 57: -33104.899588536886}\n",
      "2025-08-12 23:02:41.162 INFO: Processing datasets for head 'Default'\n",
      "2025-08-12 23:03:49.619 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-08-12 23:03:49.807 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-08-12 23:03:49.807 INFO: Combined validation datasets for Default\n",
      "2025-08-12 23:03:49.807 INFO: Head 'Default' training dataset size: 5986\n",
      "2025-08-12 23:03:49.807 INFO: Computing average number of neighbors\n",
      "2025-08-12 23:03:57.404 INFO: Average number of neighbors: 70.13425544547107\n",
      "2025-08-12 23:03:57.405 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-08-12 23:03:57.405 INFO: ===========MODEL DETAILS===========\n",
      "2025-08-12 23:04:04.345 INFO: Loading FOUNDATION model\n",
      "2025-08-12 23:04:04.346 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-08-12 23:04:04.346 INFO: Model configuration extracted from foundation model\n",
      "2025-08-12 23:04:04.346 INFO: Using weighted loss function for fine-tuning\n",
      "2025-08-12 23:04:04.346 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-08-12 23:04:04.346 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-08-12 23:04:04.346 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-08-12 23:04:04.346 INFO: Distance transform for radial basis functions: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/checkpoint.py:187: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-12 23:04:05.662 INFO: Total number of parameters: 894362\n",
      "2025-08-12 23:04:05.663 INFO: \n",
      "2025-08-12 23:04:05.663 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-08-12 23:04:05.663 INFO: Using ADAM as parameter optimizer\n",
      "2025-08-12 23:04:05.663 INFO: Batch size: 2\n",
      "2025-08-12 23:04:05.663 INFO: Number of gradient updates: 23944\n",
      "2025-08-12 23:04:05.663 INFO: Learning rate: 0.003, weight decay: 1e-08\n",
      "2025-08-12 23:04:05.663 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=30.000)\n",
      "2025-08-12 23:04:05.686 WARNING: No SWA checkpoint found, while SWA is enabled. Compare the swa_start parameter and the latest checkpoint.\n",
      "2025-08-12 23:04:05.686 INFO: Loading checkpoint: ./checkpoints/mace_T1_singlehead_sanity_run-123_epoch-6.pt\n",
      "2025-08-12 23:04:05.725 INFO: Using gradient clipping with tolerance=3.000\n",
      "2025-08-12 23:04:05.725 INFO: \n",
      "2025-08-12 23:04:05.725 INFO: ===========TRAINING===========\n",
      "2025-08-12 23:04:05.725 INFO: Started training, reporting errors on validation set\n",
      "2025-08-12 23:04:05.725 INFO: Loss metrics on validation set\n",
      "2025-08-12 23:04:15.811 INFO: Initial: head: Default, loss=367457.72067584, RMSE_E_per_atom=42656.90 meV, RMSE_F=125061.38 meV / A\n",
      "2025-08-12 23:31:45.944 INFO: Epoch 6: head: Default, loss=10280532.55915516, RMSE_E_per_atom=81693.78 meV, RMSE_F=814561.83 meV / A\n",
      "2025-08-12 23:59:13.956 INFO: Epoch 7: head: Default, loss=30973520.20062055, RMSE_E_per_atom=117535.69 meV, RMSE_F=1179387.85 meV / A\n",
      "2025-08-12 23:59:13.958 INFO: Training complete\n",
      "2025-08-12 23:59:13.958 INFO: \n",
      "2025-08-12 23:59:13.958 INFO: ===========RESULTS===========\n",
      "2025-08-12 23:59:13.962 INFO: Loading checkpoint: ./checkpoints/mace_T1_singlehead_sanity_run-123_epoch-6.pt\n",
      "2025-08-12 23:59:14.001 INFO: Loaded Stage one model from epoch 6 for evaluation\n",
      "2025-08-12 23:59:14.001 INFO: Saving model to checkpoints/mace_T1_singlehead_sanity_run-123.model\n",
      "2025-08-12 23:59:14.269 INFO: Compiling model, saving metadata to mace_T1_singlehead_sanity_compiled.model\n",
      "2025-08-12 23:59:15.169 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-08-12 23:59:15.170 INFO: Skipping evaluation for heads: ['pt_head']\n",
      "2025-08-12 23:59:15.170 INFO: Evaluating train_Default ...\n",
      "2025-08-13 00:09:55.599 INFO: Evaluating valid_Default ...\n",
      "2025-08-13 00:10:03.179 INFO: Error-table on TRAIN and VALID:\n",
      "+---------------+---------------------+------------------+-------------------+\n",
      "|  config_type  | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+---------------+---------------------+------------------+-------------------+\n",
      "| train_Default |      1317364.4      |      80273.9     |        421.07     |\n",
      "| valid_Default |        81693.8      |     814561.8     |       3396.59     |\n",
      "+---------------+---------------------+------------------+-------------------+\n",
      "2025-08-13 00:10:03.271 INFO: Done\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, subprocess, sys\n",
    "\n",
    "def main():\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    os.environ[\"PYTHONPATH\"] = \"/home/phanim/harshitrawat/mace/mace\"\n",
    "\n",
    "    cmd = [\n",
    "        \"mace_run_train\",\n",
    "        \"--name\",\"mace_T1_singlehead_sanity\",\n",
    "        \"--model\",\"MACE\",\n",
    "        \"--num_interactions\",\"2\",\n",
    "        \"--foundation_model\",\"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "\n",
    "        \"--train_file\",\"/home/phanim/harshitrawat/summer/final_work_extxyz/T1_labeled.extxyz\",\n",
    "        \"--valid_file\",\"/home/phanim/harshitrawat/summer/final_work_extxyz/val_labeled.extxyz\",\n",
    "        \"--energy_key\",\"energy_eV\",\n",
    "        \"--forces_key\",\"forces_per_atom_eV_per_A\",\n",
    "\n",
    "        \"--device\",\"cuda\",\n",
    "        \"--batch_size\",\"2\",\"--valid_batch_size\",\"2\",\"--valid_fraction\",\"0.0\",\n",
    "        \"--r_max\",\"5.0\",\"--max_num_epochs\",\"\",\"--restart_latest\",\n",
    "\n",
    "        \"--forces_weight\",\"30.0\",\"--energy_weight\",\"1.0\",\n",
    "        \"--lr\",\"3e-3\",\"--weight_decay\",\"1e-8\",\"--clip_grad\",\"3\",\"--patience\",\"3\",\n",
    "        \"--ema_decay\",\"0.0\",\n",
    "\n",
    "        \"--E0s\",\"average\",\n",
    "    ]\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5e586e-bb67-4d0a-b856-d638cfd041ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Fetching elemental entries...\n",
      "⚠️ Could not find stable entry for Li\n",
      "⚠️ Could not find stable entry for La\n",
      "⚠️ Could not find stable entry for Zr\n",
      "⚠️ Could not find stable entry for O\n"
     ]
    }
   ],
   "source": [
    "from pymatgen.ext.matproj import MPRester\n",
    "from pymatgen.core import Element\n",
    "from pymatgen.analysis.phase_diagram import PhaseDiagram\n",
    "\n",
    "# 🔑 Insert your Materials Project API key here\n",
    "API_KEY = \"j3J85pX4nLw6asHG9E2lbbCHEKDKgrjc\"  # <-- paste your key as a string\n",
    "\n",
    "ELEMENTS = [\"Li\", \"La\", \"Zr\", \"O\"]\n",
    "\n",
    "with MPRester(API_KEY) as mpr:\n",
    "    print(\"🔍 Fetching elemental entries...\")\n",
    "    entries = mpr.get_entries_in_chemsys(ELEMENTS, inc_structure=\"final\")\n",
    "    el_entries = [e for e in entries if len(e.composition) == 1]\n",
    "\n",
    "    pd = PhaseDiagram(el_entries)\n",
    "    stable = {}\n",
    "\n",
    "    for el in ELEMENTS:\n",
    "        try:\n",
    "            entry = pd.get_stable_entry(Element(el))\n",
    "            stable[el] = entry\n",
    "        except:\n",
    "            print(f\"⚠️ Could not find stable entry for {el}\")\n",
    "\n",
    "    for el, entry in stable.items():\n",
    "        structure = entry.structure\n",
    "        filename = f\"{el}.cif\"\n",
    "        structure.to(fmt=\"cif\", filename=filename)\n",
    "        print(f\"✅ Saved {el}: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01bd5d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Searching lowest-energy structure for Li...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3205825/1875912313.py:13: DeprecationWarning: The inc_structure, conventional_unit_cell, and sort_by_e_above_hull arguments are deprecated. These arguments have no effect and will be removed in 2026.1.1.\n",
      "  entries = mpr.get_entries(el, inc_structure=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved Li: Li.cif\n",
      "🔍 Searching lowest-energy structure for La...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3205825/1875912313.py:13: DeprecationWarning: The inc_structure, conventional_unit_cell, and sort_by_e_above_hull arguments are deprecated. These arguments have no effect and will be removed in 2026.1.1.\n",
      "  entries = mpr.get_entries(el, inc_structure=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved La: La.cif\n",
      "🔍 Searching lowest-energy structure for Zr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3205825/1875912313.py:13: DeprecationWarning: The inc_structure, conventional_unit_cell, and sort_by_e_above_hull arguments are deprecated. These arguments have no effect and will be removed in 2026.1.1.\n",
      "  entries = mpr.get_entries(el, inc_structure=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved Zr: Zr.cif\n",
      "🔍 Searching lowest-energy structure for O...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3205825/1875912313.py:13: DeprecationWarning: The inc_structure, conventional_unit_cell, and sort_by_e_above_hull arguments are deprecated. These arguments have no effect and will be removed in 2026.1.1.\n",
      "  entries = mpr.get_entries(el, inc_structure=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved O: O.cif\n"
     ]
    }
   ],
   "source": [
    "from pymatgen.ext.matproj import MPRester\n",
    "from pymatgen.core import Composition\n",
    "import json\n",
    "\n",
    "# Insert your Materials Project API key here\n",
    "API_KEY = \"j3J85pX4nLw6asHG9E2lbbCHEKDKgrjc\"  # <--- Replace this with your key\n",
    "\n",
    "ELEMENTS = [\"Li\", \"La\", \"Zr\", \"O\"]\n",
    "\n",
    "with MPRester(API_KEY) as mpr:\n",
    "    for el in ELEMENTS:\n",
    "        print(f\"🔍 Searching lowest-energy structure for {el}...\")\n",
    "        entries = mpr.get_entries(el, inc_structure=True)\n",
    "        \n",
    "        # Filter only elemental ones\n",
    "        elemental_entries = [e for e in entries if Composition(el).reduced_formula == e.composition.reduced_formula]\n",
    "\n",
    "        if not elemental_entries:\n",
    "            print(f\"❌ No elemental structure found for {el}\")\n",
    "            continue\n",
    "\n",
    "        # Sort by energy per atom\n",
    "        elemental_entries.sort(key=lambda e: e.energy_per_atom)\n",
    "        best_entry = elemental_entries[0]\n",
    "        structure = best_entry.structure\n",
    "        filename = f\"{el}.cif\"\n",
    "        structure.to(fmt=\"cif\", filename=filename)\n",
    "        print(f\"✅ Saved {el}: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b5b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mace_0.3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
