{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74803bd4-0874-4a95-bcd2-702000a67339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: mace_run_train \\\n",
      "    --name \\\n",
      "    mace_T2_including_replay_w2 \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --pt_train_file \\\n",
      "    /home/phanim/harshitrawat/summer/replay_data/mp_finetuning-mace_T2_mp_replay_run-42.xyz \\\n",
      "    --atomic_numbers \\\n",
      "    [3,8,40,57] \\\n",
      "    --multiheads_finetuning \\\n",
      "    True \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz \\\n",
      "    --batch_size \\\n",
      "    2 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --forces_weight \\\n",
      "    0 \\\n",
      "    --energy_weight \\\n",
      "    10 \\\n",
      "    --stress_weight \\\n",
      "    0 \\\n",
      "    --lr \\\n",
      "    0.006 \\\n",
      "    --scheduler_patience \\\n",
      "    4 \\\n",
      "    --clip_grad \\\n",
      "    1 \\\n",
      "    --weight_decay \\\n",
      "    1e-8 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    130 \\\n",
      "    --E0s \\\n",
      "    {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549} \\\n",
      "    --seed \\\n",
      "    84 \\\n",
      "    --patience \\\n",
      "    8 \\\n",
      "    --restart_latest\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-21 10:16:49.590 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-08-21 10:16:49.591 INFO: MACE version: 0.3.14\n",
      "2025-08-21 10:16:50.231 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py:152: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-21 10:16:50.719 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-08-21 10:16:50.721 INFO: Multihead finetuning mode, setting learning rate to 0.0001 and EMA to True. To use a different learning rate, set --force_mh_ft_lr=True.\n",
      "2025-08-21 10:16:50.721 INFO: Using multiheads finetuning mode, setting learning rate to 0.0001 and EMA to True\n",
      "2025-08-21 10:16:50.721 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-08-21 10:16:50.721 INFO: Using heads: ['Default', 'pt_head']\n",
      "2025-08-21 10:16:50.721 INFO: Using the key specifications to parse data:\n",
      "2025-08-21 10:16:50.721 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-21 10:16:50.721 INFO: pt_head: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-21 10:16:50.721 INFO: =============    Processing head Default     ===========\n",
      "2025-08-21 10:16:56.090 INFO: Training set 1/1 [energy: 1612, stress: 0, virials: 0, dipole components: 0, head: 1612, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1612, charges: 0]\n",
      "2025-08-21 10:16:56.094 INFO: Total Training set [energy: 1612, stress: 0, virials: 0, dipole components: 0, head: 1612, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1612, charges: 0]\n",
      "2025-08-21 10:16:58.401 INFO: Validation set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-08-21 10:16:58.403 INFO: Total Validation set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-08-21 10:16:58.403 INFO: Total number of configurations: train=1612, valid=705, tests=[],\n",
      "2025-08-21 10:16:58.403 INFO: =============    Processing head pt_head     ===========\n",
      "2025-08-21 10:17:01.415 INFO: Training set 1/1 [energy: 10000, stress: 10000, virials: 0, dipole components: 0, head: 10000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 10000, charges: 0]\n",
      "2025-08-21 10:17:01.443 INFO: Total Training set [energy: 10000, stress: 10000, virials: 0, dipole components: 0, head: 10000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 10000, charges: 0]\n",
      "2025-08-21 10:17:01.443 INFO: No validation set provided, splitting training data instead.\n",
      "2025-08-21 10:17:01.447 INFO: Using random 10% of training set for validation with indices saved in: ./valid_indices_84.txt\n",
      "2025-08-21 10:17:01.477 INFO: Random Split Training set [energy: 9000, stress: 9000, virials: 0, dipole components: 0, head: 9000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 9000, charges: 0]\n",
      "2025-08-21 10:17:01.480 INFO: Random Split Validation set [energy: 1000, stress: 1000, virials: 0, dipole components: 0, head: 1000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1000, charges: 0]\n",
      "2025-08-21 10:17:01.480 INFO: Total number of configurations: train=9000, valid=1000, tests=[],\n",
      "2025-08-21 10:17:01.481 INFO: ==================Using multiheads finetuning mode==================\n",
      "2025-08-21 10:17:01.481 INFO: Total number of configurations in pretraining: train=9000, valid=1000\n",
      "2025-08-21 10:17:01.481 INFO: Using atomic numbers from command line argument\n",
      "2025-08-21 10:17:01.506 INFO: Atomic Numbers used: [1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91, 92, 93, 94]\n",
      "2025-08-21 10:17:01.506 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-08-21 10:17:01.510 INFO: Atomic Energies used (z: eV) for head Default: {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\n",
      "2025-08-21 10:17:01.510 INFO: Atomic Energies used (z: eV) for head pt_head: {1: -3.667168021358939, 3: -3.482100566595956, 4: -4.736697230897597, 5: -7.724935420523256, 6: -8.405573550273285, 7: -7.360100452662763, 8: -7.28459863421322, 9: -4.896490881731322, 11: -2.7593613569762425, 12: -2.814047612069227, 13: -4.846881245288104, 14: -7.694793133351899, 15: -6.9632957911820235, 16: -4.672630400190884, 17: -2.8116892814008096, 19: -2.6176454856894793, 20: -5.390461060484104, 21: -7.8857952163517675, 22: -10.268392986214433, 23: -8.665147785496703, 24: -9.233050763772013, 25: -8.304951520770791, 26: -7.0489865771593765, 27: -5.577439766222147, 28: -5.172747618813715, 29: -3.2520726958619472, 30: -1.2901611618726314, 31: -3.527082192997912, 32: -4.70845955030298, 33: -3.9765109025623238, 34: -3.886231055836541, 35: -2.5184940099633986, 36: 6.766947645687137, 37: -2.5634958965928316, 38: -4.938005211501922, 39: -10.149818838085771, 40: -11.846857579882572, 41: -12.138896361658485, 42: -8.791678800595722, 43: -8.78694939675911, 44: -7.78093221529871, 45: -6.850021409115055, 46: -4.891019073240479, 47: -2.0634296773864045, 48: -0.6395695518943755, 49: -2.7887442084286693, 50: -3.818604275441892, 51: -3.587068329278862, 52: -2.8804045971118897, 53: -1.6355986842433357, 54: 9.846723842807721, 55: -2.765284507132287, 56: -4.990956432167774, 57: -8.933684809576345, 58: -8.735591176647514, 59: -8.018966025544966, 60: -8.251491970213372, 61: -7.591719594359237, 62: -8.169659881166858, 63: -13.592664636171698, 64: -18.517523458456985, 65: -7.647396572993602, 66: -8.122981037851925, 67: -7.607787319678067, 68: -6.85029094445494, 69: -7.8268821327130365, 70: -3.584786591677161, 71: -7.455406192077973, 72: -12.796283502572146, 73: -14.108127281277586, 74: -9.354916969477486, 75: -11.387537567890853, 76: -9.621909492152557, 77: -7.324393429417677, 78: -5.3046964808341945, 79: -2.380092582080244, 80: 0.24948924158195362, 81: -2.3239789120665026, 82: -3.730042357127322, 83: -3.438792347649683, 89: -5.062878214511315, 90: -11.02462566385297, 91: -12.265613551943261, 92: -13.855648206100362, 93: -14.933092020258243, 94: -15.282826131998245}\n",
      "2025-08-21 10:17:01.510 INFO: Processing datasets for head 'Default'\n",
      "2025-08-21 10:17:21.318 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-08-21 10:17:30.254 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-08-21 10:17:30.254 INFO: Combined validation datasets for Default\n",
      "2025-08-21 10:17:30.254 INFO: Head 'Default' training dataset size: 1612\n",
      "2025-08-21 10:17:30.254 INFO: Processing datasets for head 'pt_head'\n",
      "2025-08-21 10:17:38.565 INFO: Combining 1 list datasets for head 'pt_head'\n",
      "2025-08-21 10:17:39.480 INFO: Head 'pt_head' training dataset size: 9000\n",
      "2025-08-21 10:17:39.480 INFO: Average number of neighbors: 61.964672446250916\n",
      "2025-08-21 10:17:39.480 INFO: During training the following quantities will be reported: energy, forces, stress\n",
      "2025-08-21 10:17:39.480 INFO: ===========MODEL DETAILS===========\n",
      "2025-08-21 10:17:46.266 INFO: Loading FOUNDATION model\n",
      "2025-08-21 10:17:46.267 INFO: Using filtered elements: [1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91, 92, 93, 94]\n",
      "2025-08-21 10:17:46.268 INFO: Model configuration extracted from foundation model\n",
      "2025-08-21 10:17:46.268 INFO: Using universal loss function for fine-tuning\n",
      "2025-08-21 10:17:46.268 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-08-21 10:17:46.268 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-08-21 10:17:46.268 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-08-21 10:17:46.268 INFO: Distance transform for radial basis functions: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/checkpoint.py:187: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-21 10:17:47.714 INFO: Total number of parameters: 5556810\n",
      "2025-08-21 10:17:47.714 INFO: \n",
      "2025-08-21 10:17:47.714 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-08-21 10:17:47.714 INFO: Using ADAM as parameter optimizer\n",
      "2025-08-21 10:17:47.714 INFO: Batch size: 2\n",
      "2025-08-21 10:17:47.714 INFO: Using Exponential Moving Average with decay: 0.99999\n",
      "2025-08-21 10:17:47.714 INFO: Number of gradient updates: 689780\n",
      "2025-08-21 10:17:47.714 INFO: Learning rate: 0.0001, weight decay: 1e-08\n",
      "2025-08-21 10:17:47.714 INFO: UniversalLoss(energy_weight=10.000, forces_weight=0.000, stress_weight=0.000)\n",
      "2025-08-21 10:17:47.725 WARNING: No SWA checkpoint found, while SWA is enabled. Compare the swa_start parameter and the latest checkpoint.\n",
      "2025-08-21 10:17:47.726 INFO: Loading checkpoint: ./checkpoints/mace_T2_including_replay_w2_run-84_epoch-59.pt\n",
      "2025-08-21 10:17:47.817 INFO: Using gradient clipping with tolerance=1.000\n",
      "2025-08-21 10:17:47.817 INFO: \n",
      "2025-08-21 10:17:47.817 INFO: ===========TRAINING===========\n",
      "2025-08-21 10:17:47.817 INFO: Started training, reporting errors on validation set\n",
      "2025-08-21 10:17:47.817 INFO: Loss metrics on validation set\n"
     ]
    }
   ],
   "source": [
    "# Now universal MACE finetuning on T2\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    cmd = [\n",
    "        \"mace_run_train\",\n",
    "        # === General settings ===\n",
    "        \"--name\",              \"mace_T2_including_replay_w2\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "    # --- MP replay (pretraining head) ---\n",
    "        \"--pt_train_file\",\"/home/phanim/harshitrawat/summer/replay_data/mp_finetuning-mace_T2_mp_replay_run-42.xyz\",              # <- MP replay shortcut\n",
    "        \"--atomic_numbers\",\"[3,8,40,57]\",    # Li, O, Zr, La\n",
    "        \"--multiheads_finetuning\",\"True\",\n",
    "\n",
    "        \"--train_file\",\"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz\",\n",
    "        \"--valid_file\",\"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz\",\n",
    "\n",
    "        \"--batch_size\",        \"2\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "\n",
    "        \"--device\",            \"cuda\",\n",
    "\n",
    "        # === Loss function weights ===\n",
    "        \"--forces_weight\",     \"0\",         # Increased force weight to balance energy better\n",
    "        \"--energy_weight\",     \"10\",   \n",
    "        \"--stress_weight\", \"0\",             # Reduced from 100 → avoid dominance + stabilize energy RMSE\n",
    "\n",
    "        # === Learning setup ===\n",
    "        \"--lr\",                \"0.006\",      # Explicit learning rate (0.0001 is too low → stagnation)\n",
    "        \"--scheduler_patience\",\"4\",          # Reduce LR if val loss doesn’t improve in 3 epochs\n",
    "        \"--clip_grad\",         \"1\",        # Avoid exploding gradients — essential when energy_weight is high\n",
    "        \"--weight_decay\",      \"1e-8\",       # Mild regularization to prevent overfitting\n",
    "\n",
    "        # === EMA helps smooth loss curve ===\n",
    "        #\"--ema_decay\",         \"0.999\",     # Smooths validation loss and helps final convergence\n",
    "\n",
    "        # === Domain + training settings ===\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"130\",\n",
    "        \"--E0s\",               \"{3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\",    # Still allowed — could optionally be replaced by manual E0s\n",
    "        \"--seed\",              \"84\",\n",
    "        \"--patience\",     \"8\",\n",
    "\n",
    "        \"--restart_latest\",                   # Resumes from checkpoint if available\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda62619-66cf-4057-a0bd-f54f8d36312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Keys renamed to REF_* and saved to:\n",
      "/home/phanim/harshitrawat/summer/replay_data/mp_finetuning-mace_T2_mp_replay_run-42_REFkeys.xyz\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd803b6-71b5-4fab-974f-2ea3ba19f232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: mace_run_train \\\n",
      "    --name \\\n",
      "    mace_T2_including_replay_w2_a \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --pt_train_file \\\n",
      "    /home/phanim/harshitrawat/summer/replay_data/mp_finetuning-just_to_get_file_combinations_run-84.xyz \\\n",
      "    --atomic_numbers \\\n",
      "    [3,8,40,57] \\\n",
      "    --multiheads_finetuning \\\n",
      "    True \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz \\\n",
      "    --batch_size \\\n",
      "    2 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --forces_weight \\\n",
      "    0 \\\n",
      "    --energy_weight \\\n",
      "    10 \\\n",
      "    --stress_weight \\\n",
      "    0 \\\n",
      "    --lr \\\n",
      "    0.006 \\\n",
      "    --scheduler_patience \\\n",
      "    4 \\\n",
      "    --clip_grad \\\n",
      "    1 \\\n",
      "    --weight_decay \\\n",
      "    1e-8 \\\n",
      "    --ema_decay \\\n",
      "    0.999 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    33 \\\n",
      "    --E0s \\\n",
      "    {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549} \\\n",
      "    --seed \\\n",
      "    84 \\\n",
      "    --patience \\\n",
      "    8 \\\n",
      "    --restart_latest\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-26 13:32:26.203 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-08-26 13:32:26.203 INFO: MACE version: 0.3.14\n",
      "2025-08-26 13:32:26.842 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py:152: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-26 13:32:27.331 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-08-26 13:32:27.339 INFO: Multihead finetuning mode, setting learning rate to 0.0001 and EMA to True. To use a different learning rate, set --force_mh_ft_lr=True.\n",
      "2025-08-26 13:32:27.339 INFO: Using multiheads finetuning mode, setting learning rate to 0.0001 and EMA to True\n",
      "2025-08-26 13:32:27.339 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-08-26 13:32:27.339 INFO: Using heads: ['Default', 'pt_head']\n",
      "2025-08-26 13:32:27.339 INFO: Using the key specifications to parse data:\n",
      "2025-08-26 13:32:27.339 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-26 13:32:27.339 INFO: pt_head: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-26 13:32:27.339 INFO: =============    Processing head Default     ===========\n",
      "2025-08-26 13:32:32.731 INFO: Training set 1/1 [energy: 1612, stress: 0, virials: 0, dipole components: 0, head: 1612, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1612, charges: 0]\n",
      "2025-08-26 13:32:32.737 INFO: Total Training set [energy: 1612, stress: 0, virials: 0, dipole components: 0, head: 1612, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1612, charges: 0]\n",
      "2025-08-26 13:32:35.071 INFO: Validation set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-08-26 13:32:35.073 INFO: Total Validation set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-08-26 13:32:35.073 INFO: Total number of configurations: train=1612, valid=705, tests=[],\n",
      "2025-08-26 13:32:35.073 INFO: =============    Processing head pt_head     ===========\n",
      "2025-08-26 13:32:37.556 WARNING: No forces found with key 'REF_forces' in '/home/phanim/harshitrawat/summer/replay_data/mp_finetuning-just_to_get_file_combinations_run-84.xyz'. If this is unexpected, Please change the key name in the command line arguments or ensure that the file contains the required data.\n",
      "2025-08-26 13:32:37.896 INFO: Training set 1/1 [energy: 10000, stress: 0, virials: 0, dipole components: 0, head: 10000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-26 13:32:37.925 INFO: Total Training set [energy: 10000, stress: 0, virials: 0, dipole components: 0, head: 10000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-26 13:32:37.925 INFO: No validation set provided, splitting training data instead.\n",
      "2025-08-26 13:32:37.929 INFO: Using random 10% of training set for validation with indices saved in: ./valid_indices_84.txt\n",
      "2025-08-26 13:32:37.959 INFO: Random Split Training set [energy: 9000, stress: 0, virials: 0, dipole components: 0, head: 9000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-26 13:32:37.962 INFO: Random Split Validation set [energy: 1000, stress: 0, virials: 0, dipole components: 0, head: 1000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-26 13:32:37.962 INFO: Total number of configurations: train=9000, valid=1000, tests=[],\n",
      "2025-08-26 13:32:37.962 INFO: ==================Using multiheads finetuning mode==================\n",
      "2025-08-26 13:32:37.962 INFO: Total number of configurations in pretraining: train=9000, valid=1000\n",
      "2025-08-26 13:32:37.962 INFO: Using atomic numbers from command line argument\n",
      "2025-08-26 13:32:37.980 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-08-26 13:32:37.980 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-08-26 13:32:37.983 INFO: Atomic Energies used (z: eV) for head Default: {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\n",
      "2025-08-26 13:32:37.983 INFO: Atomic Energies used (z: eV) for head pt_head: {3: -3.482100566595956, 8: -7.28459863421322, 40: -11.846857579882572, 57: -8.933684809576345}\n",
      "2025-08-26 13:32:37.983 INFO: Processing datasets for head 'Default'\n",
      "2025-08-26 13:32:57.108 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-08-26 13:33:05.634 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-08-26 13:33:05.635 INFO: Combined validation datasets for Default\n",
      "2025-08-26 13:33:05.635 INFO: Head 'Default' training dataset size: 1612\n",
      "2025-08-26 13:33:05.635 INFO: Processing datasets for head 'pt_head'\n",
      "2025-08-26 13:33:12.406 INFO: Combining 1 list datasets for head 'pt_head'\n",
      "2025-08-26 13:33:13.111 INFO: Head 'pt_head' training dataset size: 9000\n",
      "2025-08-26 13:33:13.111 INFO: Average number of neighbors: 61.964672446250916\n",
      "2025-08-26 13:33:13.112 INFO: During training the following quantities will be reported: energy, forces, stress\n",
      "2025-08-26 13:33:13.112 INFO: ===========MODEL DETAILS===========\n",
      "2025-08-26 13:33:18.937 WARNING: Standard deviation of the scaling is zero, Changing to no scaling\n",
      "2025-08-26 13:33:18.949 INFO: Loading FOUNDATION model\n",
      "2025-08-26 13:33:18.950 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-08-26 13:33:18.950 INFO: Model configuration extracted from foundation model\n",
      "2025-08-26 13:33:18.950 INFO: Using universal loss function for fine-tuning\n",
      "2025-08-26 13:33:18.950 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-08-26 13:33:18.950 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-08-26 13:33:18.950 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-08-26 13:33:18.950 INFO: Distance transform for radial basis functions: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/checkpoint.py:187: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-26 13:33:20.212 INFO: Total number of parameters: 896586\n",
      "2025-08-26 13:33:20.212 INFO: \n",
      "2025-08-26 13:33:20.212 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-08-26 13:33:20.212 INFO: Using ADAM as parameter optimizer\n",
      "2025-08-26 13:33:20.212 INFO: Batch size: 2\n",
      "2025-08-26 13:33:20.212 INFO: Using Exponential Moving Average with decay: 0.99999\n",
      "2025-08-26 13:33:20.212 INFO: Number of gradient updates: 175098\n",
      "2025-08-26 13:33:20.212 INFO: Learning rate: 0.0001, weight decay: 1e-08\n",
      "2025-08-26 13:33:20.212 INFO: UniversalLoss(energy_weight=10.000, forces_weight=0.000, stress_weight=0.000)\n",
      "2025-08-26 13:33:20.227 WARNING: No SWA checkpoint found, while SWA is enabled. Compare the swa_start parameter and the latest checkpoint.\n",
      "2025-08-26 13:33:20.228 INFO: Loading checkpoint: ./checkpoints/mace_T2_including_replay_w2_a_run-84_epoch-31.pt\n",
      "2025-08-26 13:33:20.692 INFO: Using gradient clipping with tolerance=1.000\n",
      "2025-08-26 13:33:20.692 INFO: \n",
      "2025-08-26 13:33:20.692 INFO: ===========TRAINING===========\n",
      "2025-08-26 13:33:20.692 INFO: Started training, reporting errors on validation set\n",
      "2025-08-26 13:33:20.692 INFO: Loss metrics on validation set\n",
      "2025-08-26 13:33:59.401 INFO: Initial: head: pt_head, loss=0.00000368, RMSE_E_per_atom=    0.86 meV, RMSE_F=None meV / A, RMSE_stress=None meV / A^3\n",
      "2025-08-26 13:35:24.825 INFO: Initial: head: Default, loss=0.00019247, RMSE_E_per_atom=    6.45 meV, RMSE_F= 1048.30 meV / A, RMSE_stress=None meV / A^3\n",
      "2025-08-26 13:48:35.139 INFO: Epoch 31: head: pt_head, loss=0.00000766, RMSE_E_per_atom=    1.24 meV, RMSE_F=None meV / A, RMSE_stress=None meV / A^3\n",
      "2025-08-26 13:50:01.139 INFO: Epoch 31: head: Default, loss=0.00019101, RMSE_E_per_atom=    6.42 meV, RMSE_F= 1043.97 meV / A, RMSE_stress=None meV / A^3\n",
      "2025-08-26 14:03:08.343 INFO: Epoch 32: head: pt_head, loss=0.00001028, RMSE_E_per_atom=    1.43 meV, RMSE_F=None meV / A, RMSE_stress=None meV / A^3\n",
      "2025-08-26 14:04:34.321 INFO: Epoch 32: head: Default, loss=0.00018869, RMSE_E_per_atom=    6.37 meV, RMSE_F= 1038.77 meV / A, RMSE_stress=None meV / A^3\n",
      "2025-08-26 14:04:34.465 INFO: Training complete\n",
      "2025-08-26 14:04:34.465 INFO: \n",
      "2025-08-26 14:04:34.465 INFO: ===========RESULTS===========\n",
      "2025-08-26 14:04:34.522 INFO: Loading checkpoint: ./checkpoints/mace_T2_including_replay_w2_a_run-84_epoch-32.pt\n",
      "2025-08-26 14:04:34.552 INFO: Loaded Stage one model from epoch 32 for evaluation\n",
      "2025-08-26 14:04:34.552 INFO: Saving model to checkpoints/mace_T2_including_replay_w2_a_run-84.model\n",
      "2025-08-26 14:04:34.826 INFO: Compiling model, saving metadata to mace_T2_including_replay_w2_a_compiled.model\n",
      "2025-08-26 14:04:35.812 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-08-26 14:04:35.813 INFO: Skipping evaluation for heads: ['pt_head']\n",
      "2025-08-26 14:04:35.813 INFO: Evaluating train_Default ...\n",
      "2025-08-26 14:07:24.512 INFO: Skipping evaluation of train_pt_head (in skip_heads list)\n",
      "2025-08-26 14:07:24.515 INFO: Evaluating valid_Default ...\n",
      "2025-08-26 14:08:50.810 INFO: Skipping evaluation of valid_pt_head (in skip_heads list)\n",
      "2025-08-26 14:08:50.986 INFO: Error-table on TRAIN and VALID:\n",
      "+---------------+---------------------+------------------+-------------------+---------------------------------------+\n",
      "|  config_type  | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % | RMSE Stress (Virials) / meV / A (A^3) |\n",
      "+---------------+---------------------+------------------+-------------------+---------------------------------------+\n",
      "| train_Default |            4.5      |       1042.0     |        300.22     |                  None                 |\n",
      "| valid_Default |            6.4      |       1038.8     |        302.23     |                  None                 |\n",
      "+---------------+---------------------+------------------+-------------------+---------------------------------------+\n",
      "2025-08-26 14:08:51.302 INFO: Done\n"
     ]
    }
   ],
   "source": [
    "# Now universal MACE finetuning on T2\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    cmd = [\n",
    "        \"mace_run_train\",\n",
    "        # === General settings ===\n",
    "        \"--name\",              \"mace_T2_including_replay_w2_a\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "    # --- MP replay (pretraining head) ---\n",
    "        \"--pt_train_file\",\"/home/phanim/harshitrawat/summer/replay_data/mp_finetuning-just_to_get_file_combinations_run-84.xyz\",              # <- MP replay shortcut\n",
    "        \"--atomic_numbers\",\"[3,8,40,57]\",    # Li, O, Zr, La\n",
    "        \"--multiheads_finetuning\",\"True\",\n",
    "\n",
    "        \"--train_file\",\"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz\",\n",
    "        \"--valid_file\",\"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz\",\n",
    "\n",
    "        \"--batch_size\",        \"2\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "\n",
    "        \"--device\",            \"cuda\",\n",
    "\n",
    "        # === Loss function weights ===\n",
    "        \"--forces_weight\",     \"0\",         # Increased force weight to balance energy better\n",
    "        \"--energy_weight\",     \"10\",   \n",
    "        \"--stress_weight\", \"0\",             # Reduced from 100 → avoid dominance + stabilize energy RMSE\n",
    "\n",
    "        # === Learning setup ===\n",
    "        \"--lr\",                \"0.006\",      # Explicit learning rate (0.0001 is too low → stagnation)\n",
    "        \"--scheduler_patience\",\"4\",          # Reduce LR if val loss doesn’t improve in 3 epochs\n",
    "        \"--clip_grad\",         \"1\",        # Avoid exploding gradients — essential when energy_weight is high\n",
    "        \"--weight_decay\",      \"1e-8\",       # Mild regularization to prevent overfitting\n",
    "\n",
    "        # === EMA helps smooth loss curve ===\n",
    "        \"--ema_decay\",         \"0.999\",     # Smooths validation loss and helps final convergence\n",
    "\n",
    "        # === Domain + training settings ===\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"33\",\n",
    "        \"--E0s\",               \"{3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\",    # Still allowed — could optionally be replaced by manual E0s\n",
    "        \"--seed\",              \"84\",\n",
    "        \"--patience\",     \"8\",\n",
    "\n",
    "        \"--restart_latest\",                   # Resumes from checkpoint if available\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eaa615d-2325-4419-80a0-3d680335b9fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "/home/phanim/harshitrawat/mace/mace/calculators/mace.py:166: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/torch/serialization.py:1328: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using head Default out of ['pt_head', 'Default']\n",
      "No dtype selected, switching to float64 to match model dtype.\n",
      "Li: μ_model = -1.775718 eV/atom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/pymatgen/core/structure.py:3107: UserWarning: Issues encountered while parsing CIF: 4 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La: μ_model = 18.380837 eV/atom\n",
      "Zr: μ_model = 26.286409 eV/atom\n",
      "O: μ_model = -20.756856 eV/atom\n"
     ]
    }
   ],
   "source": [
    "from mace.calculators import MACECalculator\n",
    "mace_calc = MACECalculator(model_paths=[\"/home/phanim/harshitrawat/summer/iteration_3/mace_T2_including_replay_w2_a_compiled.model\"], device=\"cuda\")  # or \"cpu\"\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "from pymatgen.core import Structure\n",
    "adaptor = AseAtomsAdaptor()\n",
    "\n",
    "pmg_structure = Structure.from_file(\"/home/phanim/harshitrawat/summer/formation_energy/cifs/Li.cif\")  # e.g. for Li\n",
    "ase_atoms = adaptor.get_atoms(pmg_structure)\n",
    "ase_atoms.calc = mace_calc\n",
    "total_energy = ase_atoms.get_potential_energy()\n",
    "mu_model_Li = total_energy / len(ase_atoms)\n",
    "print(f\"Li: μ_model = {mu_model_Li:.6f} eV/atom\")\n",
    "# Let us do this for La, Zr, and O as well\n",
    "pmg_structure = Structure.from_file(\"/home/phanim/harshitrawat/summer/formation_energy/cifs/La.cif\")\n",
    "ase_atoms = adaptor.get_atoms(pmg_structure)\n",
    "ase_atoms.calc = mace_calc\n",
    "total_energy = ase_atoms.get_potential_energy()\n",
    "mu_model_La = total_energy / len(ase_atoms)\n",
    "print(f\"La: μ_model = {mu_model_La:.6f} eV/atom\")\n",
    "pmg_structure = Structure.from_file(\"/home/phanim/harshitrawat/summer/formation_energy/cifs/Zr.cif\")\n",
    "ase_atoms = adaptor.get_atoms(pmg_structure)\n",
    "ase_atoms.calc = mace_calc\n",
    "total_energy = ase_atoms.get_potential_energy()\n",
    "mu_model_Zr = total_energy / len(ase_atoms)\n",
    "print(f\"Zr: μ_model = {mu_model_Zr:.6f} eV/atom\")\n",
    "pmg_structure = Structure.from_file(\"/home/phanim/harshitrawat/summer/formation_energy/cifs/O2.cif\")  # Needs to be a periodic solid O2 structure\n",
    "ase_atoms = adaptor.get_atoms(pmg_structure)\n",
    "ase_atoms.calc = mace_calc\n",
    "total_energy = ase_atoms.get_potential_energy()\n",
    "mu_model_O = total_energy / len(ase_atoms)\n",
    "print(f\"O: μ_model = {mu_model_O:.6f} eV/atom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbf7d5e-8bc8-4e2e-9b3f-45958225d488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/mace/mace/calculators/mace.py:166: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/torch/serialization.py:1328: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using head Default out of ['pt_head', 'Default']\n",
      "No dtype selected, switching to float64 to match model dtype.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/pymatgen/core/structure.py:3107: UserWarning: Issues encountered while parsing CIF: 12 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li2O2          :  E_form (MACE_T2_w2_it3) = -0.061559 eV/atom\n",
      "Li2O           :  E_form (MACE_T2_w2_it3) = -0.104004 eV/atom\n",
      "Li7La3Zr2O12   :  E_form (MACE_T2_w2_it3) = -1.424687 eV/atom\n",
      "ZrO2           :  E_form (MACE_T2_w2_it3) = -2.953763 eV/atom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/pymatgen/core/structure.py:3107: UserWarning: Issues encountered while parsing CIF: 8 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La2O3          :  E_form (MACE_T2_w2_it3) = -2.602734 eV/atom\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# ---- 1. Load the MACE model -------------------------------------------\n",
    "calculator = MACECalculator(model_paths=[\"/home/phanim/harshitrawat/summer/iteration_3/mace_T2_including_replay_w2_a_compiled.model\"], device=\"cuda\")  # or \"cpu\"\n",
    "\n",
    "# ---- 2. Reference μ_model from MACE -----------------------------------\n",
    "\n",
    "mu_mace = {\n",
    "    \"Li\": -1.775718,\n",
    "    \"La\": 18.380837,\n",
    "    \"Zr\": 26.286409,\n",
    "    \"O\":  -20.756856,\n",
    "}\n",
    "\n",
    "# ---- 3. CIF files ------------------------------------------------------\n",
    "cif_dir = \"/home/phanim/harshitrawat/summer/formation_energy/cifs\"\n",
    "compounds = {\n",
    "    \"mp-841.cif\": \"Li2O2\",\n",
    "    \"mp-1960.cif\": \"Li2O\",\n",
    "    \"mp-942733.cif\": \"Li7La3Zr2O12\",\n",
    "    \"mp-2858.cif\": \"ZrO2\",\n",
    "    \"mp-1968.cif\": \"La2O3\",\n",
    "}\n",
    "\n",
    "# ---- 4. Predict formation energy per atom -----------------------------\n",
    "for fname, label in compounds.items():\n",
    "    struct = Structure.from_file(os.path.join(cif_dir, fname))\n",
    "    comp = struct.composition\n",
    "    n_atoms = comp.num_atoms\n",
    "\n",
    "    # Convert to ASE\n",
    "    ase_atoms = AseAtomsAdaptor.get_atoms(struct)\n",
    "\n",
    "    # Assign calculator and predict energy\n",
    "    ase_atoms.calc = calculator\n",
    "    energy_total = ase_atoms.get_potential_energy()  # eV (total)\n",
    "\n",
    "    # Reference energy from MACE chemical potentials\n",
    "    ref_total = sum(comp[el] * mu_mace[el.symbol] for el in comp.elements)\n",
    "\n",
    "    # Formation energy per atom\n",
    "    e_form = (energy_total - ref_total) / n_atoms\n",
    "\n",
    "    print(f\"{label:15s}:  E_form (MACE_T2_w2_it3) = {e_form: .6f} eV/atom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a3f4b-8cc1-463f-bb7d-a3ce9825d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Li2O2          :  E_form (MACE_T2_w2_it3) = -0.061559 eV/atom\n",
    "Li2O           :  E_form (MACE_T2_w2_it3) = -0.104004 eV/atom\n",
    "Li7La3Zr2O12   :  E_form (MACE_T2_w2_it3) = -1.424687 eV/atom\n",
    "ZrO2           :  E_form (MACE_T2_w2_it3) = -2.953763 eV/atom\n",
    "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/pymatgen/core/structure.py:3107: UserWarning: Issues encountered while parsing CIF: 8 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
    "  struct = parser.parse_structures(primitive=primitive)[0]\n",
    "La2O3          :  E_form (MACE_T2_w2_it3) = -2.602734 eV/atom"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU mace_0.3.8)",
   "language": "python",
   "name": "mace_0.3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
