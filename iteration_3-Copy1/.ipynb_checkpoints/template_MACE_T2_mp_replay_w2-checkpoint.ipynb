{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74803bd4-0874-4a95-bcd2-702000a67339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: mace_run_train \\\n",
      "    --name \\\n",
      "    mace_T2_including_replay_w2 \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/iteration_3-Copy1/one.xyz \\\n",
      "    --valid_fraction \\\n",
      "    0.5 \\\n",
      "    --batch_size \\\n",
      "    1 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --device \\\n",
      "    cpu \\\n",
      "    --forces_weight \\\n",
      "    0 \\\n",
      "    --energy_weight \\\n",
      "    0 \\\n",
      "    --stress_weight \\\n",
      "    0 \\\n",
      "    --lr \\\n",
      "    0.006 \\\n",
      "    --scheduler_patience \\\n",
      "    4 \\\n",
      "    --clip_grad \\\n",
      "    1 \\\n",
      "    --weight_decay \\\n",
      "    1e-8 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    1 \\\n",
      "    --E0s \\\n",
      "    {3: -1.2302615750354944} \\\n",
      "    --seed \\\n",
      "    84 \\\n",
      "    --patience \\\n",
      "    8 \\\n",
      "    --restart_latest\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-21 20:19:52.743 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-08-21 20:19:52.743 INFO: MACE version: 0.3.14\n",
      "2025-08-21 20:19:52.743 INFO: Using CPU\n",
      "2025-08-21 20:19:52.864 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-08-21 20:19:52.864 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-08-21 20:19:52.864 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-08-21 20:19:52.864 INFO: Using heads: ['Default']\n",
      "2025-08-21 20:19:52.864 INFO: Using the key specifications to parse data:\n",
      "2025-08-21 20:19:52.864 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-21 20:19:52.864 INFO: =============    Processing head Default     ===========\n",
      "2025-08-21 20:19:52.869 WARNING: No forces found with key 'REF_forces' in '/home/phanim/harshitrawat/summer/iteration_3-Copy1/one.xyz'. If this is unexpected, Please change the key name in the command line arguments or ensure that the file contains the required data.\n",
      "2025-08-21 20:19:52.869 INFO: Training set 1/1 [energy: 2, stress: 0, virials: 0, dipole components: 0, head: 2, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-21 20:19:52.869 INFO: Total Training set [energy: 2, stress: 0, virials: 0, dipole components: 0, head: 2, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-21 20:19:52.869 INFO: No validation set provided, splitting training data instead.\n",
      "2025-08-21 20:19:52.869 INFO: Using random 50% of training set for validation with following indices: [0]\n",
      "2025-08-21 20:19:52.869 INFO: Random Split Training set [energy: 1, stress: 0, virials: 0, dipole components: 0, head: 1, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-21 20:19:52.869 INFO: Random Split Validation set [energy: 1, stress: 0, virials: 0, dipole components: 0, head: 1, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-21 20:19:52.869 INFO: Total number of configurations: train=1, valid=1, tests=[],\n",
      "2025-08-21 20:19:52.869 INFO: Atomic Numbers used: [3]\n",
      "2025-08-21 20:19:52.869 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-08-21 20:19:52.870 INFO: Atomic Energies used (z: eV) for head Default: {3: -1.2302615750354944}\n",
      "2025-08-21 20:19:52.870 INFO: Processing datasets for head 'Default'\n",
      "2025-08-21 20:19:52.870 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-08-21 20:19:52.871 INFO: Head 'Default' training dataset size: 1\n",
      "2025-08-21 20:19:52.871 INFO: Computing average number of neighbors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py:152: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-21 20:19:53.175 INFO: Average number of neighbors: nan\n",
      "2025-08-21 20:19:53.175 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-08-21 20:19:53.175 INFO: ===========MODEL DETAILS===========\n",
      "2025-08-21 20:19:53.177 WARNING: Standard deviation of the scaling is zero, Changing to no scaling\n",
      "2025-08-21 20:19:53.177 INFO: Loading FOUNDATION model\n",
      "2025-08-21 20:19:53.177 INFO: Using filtered elements: [3]\n",
      "2025-08-21 20:19:53.177 INFO: Model configuration extracted from foundation model\n",
      "2025-08-21 20:19:53.177 INFO: Using weighted loss function for fine-tuning\n",
      "2025-08-21 20:19:53.177 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-08-21 20:19:53.177 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-08-21 20:19:53.177 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-08-21 20:19:53.177 INFO: Distance transform for radial basis functions: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/checkpoint.py:187: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-21 20:20:04.057 INFO: Total number of parameters: 723866\n",
      "2025-08-21 20:20:04.057 INFO: \n",
      "2025-08-21 20:20:04.057 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-08-21 20:20:04.057 INFO: Using ADAM as parameter optimizer\n",
      "2025-08-21 20:20:04.057 INFO: Batch size: 1\n",
      "2025-08-21 20:20:04.057 INFO: Number of gradient updates: 1\n",
      "2025-08-21 20:20:04.057 INFO: Learning rate: 0.006, weight decay: 1e-08\n",
      "2025-08-21 20:20:04.057 INFO: WeightedEnergyForcesLoss(energy_weight=0.000, forces_weight=0.000)\n",
      "2025-08-21 20:20:04.059 WARNING: No SWA checkpoint found, while SWA is enabled. Compare the swa_start parameter and the latest checkpoint.\n",
      "2025-08-21 20:20:04.059 INFO: Loading checkpoint: ./checkpoints/mace_T2_including_replay_w2_run-84_epoch-77.pt\n",
      "2025-08-21 20:20:04.213 INFO: Using gradient clipping with tolerance=1.000\n",
      "2025-08-21 20:20:04.213 INFO: \n",
      "2025-08-21 20:20:04.213 INFO: ===========TRAINING===========\n",
      "2025-08-21 20:20:04.213 INFO: Started training, reporting errors on validation set\n",
      "2025-08-21 20:20:04.213 INFO: Loss metrics on validation set\n",
      "2025-08-21 20:20:04.829 INFO: Initial: head: Default, loss=0.00000000, RMSE_E_per_atom= 1384.88 meV, RMSE_F=None meV / A\n",
      "2025-08-21 20:20:07.407 INFO: Epoch 0: head: Default, loss=0.00000000, RMSE_E_per_atom= 1384.88 meV, RMSE_F=None meV / A\n",
      "2025-08-21 20:20:07.491 INFO: Training complete\n",
      "2025-08-21 20:20:07.491 INFO: \n",
      "2025-08-21 20:20:07.491 INFO: ===========RESULTS===========\n",
      "2025-08-21 20:20:07.491 INFO: Loading checkpoint: ./checkpoints/mace_T2_including_replay_w2_run-84_epoch-77.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/bin/mace_run_train\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m940\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    epoch = checkpoint_handler.load_latest(\n",
      "        state=tools.CheckpointState(model, optimizer, lr_scheduler),\n",
      "        swa=swa_eval,\n",
      "        device=device,\n",
      "    )\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/checkpoint.py\"\u001b[0m, line \u001b[35m215\u001b[0m, in \u001b[35mload_latest\u001b[0m\n",
      "    \u001b[31mself.builder.load_checkpoint\u001b[0m\u001b[1;31m(state=state, checkpoint=checkpoint, strict=strict)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/checkpoint.py\"\u001b[0m, line \u001b[35m40\u001b[0m, in \u001b[35mload_checkpoint\u001b[0m\n",
      "    \u001b[31mstate.model.load_state_dict\u001b[0m\u001b[1;31m(checkpoint[\"model\"], strict=strict)\u001b[0m  # type: ignore\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m2593\u001b[0m, in \u001b[35mload_state_dict\u001b[0m\n",
      "    raise RuntimeError(\n",
      "    ...<3 lines>...\n",
      "    )\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mError(s) in loading state_dict for ScaleShiftMACE:\n",
      "\tsize mismatch for atomic_numbers: copying a param with shape torch.Size([86]) from checkpoint, the shape in current model is torch.Size([1]).\n",
      "\tsize mismatch for node_embedding.linear.weight: copying a param with shape torch.Size([11008]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for atomic_energies_fn.atomic_energies: copying a param with shape torch.Size([2, 86]) from checkpoint, the shape in current model is torch.Size([1, 1]).\n",
      "\tsize mismatch for interactions.0.skip_tp.weight: copying a param with shape torch.Size([1409024]) from checkpoint, the shape in current model is torch.Size([16384]).\n",
      "\tsize mismatch for interactions.1.skip_tp.weight: copying a param with shape torch.Size([1409024]) from checkpoint, the shape in current model is torch.Size([16384]).\n",
      "\tsize mismatch for products.0.symmetric_contractions.contractions.0.weights_max: copying a param with shape torch.Size([86, 23, 128]) from checkpoint, the shape in current model is torch.Size([1, 23, 128]).\n",
      "\tsize mismatch for products.0.symmetric_contractions.contractions.0.weights.0: copying a param with shape torch.Size([86, 4, 128]) from checkpoint, the shape in current model is torch.Size([1, 4, 128]).\n",
      "\tsize mismatch for products.0.symmetric_contractions.contractions.0.weights.1: copying a param with shape torch.Size([86, 1, 128]) from checkpoint, the shape in current model is torch.Size([1, 1, 128]).\n",
      "\tsize mismatch for products.0.symmetric_contractions.contractions.1.weights_max: copying a param with shape torch.Size([86, 51, 128]) from checkpoint, the shape in current model is torch.Size([1, 51, 128]).\n",
      "\tsize mismatch for products.0.symmetric_contractions.contractions.1.weights.0: copying a param with shape torch.Size([86, 6, 128]) from checkpoint, the shape in current model is torch.Size([1, 6, 128]).\n",
      "\tsize mismatch for products.0.symmetric_contractions.contractions.1.weights.1: copying a param with shape torch.Size([86, 1, 128]) from checkpoint, the shape in current model is torch.Size([1, 1, 128]).\n",
      "\tsize mismatch for products.0.symmetric_contractions.contractions.2.weights_max: copying a param with shape torch.Size([86, 65, 128]) from checkpoint, the shape in current model is torch.Size([1, 65, 128]).\n",
      "\tsize mismatch for products.0.symmetric_contractions.contractions.2.weights.0: copying a param with shape torch.Size([86, 7, 128]) from checkpoint, the shape in current model is torch.Size([1, 7, 128]).\n",
      "\tsize mismatch for products.0.symmetric_contractions.contractions.2.weights.1: copying a param with shape torch.Size([86, 1, 128]) from checkpoint, the shape in current model is torch.Size([1, 1, 128]).\n",
      "\tsize mismatch for products.1.symmetric_contractions.contractions.0.weights_max: copying a param with shape torch.Size([86, 23, 128]) from checkpoint, the shape in current model is torch.Size([1, 23, 128]).\n",
      "\tsize mismatch for products.1.symmetric_contractions.contractions.0.weights.0: copying a param with shape torch.Size([86, 4, 128]) from checkpoint, the shape in current model is torch.Size([1, 4, 128]).\n",
      "\tsize mismatch for products.1.symmetric_contractions.contractions.0.weights.1: copying a param with shape torch.Size([86, 1, 128]) from checkpoint, the shape in current model is torch.Size([1, 1, 128]).\n",
      "\tsize mismatch for readouts.0.linear.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for readouts.0.linear.output_mask: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([1]).\n",
      "\tsize mismatch for readouts.1.linear_1.weight: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([2048]).\n",
      "\tsize mismatch for readouts.1.linear_1.output_mask: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n",
      "\tsize mismatch for readouts.1.linear_2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([16]).\n",
      "\tsize mismatch for readouts.1.linear_2.output_mask: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([1]).\n",
      "\tsize mismatch for scale_shift.scale: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([1]).\n",
      "\tsize mismatch for scale_shift.shift: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([1]).\u001b[0m\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['mace_run_train', '--name', 'mace_T2_including_replay_w2', '--model', 'MACE', '--num_interactions', '2', '--foundation_model', '/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model', '--foundation_model_readout', '--train_file', '/home/phanim/harshitrawat/summer/iteration_3-Copy1/one.xyz', '--valid_fraction', '0.5', '--batch_size', '1', '--valid_batch_size', '1', '--device', 'cpu', '--forces_weight', '0', '--energy_weight', '0', '--stress_weight', '0', '--lr', '0.006', '--scheduler_patience', '4', '--clip_grad', '1', '--weight_decay', '1e-8', '--r_max', '5.0', '--max_num_epochs', '1', '--E0s', '{3: -1.2302615750354944}', '--seed', '84', '--patience', '8', '--restart_latest']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mrun(cmd, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 56\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m cmd \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmace_run_train\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# === General settings ===\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--restart_latest\u001b[39m\u001b[38;5;124m\"\u001b[39m,                   \u001b[38;5;66;03m# Resumes from checkpoint if available\u001b[39;00m\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cmd), file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m---> 56\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mace_0.3.8/lib/python3.10/subprocess.py:526\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 526\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    527\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['mace_run_train', '--name', 'mace_T2_including_replay_w2', '--model', 'MACE', '--num_interactions', '2', '--foundation_model', '/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model', '--foundation_model_readout', '--train_file', '/home/phanim/harshitrawat/summer/iteration_3-Copy1/one.xyz', '--valid_fraction', '0.5', '--batch_size', '1', '--valid_batch_size', '1', '--device', 'cpu', '--forces_weight', '0', '--energy_weight', '0', '--stress_weight', '0', '--lr', '0.006', '--scheduler_patience', '4', '--clip_grad', '1', '--weight_decay', '1e-8', '--r_max', '5.0', '--max_num_epochs', '1', '--E0s', '{3: -1.2302615750354944}', '--seed', '84', '--patience', '8', '--restart_latest']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# Now universal MACE finetuning on T2\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    cmd = [\n",
    "        \"mace_run_train\",\n",
    "        # === General settings ===\n",
    "        \"--name\",              \"mace_T2_including_replay_w2\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "    # --- MP replay (pretraining head) ---\n",
    "        \"--pt_train_file\",\"/home/phanim/harshitrawat/summer/replay_data/mp_finetuning-mace_T2_mp_replay_run-42.xyz\",              # <- MP replay shortcut\n",
    "        \"--atomic_numbers\",\"[3,8,40,57]\",    # Li, O, Zr, La\n",
    "        \"--multiheads_finetuning\",\"True\",\n",
    "\n",
    "        \"--train_file\",\"/home/phanim/harshitrawat/summer/iteration_3-Copy1/one.xyz\",\n",
    "        \"--valid_fraction\", \"0.5\",\n",
    "        \"--batch_size\",        \"1\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "\n",
    "        \"--device\",            \"cpu\",\n",
    "\n",
    "        # === Loss function weights ===\n",
    "        \"--forces_weight\",     \"0\",         # Increased force weight to balance energy better\n",
    "        \"--energy_weight\",     \"0\",   \n",
    "        \"--stress_weight\", \"0\",             # Reduced from 100 → avoid dominance + stabilize energy RMSE\n",
    "\n",
    "        # === Learning setup ===\n",
    "        \"--lr\",                \"0.006\",      # Explicit learning rate (0.0001 is too low → stagnation)\n",
    "        \"--scheduler_patience\",\"4\",          # Reduce LR if val loss doesn’t improve in 3 epochs\n",
    "        \"--clip_grad\",         \"1\",        # Avoid exploding gradients — essential when energy_weight is high\n",
    "        \"--weight_decay\",      \"1e-8\",       # Mild regularization to prevent overfitting\n",
    "\n",
    "        # === EMA helps smooth loss curve ===\n",
    "        #\"--ema_decay\",         \"0.999\",     # Smooths validation loss and helps final convergence\n",
    "\n",
    "        # === Domain + training settings ===\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"1\",\n",
    "        \"--E0s\",               \"average\",    # Still allowed — could optionally be replaced by manual E0s\n",
    "        \"--seed\",              \"84\",\n",
    "        \"--patience\",     \"8\",\n",
    "\n",
    "        \"--restart_latest\",                   # Resumes from checkpoint if available\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda62619-66cf-4057-a0bd-f54f8d36312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Keys renamed to REF_* and saved to:\n",
      "/home/phanim/harshitrawat/summer/replay_data/mp_finetuning-mace_T2_mp_replay_run-42_REFkeys.xyz\n"
     ]
    }
   ],
   "source": [
    "# Now universal MACE finetuning on T2\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    cmd = [\n",
    "        \"mace_run_train\",\n",
    "        # === General settings ===\n",
    "        \"--name\",              \"template_mace_T2_including_replay_w2\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "    # --- MP replay (pretraining head) ---\n",
    "        # \"--pt_train_file\",\"/home/phanim/harshitrawat/summer/replay_data/mp_finetuning-mace_T2_mp_replay_run-42.xyz\",              # <- MP replay shortcut\n",
    "        # \"--atomic_numbers\",\"[3,8,40,57]\",    # Li, O, Zr, La\n",
    "        # \"--multiheads_finetuning\",\"True\",\n",
    "\n",
    "        \"--train_file\",\"/home/phanim/harshitrawat/summer/iteration_3-Copy1/one.xyz\",\n",
    "        \"--valid_fraction\", \"0.5\",\n",
    "        \"--batch_size\",        \"1\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "\n",
    "        \"--device\",            \"cpu\",\n",
    "\n",
    "        # === Loss function weights ===\n",
    "        \"--forces_weight\",     \"0\",         # Increased force weight to balance energy better\n",
    "        \"--energy_weight\",     \"10\",   \n",
    "        \"--stress_weight\", \"0\",             # Reduced from 100 → avoid dominance + stabilize energy RMSE\n",
    "\n",
    "        # === Learning setup ===\n",
    "        \"--lr\",                \"0.006\",      # Explicit learning rate (0.0001 is too low → stagnation)\n",
    "        \"--scheduler_patience\",\"4\",          # Reduce LR if val loss doesn’t improve in 3 epochs\n",
    "        \"--clip_grad\",         \"1\",        # Avoid exploding gradients — essential when energy_weight is high\n",
    "        \"--weight_decay\",      \"1e-8\",       # Mild regularization to prevent overfitting\n",
    "\n",
    "        # === EMA helps smooth loss curve ===\n",
    "        #\"--ema_decay\",         \"0.999\",     # Smooths validation loss and helps final convergence\n",
    "\n",
    "        # === Domain + training settings ===\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"1\",\n",
    "        \"--E0s\",               \"average\",    # Still allowed — could optionally be replaced by manual E0s\n",
    "        \"--seed\",              \"84\",\n",
    "        \"--patience\",     \"8\",\n",
    "\n",
    "        \"--restart_latest\",                   # Resumes from checkpoint if available\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd803b6-71b5-4fab-974f-2ea3ba19f232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: mace_run_train \\\n",
      "    --name \\\n",
      "    dummy_mace_T2_including_replay_w2 \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --pt_train_file \\\n",
      "    /home/phanim/harshitrawat/summer/replay_data/mp_finetuning-mace_T2_mp_replay_run-42.xyz \\\n",
      "    --atomic_numbers \\\n",
      "    [3,8,40,57] \\\n",
      "    --multiheads_finetuning \\\n",
      "    True \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/dummy.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/dummy.extxyz \\\n",
      "    --batch_size \\\n",
      "    2 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --forces_weight \\\n",
      "    0 \\\n",
      "    --energy_weight \\\n",
      "    10 \\\n",
      "    --stress_weight \\\n",
      "    0 \\\n",
      "    --lr \\\n",
      "    0.006 \\\n",
      "    --scheduler_patience \\\n",
      "    4 \\\n",
      "    --clip_grad \\\n",
      "    1 \\\n",
      "    --weight_decay \\\n",
      "    1e-8 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    1 \\\n",
      "    --E0s \\\n",
      "    {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549} \\\n",
      "    --seed \\\n",
      "    84 \\\n",
      "    --patience \\\n",
      "    8 \\\n",
      "    --restart_latest\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-22 03:17:46.609 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-08-22 03:17:46.609 INFO: MACE version: 0.3.14\n",
      "2025-08-22 03:17:46.655 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py:152: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-22 03:17:47.240 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-08-22 03:17:47.241 INFO: Multihead finetuning mode, setting learning rate to 0.0001 and EMA to True. To use a different learning rate, set --force_mh_ft_lr=True.\n",
      "2025-08-22 03:17:47.241 INFO: Using multiheads finetuning mode, setting learning rate to 0.0001 and EMA to True\n",
      "2025-08-22 03:17:47.241 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-08-22 03:17:47.241 INFO: Using heads: ['Default', 'pt_head']\n",
      "2025-08-22 03:17:47.241 INFO: Using the key specifications to parse data:\n",
      "2025-08-22 03:17:47.241 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-22 03:17:47.241 INFO: pt_head: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-22 03:17:47.241 INFO: =============    Processing head Default     ===========\n",
      "2025-08-22 03:17:47.251 WARNING: No forces found with key 'REF_forces' in '/home/phanim/harshitrawat/summer/dummy.extxyz'. If this is unexpected, Please change the key name in the command line arguments or ensure that the file contains the required data.\n",
      "2025-08-22 03:17:47.252 INFO: Training set 1/1 [energy: 2, stress: 0, virials: 0, dipole components: 0, head: 2, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-22 03:17:47.252 INFO: Total Training set [energy: 2, stress: 0, virials: 0, dipole components: 0, head: 2, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-22 03:17:47.253 WARNING: No forces found with key 'REF_forces' in '/home/phanim/harshitrawat/summer/dummy.extxyz'. If this is unexpected, Please change the key name in the command line arguments or ensure that the file contains the required data.\n",
      "2025-08-22 03:17:47.253 INFO: Validation set 1/1 [energy: 2, stress: 0, virials: 0, dipole components: 0, head: 2, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-22 03:17:47.253 INFO: Total Validation set [energy: 2, stress: 0, virials: 0, dipole components: 0, head: 2, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-22 03:17:47.253 INFO: Total number of configurations: train=2, valid=2, tests=[],\n",
      "2025-08-22 03:17:47.253 INFO: =============    Processing head pt_head     ===========\n",
      "2025-08-22 03:17:50.089 INFO: Training set 1/1 [energy: 10000, stress: 10000, virials: 0, dipole components: 0, head: 10000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 10000, charges: 0]\n",
      "2025-08-22 03:17:50.115 INFO: Total Training set [energy: 10000, stress: 10000, virials: 0, dipole components: 0, head: 10000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 10000, charges: 0]\n",
      "2025-08-22 03:17:50.115 INFO: No validation set provided, splitting training data instead.\n",
      "2025-08-22 03:17:50.117 INFO: Using random 10% of training set for validation with indices saved in: ./valid_indices_84.txt\n",
      "2025-08-22 03:17:50.143 INFO: Random Split Training set [energy: 9000, stress: 9000, virials: 0, dipole components: 0, head: 9000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 9000, charges: 0]\n",
      "2025-08-22 03:17:50.145 INFO: Random Split Validation set [energy: 1000, stress: 1000, virials: 0, dipole components: 0, head: 1000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1000, charges: 0]\n",
      "2025-08-22 03:17:50.145 INFO: Total number of configurations: train=9000, valid=1000, tests=[],\n",
      "2025-08-22 03:17:50.146 INFO: ==================Using multiheads finetuning mode==================\n",
      "2025-08-22 03:17:50.146 WARNING: Ratio of the number of configurations in the training set and the in the pt_train_file is 0.00022222222222222223, increasing the number of configurations in the fine-tuning heads by 450\n",
      "2025-08-22 03:17:50.146 INFO: Total number of configurations in pretraining: train=9000, valid=1000\n",
      "2025-08-22 03:17:50.146 INFO: Using atomic numbers from command line argument\n",
      "2025-08-22 03:17:50.167 INFO: Atomic Numbers used: [1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91, 92, 93, 94]\n",
      "2025-08-22 03:17:50.167 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-08-22 03:17:50.171 INFO: Atomic Energies used (z: eV) for head Default: {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\n",
      "2025-08-22 03:17:50.171 INFO: Atomic Energies used (z: eV) for head pt_head: {1: -3.667168021358939, 3: -3.482100566595956, 4: -4.736697230897597, 5: -7.724935420523256, 6: -8.405573550273285, 7: -7.360100452662763, 8: -7.28459863421322, 9: -4.896490881731322, 11: -2.7593613569762425, 12: -2.814047612069227, 13: -4.846881245288104, 14: -7.694793133351899, 15: -6.9632957911820235, 16: -4.672630400190884, 17: -2.8116892814008096, 19: -2.6176454856894793, 20: -5.390461060484104, 21: -7.8857952163517675, 22: -10.268392986214433, 23: -8.665147785496703, 24: -9.233050763772013, 25: -8.304951520770791, 26: -7.0489865771593765, 27: -5.577439766222147, 28: -5.172747618813715, 29: -3.2520726958619472, 30: -1.2901611618726314, 31: -3.527082192997912, 32: -4.70845955030298, 33: -3.9765109025623238, 34: -3.886231055836541, 35: -2.5184940099633986, 36: 6.766947645687137, 37: -2.5634958965928316, 38: -4.938005211501922, 39: -10.149818838085771, 40: -11.846857579882572, 41: -12.138896361658485, 42: -8.791678800595722, 43: -8.78694939675911, 44: -7.78093221529871, 45: -6.850021409115055, 46: -4.891019073240479, 47: -2.0634296773864045, 48: -0.6395695518943755, 49: -2.7887442084286693, 50: -3.818604275441892, 51: -3.587068329278862, 52: -2.8804045971118897, 53: -1.6355986842433357, 54: 9.846723842807721, 55: -2.765284507132287, 56: -4.990956432167774, 57: -8.933684809576345, 58: -8.735591176647514, 59: -8.018966025544966, 60: -8.251491970213372, 61: -7.591719594359237, 62: -8.169659881166858, 63: -13.592664636171698, 64: -18.517523458456985, 65: -7.647396572993602, 66: -8.122981037851925, 67: -7.607787319678067, 68: -6.85029094445494, 69: -7.8268821327130365, 70: -3.584786591677161, 71: -7.455406192077973, 72: -12.796283502572146, 73: -14.108127281277586, 74: -9.354916969477486, 75: -11.387537567890853, 76: -9.621909492152557, 77: -7.324393429417677, 78: -5.3046964808341945, 79: -2.380092582080244, 80: 0.24948924158195362, 81: -2.3239789120665026, 82: -3.730042357127322, 83: -3.438792347649683, 89: -5.062878214511315, 90: -11.02462566385297, 91: -12.265613551943261, 92: -13.855648206100362, 93: -14.933092020258243, 94: -15.282826131998245}\n",
      "2025-08-22 03:17:50.171 INFO: Processing datasets for head 'Default'\n",
      "2025-08-22 03:17:50.307 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-08-22 03:17:50.307 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-08-22 03:17:50.307 INFO: Combined validation datasets for Default\n",
      "2025-08-22 03:17:50.307 INFO: Head 'Default' training dataset size: 902\n",
      "2025-08-22 03:17:50.308 INFO: Processing datasets for head 'pt_head'\n",
      "2025-08-22 03:17:58.117 INFO: Combining 1 list datasets for head 'pt_head'\n",
      "2025-08-22 03:17:58.965 INFO: Head 'pt_head' training dataset size: 9000\n",
      "2025-08-22 03:17:58.966 INFO: Average number of neighbors: 61.964672446250916\n",
      "2025-08-22 03:17:58.966 INFO: During training the following quantities will be reported: energy, forces, stress\n",
      "2025-08-22 03:17:58.966 INFO: ===========MODEL DETAILS===========\n",
      "2025-08-22 03:18:03.704 WARNING: Standard deviation of the scaling is zero, Changing to no scaling\n",
      "2025-08-22 03:18:03.712 INFO: Loading FOUNDATION model\n",
      "2025-08-22 03:18:03.713 INFO: Using filtered elements: [1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91, 92, 93, 94]\n",
      "2025-08-22 03:18:03.713 INFO: Model configuration extracted from foundation model\n",
      "2025-08-22 03:18:03.713 INFO: Using universal loss function for fine-tuning\n",
      "2025-08-22 03:18:03.714 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-08-22 03:18:03.714 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-08-22 03:18:03.714 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-08-22 03:18:03.714 INFO: Distance transform for radial basis functions: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-22 03:18:13.041 INFO: Total number of parameters: 5556810\n",
      "2025-08-22 03:18:13.042 INFO: \n",
      "2025-08-22 03:18:13.042 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-08-22 03:18:13.042 INFO: Using ADAM as parameter optimizer\n",
      "2025-08-22 03:18:13.042 INFO: Batch size: 2\n",
      "2025-08-22 03:18:13.042 INFO: Using Exponential Moving Average with decay: 0.99999\n",
      "2025-08-22 03:18:13.042 INFO: Number of gradient updates: 4951\n",
      "2025-08-22 03:18:13.042 INFO: Learning rate: 0.0001, weight decay: 1e-08\n",
      "2025-08-22 03:18:13.042 INFO: UniversalLoss(energy_weight=10.000, forces_weight=0.000, stress_weight=0.000)\n",
      "2025-08-22 03:18:13.046 WARNING: Cannot find checkpoint with tag 'dummy_mace_T2_including_replay_w2_run-84' in './checkpoints'\n",
      "2025-08-22 03:18:13.047 INFO: Using gradient clipping with tolerance=1.000\n",
      "2025-08-22 03:18:13.047 INFO: \n",
      "2025-08-22 03:18:13.047 INFO: ===========TRAINING===========\n",
      "2025-08-22 03:18:13.047 INFO: Started training, reporting errors on validation set\n",
      "2025-08-22 03:18:13.047 INFO: Loss metrics on validation set\n",
      "2025-08-22 03:20:46.748 INFO: Initial: head: pt_head, loss=0.03504716, RMSE_E_per_atom=  468.57 meV, RMSE_F=   99.92 meV / A, RMSE_stress=    6.45 meV / A^3\n",
      "2025-08-22 03:20:46.979 INFO: Initial: head: Default, loss=0.99056127, RMSE_E_per_atom= 9910.61 meV, RMSE_F=None meV / A, RMSE_stress=None meV / A^3\n",
      "2025-08-22 03:39:39.815 INFO: Epoch 0: head: pt_head, loss=0.00287404, RMSE_E_per_atom=   60.53 meV, RMSE_F=  277.05 meV / A, RMSE_stress=   15.03 meV / A^3\n",
      "2025-08-22 03:39:40.013 INFO: Epoch 0: head: Default, loss=0.00011023, RMSE_E_per_atom=    4.70 meV, RMSE_F=None meV / A, RMSE_stress=None meV / A^3\n",
      "2025-08-22 03:39:40.328 INFO: Training complete\n",
      "2025-08-22 03:39:40.328 INFO: \n",
      "2025-08-22 03:39:40.329 INFO: ===========RESULTS===========\n",
      "2025-08-22 03:39:40.331 INFO: Loading checkpoint: ./checkpoints/dummy_mace_T2_including_replay_w2_run-84_epoch-0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/checkpoint.py:187: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-22 03:39:40.823 INFO: Loaded Stage one model from epoch 0 for evaluation\n",
      "2025-08-22 03:39:40.823 INFO: Saving model to checkpoints/dummy_mace_T2_including_replay_w2_run-84.model\n",
      "2025-08-22 03:39:41.158 INFO: Compiling model, saving metadata to dummy_mace_T2_including_replay_w2_compiled.model\n",
      "2025-08-22 03:39:41.776 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-08-22 03:39:41.777 INFO: Skipping evaluation for heads: ['pt_head']\n",
      "2025-08-22 03:39:41.777 INFO: Evaluating train_Default ...\n",
      "2025-08-22 03:40:31.165 INFO: Skipping evaluation of train_pt_head (in skip_heads list)\n",
      "2025-08-22 03:40:31.165 INFO: Evaluating valid_Default ...\n",
      "2025-08-22 03:40:31.366 INFO: Skipping evaluation of valid_pt_head (in skip_heads list)\n",
      "2025-08-22 03:40:31.374 INFO: Error-table on TRAIN and VALID:\n",
      "+---------------+---------------------+------------------+-------------------+---------------------------------------+\n",
      "|  config_type  | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % | RMSE Stress (Virials) / meV / A (A^3) |\n",
      "+---------------+---------------------+------------------+-------------------+---------------------------------------+\n",
      "| train_Default |            4.7      |       None       |        None       |                  None                 |\n",
      "| valid_Default |            4.7      |       None       |        None       |                  None                 |\n",
      "+---------------+---------------------+------------------+-------------------+---------------------------------------+\n",
      "2025-08-22 03:40:31.452 INFO: Done\n"
     ]
    }
   ],
   "source": [
    "# Now universal MACE finetuning on T2\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    cmd = [\n",
    "        \"mace_run_train\",\n",
    "        # === General settings ===\n",
    "        \"--name\",              \"dummy_mace_T2_including_replay_w2\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "    # --- MP replay (pretraining head) ---\n",
    "        \"--pt_train_file\",\"/home/phanim/harshitrawat/summer/replay_data/mp_finetuning-mace_T2_mp_replay_run-42.xyz\",              # <- MP replay shortcut\n",
    "        \"--atomic_numbers\",\"[3,8,40,57]\",    # Li, O, Zr, La\n",
    "        \"--multiheads_finetuning\",\"True\",\n",
    "\n",
    "        \"--train_file\", \"/home/phanim/harshitrawat/summer/dummy.extxyz\",\n",
    "        \"--valid_file\", \"/home/phanim/harshitrawat/summer/dummy.extxyz\",\n",
    "\n",
    "        \"--batch_size\",        \"2\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "\n",
    "        \"--device\",            \"cuda\",\n",
    "\n",
    "        # === Loss function weights ===\n",
    "        \"--forces_weight\",     \"0\",         # Increased force weight to balance energy better\n",
    "        \"--energy_weight\",     \"10\",   \n",
    "        \"--stress_weight\", \"0\",             # Reduced from 100 → avoid dominance + stabilize energy RMSE\n",
    "\n",
    "        # === Learning setup ===\n",
    "        \"--lr\",                \"0.006\",      # Explicit learning rate (0.0001 is too low → stagnation)\n",
    "        \"--scheduler_patience\",\"4\",          # Reduce LR if val loss doesn’t improve in 3 epochs\n",
    "        \"--clip_grad\",         \"1\",        # Avoid exploding gradients — essential when energy_weight is high\n",
    "        \"--weight_decay\",      \"1e-8\",       # Mild regularization to prevent overfitting\n",
    "\n",
    "        # === EMA helps smooth loss curve ===\n",
    "        #\"--ema_decay\",         \"0.999\",     # Smooths validation loss and helps final convergence\n",
    "\n",
    "        # === Domain + training settings ===\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"1\",\n",
    "        \"--E0s\",               \"{3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\",    # Still allowed — could optionally be replaced by manual E0s\n",
    "        \"--seed\",              \"84\",\n",
    "        \"--patience\",     \"8\",\n",
    "\n",
    "        \"--restart_latest\",                   # Resumes from checkpoint if available\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b829b8-3b87-4b55-ba09-4c5052317f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: mace_run_train \\\n",
      "    --name \\\n",
      "    mace_T2_including_replay_w2_just_142 \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --pt_train_file \\\n",
      "    /home/phanim/harshitrawat/summer/replay_data/mp_finetuning-just_to_get_file_comb_142_run-84.xyz \\\n",
      "    --atomic_numbers \\\n",
      "    [3,8,40,57] \\\n",
      "    --multiheads_finetuning \\\n",
      "    True \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz \\\n",
      "    --batch_size \\\n",
      "    2 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --forces_weight \\\n",
      "    0 \\\n",
      "    --energy_weight \\\n",
      "    10 \\\n",
      "    --stress_weight \\\n",
      "    0 \\\n",
      "    --lr \\\n",
      "    0.06 \\\n",
      "    --scheduler_patience \\\n",
      "    4 \\\n",
      "    --clip_grad \\\n",
      "    1 \\\n",
      "    --weight_decay \\\n",
      "    1e-8 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    50 \\\n",
      "    --E0s \\\n",
      "    {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549} \\\n",
      "    --seed \\\n",
      "    84 \\\n",
      "    --patience \\\n",
      "    8 \\\n",
      "    --restart_latest\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-26 04:52:27.829 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-08-26 04:52:27.829 INFO: MACE version: 0.3.14\n",
      "2025-08-26 04:52:28.367 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py:152: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-26 04:52:28.889 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-08-26 04:52:28.890 INFO: Multihead finetuning mode, setting learning rate to 0.0001 and EMA to True. To use a different learning rate, set --force_mh_ft_lr=True.\n",
      "2025-08-26 04:52:28.890 INFO: Using multiheads finetuning mode, setting learning rate to 0.0001 and EMA to True\n",
      "2025-08-26 04:52:28.890 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-08-26 04:52:28.890 INFO: Using heads: ['Default', 'pt_head']\n",
      "2025-08-26 04:52:28.890 INFO: Using the key specifications to parse data:\n",
      "2025-08-26 04:52:28.890 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-26 04:52:28.890 INFO: pt_head: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-26 04:52:28.890 INFO: =============    Processing head Default     ===========\n",
      "2025-08-26 04:52:34.299 INFO: Training set 1/1 [energy: 1612, stress: 0, virials: 0, dipole components: 0, head: 1612, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1612, charges: 0]\n",
      "2025-08-26 04:52:34.303 INFO: Total Training set [energy: 1612, stress: 0, virials: 0, dipole components: 0, head: 1612, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1612, charges: 0]\n",
      "2025-08-26 04:52:36.631 INFO: Validation set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-08-26 04:52:36.633 INFO: Total Validation set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-08-26 04:52:36.633 INFO: Total number of configurations: train=1612, valid=705, tests=[],\n",
      "2025-08-26 04:52:36.633 INFO: =============    Processing head pt_head     ===========\n",
      "2025-08-26 04:52:36.680 WARNING: No forces found with key 'REF_forces' in '/home/phanim/harshitrawat/summer/replay_data/mp_finetuning-just_to_get_file_comb_142_run-84.xyz'. If this is unexpected, Please change the key name in the command line arguments or ensure that the file contains the required data.\n",
      "2025-08-26 04:52:36.682 INFO: Training set 1/1 [energy: 142, stress: 0, virials: 0, dipole components: 0, head: 142, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-26 04:52:36.683 INFO: Total Training set [energy: 142, stress: 0, virials: 0, dipole components: 0, head: 142, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-26 04:52:36.683 INFO: No validation set provided, splitting training data instead.\n",
      "2025-08-26 04:52:36.686 INFO: Using random 10% of training set for validation with indices saved in: ./valid_indices_84.txt\n",
      "2025-08-26 04:52:36.686 INFO: Random Split Training set [energy: 128, stress: 0, virials: 0, dipole components: 0, head: 128, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-26 04:52:36.687 INFO: Random Split Validation set [energy: 14, stress: 0, virials: 0, dipole components: 0, head: 14, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-26 04:52:36.687 INFO: Total number of configurations: train=128, valid=14, tests=[],\n",
      "2025-08-26 04:52:36.687 INFO: ==================Using multiheads finetuning mode==================\n",
      "2025-08-26 04:52:36.687 INFO: Total number of configurations in pretraining: train=128, valid=14\n",
      "2025-08-26 04:52:36.687 INFO: Using atomic numbers from command line argument\n",
      "2025-08-26 04:52:36.687 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-08-26 04:52:36.687 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-08-26 04:52:36.689 INFO: Atomic Energies used (z: eV) for head Default: {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\n",
      "2025-08-26 04:52:36.690 INFO: Atomic Energies used (z: eV) for head pt_head: {3: -3.482100566595956, 8: -7.28459863421322, 40: -11.846857579882572, 57: -8.933684809576345}\n",
      "2025-08-26 04:52:36.690 INFO: Processing datasets for head 'Default'\n",
      "2025-08-26 04:52:55.797 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-08-26 04:53:04.419 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-08-26 04:53:04.420 INFO: Combined validation datasets for Default\n",
      "2025-08-26 04:53:04.420 INFO: Head 'Default' training dataset size: 1612\n",
      "2025-08-26 04:53:04.420 INFO: Processing datasets for head 'pt_head'\n",
      "2025-08-26 04:53:04.521 INFO: Combining 1 list datasets for head 'pt_head'\n",
      "2025-08-26 04:53:04.528 INFO: Head 'pt_head' training dataset size: 128\n",
      "2025-08-26 04:53:04.529 INFO: Average number of neighbors: 61.964672446250916\n",
      "2025-08-26 04:53:04.529 INFO: During training the following quantities will be reported: energy, forces, stress\n",
      "2025-08-26 04:53:04.529 INFO: ===========MODEL DETAILS===========\n",
      "2025-08-26 04:53:06.070 WARNING: Standard deviation of the scaling is zero, Changing to no scaling\n",
      "2025-08-26 04:53:06.072 INFO: Loading FOUNDATION model\n",
      "2025-08-26 04:53:06.073 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-08-26 04:53:06.073 INFO: Model configuration extracted from foundation model\n",
      "2025-08-26 04:53:06.073 INFO: Using universal loss function for fine-tuning\n",
      "2025-08-26 04:53:06.073 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-08-26 04:53:06.073 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-08-26 04:53:06.073 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-08-26 04:53:06.073 INFO: Distance transform for radial basis functions: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/checkpoint.py:187: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-26 04:53:07.368 INFO: Total number of parameters: 896586\n",
      "2025-08-26 04:53:07.369 INFO: \n",
      "2025-08-26 04:53:07.369 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-08-26 04:53:07.369 INFO: Using ADAM as parameter optimizer\n",
      "2025-08-26 04:53:07.369 INFO: Batch size: 2\n",
      "2025-08-26 04:53:07.369 INFO: Using Exponential Moving Average with decay: 0.99999\n",
      "2025-08-26 04:53:07.369 INFO: Number of gradient updates: 43500\n",
      "2025-08-26 04:53:07.369 INFO: Learning rate: 0.0001, weight decay: 1e-08\n",
      "2025-08-26 04:53:07.369 INFO: UniversalLoss(energy_weight=10.000, forces_weight=0.000, stress_weight=0.000)\n",
      "2025-08-26 04:53:07.372 WARNING: No SWA checkpoint found, while SWA is enabled. Compare the swa_start parameter and the latest checkpoint.\n",
      "2025-08-26 04:53:07.372 INFO: Loading checkpoint: ./checkpoints/mace_T2_including_replay_w2_just_142_run-84_epoch-0.pt\n",
      "2025-08-26 04:53:07.431 INFO: Using gradient clipping with tolerance=1.000\n",
      "2025-08-26 04:53:07.431 INFO: \n",
      "2025-08-26 04:53:07.431 INFO: ===========TRAINING===========\n",
      "2025-08-26 04:53:07.431 INFO: Started training, reporting errors on validation set\n",
      "2025-08-26 04:53:07.431 INFO: Loss metrics on validation set\n",
      "2025-08-26 04:53:10.330 INFO: Initial: head: pt_head, loss=0.04039491, RMSE_E_per_atom=  604.62 meV, RMSE_F=None meV / A, RMSE_stress=None meV / A^3\n",
      "2025-08-26 04:54:35.208 INFO: Initial: head: Default, loss=0.00673445, RMSE_E_per_atom=   88.83 meV, RMSE_F=  747.53 meV / A, RMSE_stress=None meV / A^3\n"
     ]
    }
   ],
   "source": [
    "# Now universal MACE finetuning on T2\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    cmd = [\n",
    "        \"mace_run_train\",\n",
    "        # === General settings ===\n",
    "        \"--name\",              \"mace_T2_including_replay_w2_just_142\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "    # --- MP replay (pretraining head) ---\n",
    "        \"--pt_train_file\",\"/home/phanim/harshitrawat/summer/replay_data/mp_finetuning-just_to_get_file_comb_142_run-84.xyz\",              # <- MP replay shortcut\n",
    "        \"--atomic_numbers\",\"[3,8,40,57]\",    # Li, O, Zr, La\n",
    "        \"--multiheads_finetuning\",\"True\",\n",
    "\n",
    "        \"--train_file\", \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz\",\n",
    "        \"--valid_file\", \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz\",\n",
    "\n",
    "        \"--batch_size\",        \"2\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "\n",
    "        \"--device\",            \"cuda\",\n",
    "\n",
    "        # === Loss function weights ===\n",
    "        \"--forces_weight\",     \"0\",         # Increased force weight to balance energy better\n",
    "        \"--energy_weight\",     \"10\",   \n",
    "        \"--stress_weight\", \"0\",             # Reduced from 100 → avoid dominance + stabilize energy RMSE\n",
    "\n",
    "        # === Learning setup ===\n",
    "        \"--lr\",                \"0.06\",      # Explicit learning rate (0.0001 is too low → stagnation)\n",
    "        \"--scheduler_patience\",\"4\",          # Reduce LR if val loss doesn’t improve in 3 epochs\n",
    "        \"--clip_grad\",         \"1\",        # Avoid exploding gradients — essential when energy_weight is high\n",
    "        \"--weight_decay\",      \"1e-8\",       # Mild regularization to prevent overfitting\n",
    "\n",
    "        # === EMA helps smooth loss curve ===\n",
    "        #\"--ema_decay\",         \"0.999\",     # Smooths validation loss and helps final convergence\n",
    "\n",
    "        # === Domain + training settings ===\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"50\",\n",
    "        \"--E0s\",               \"{3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\",    # Still allowed — could optionally be replaced by manual E0s\n",
    "        \"--seed\",              \"84\",\n",
    "        \"--patience\",     \"8\",\n",
    "\n",
    "        \"--restart_latest\",                   # Resumes from checkpoint if available\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f51981-781f-4180-8a1c-3c40e6ce30e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eaa615d-2325-4419-80a0-3d680335b9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing keys: []\n",
      "unexpected keys: []\n",
      "✅ Wrote: /home/phanim/harshitrawat/summer/formation_energy/mace_T2_frozen_220825.model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ckpt_path = \"/home/phanim/harshitrawat/summer/iteration_3/checkpoints/mace_T2_including_replay_w2_run-84_epoch-77.pt\"\n",
    "template_model_path = \"/home/phanim/harshitrawat/summer/iteration_3-Copy1/checkpoints/dummy_mace_T2_including_replay_w2_run-84.model\"\n",
    "out_model_path = \"/home/phanim/harshitrawat/summer/formation_energy/mace_T2_frozen_220825.model\"\n",
    "\n",
    "# Load both with full objects (not weights_only)\n",
    "ckpt  = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "model = torch.load(template_model_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# Inject weights from checkpoint\n",
    "missing, unexpected = model.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "print(\"missing keys:\", missing)\n",
    "print(\"unexpected keys:\", unexpected)\n",
    "\n",
    "# Save the usable .model\n",
    "torch.save(model, out_model_path)\n",
    "print(\"✅ Wrote:\", out_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45a6c825-de67-4ba0-ae3d-c2de2887d744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/mace/mace/calculators/mace.py:166: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using head Default out of ['pt_head', 'Default']\n",
      "No dtype selected, switching to float64 to match model dtype.\n",
      "Li: μ_model = -1.738880 eV/atom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/pymatgen/core/structure.py:3107: UserWarning: Issues encountered while parsing CIF: 4 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La: μ_model = 18.890417 eV/atom\n",
      "Zr: μ_model = 26.440122 eV/atom\n",
      "O: μ_model = -20.875662 eV/atom\n"
     ]
    }
   ],
   "source": [
    "from mace.calculators import MACECalculator\n",
    "mace_calc = MACECalculator(model_paths=[\"/home/phanim/harshitrawat/summer/formation_energy/mace_T2_frozen_220825.model\"], device=\"cuda\")  # or \"cpu\"\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "from pymatgen.core import Structure\n",
    "adaptor = AseAtomsAdaptor()\n",
    "\n",
    "pmg_structure = Structure.from_file(\"/home/phanim/harshitrawat/summer/formation_energy/cifs/Li.cif\")  # e.g. for Li\n",
    "ase_atoms = adaptor.get_atoms(pmg_structure)\n",
    "ase_atoms.calc = mace_calc\n",
    "total_energy = ase_atoms.get_potential_energy()\n",
    "mu_model_Li = total_energy / len(ase_atoms)\n",
    "print(f\"Li: μ_model = {mu_model_Li:.6f} eV/atom\")\n",
    "# Let us do this for La, Zr, and O as well\n",
    "pmg_structure = Structure.from_file(\"/home/phanim/harshitrawat/summer/formation_energy/cifs/La.cif\")\n",
    "ase_atoms = adaptor.get_atoms(pmg_structure)\n",
    "ase_atoms.calc = mace_calc\n",
    "total_energy = ase_atoms.get_potential_energy()\n",
    "mu_model_La = total_energy / len(ase_atoms)\n",
    "print(f\"La: μ_model = {mu_model_La:.6f} eV/atom\")\n",
    "pmg_structure = Structure.from_file(\"/home/phanim/harshitrawat/summer/formation_energy/cifs/Zr.cif\")\n",
    "ase_atoms = adaptor.get_atoms(pmg_structure)\n",
    "ase_atoms.calc = mace_calc\n",
    "total_energy = ase_atoms.get_potential_energy()\n",
    "mu_model_Zr = total_energy / len(ase_atoms)\n",
    "print(f\"Zr: μ_model = {mu_model_Zr:.6f} eV/atom\")\n",
    "pmg_structure = Structure.from_file(\"/home/phanim/harshitrawat/summer/formation_energy/cifs/O2.cif\")  # Needs to be a periodic solid O2 structure\n",
    "ase_atoms = adaptor.get_atoms(pmg_structure)\n",
    "ase_atoms.calc = mace_calc\n",
    "total_energy = ase_atoms.get_potential_energy()\n",
    "mu_model_O = total_energy / len(ase_atoms)\n",
    "print(f\"O: μ_model = {mu_model_O:.6f} eV/atom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecb73e79-1774-43ff-ab2d-b99db10fafb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/mace/mace/calculators/mace.py:166: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using head Default out of ['pt_head', 'Default']\n",
      "No dtype selected, switching to float64 to match model dtype.\n",
      "Li2O2          :  E_form (MACE_T2_w2_it3) = -0.311494 eV/atom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/pymatgen/core/structure.py:3107: UserWarning: Issues encountered while parsing CIF: 12 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li2O           :  E_form (MACE_T2_w2_it3) = -0.354533 eV/atom\n",
      "Li7La3Zr2O12   :  E_form (MACE_T2_w2_it3) = -1.119615 eV/atom\n",
      "ZrO2           :  E_form (MACE_T2_w2_it3) = -3.445901 eV/atom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/pymatgen/core/structure.py:3107: UserWarning: Issues encountered while parsing CIF: 8 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La2O3          :  E_form (MACE_T2_w2_it3) = -3.348167 eV/atom\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# ---- 1. Load the MACE model -------------------------------------------\n",
    "calculator = MACECalculator(model_paths=[\"/home/phanim/harshitrawat/summer/formation_energy/mace_T2_frozen_220825.model\"], device=\"cuda\")  # or \"cpu\"\n",
    "\n",
    "# ---- 2. Reference μ_model from MACE -----------------------------------\n",
    "\n",
    "mu_mace = {\n",
    "    \"Li\": -1.738880,\n",
    "    \"La\": 18.890417,\n",
    "    \"Zr\": 26.440122,\n",
    "    \"O\":  -20.875662,\n",
    "}\n",
    "\n",
    "# ---- 3. CIF files ------------------------------------------------------\n",
    "cif_dir = \"/home/phanim/harshitrawat/summer/formation_energy/cifs\"\n",
    "compounds = {\n",
    "    \"mp-841.cif\": \"Li2O2\",\n",
    "    \"mp-1960.cif\": \"Li2O\",\n",
    "    \"mp-942733.cif\": \"Li7La3Zr2O12\",\n",
    "    \"mp-2858.cif\": \"ZrO2\",\n",
    "    \"mp-1968.cif\": \"La2O3\",\n",
    "}\n",
    "\n",
    "# ---- 4. Predict formation energy per atom -----------------------------\n",
    "for fname, label in compounds.items():\n",
    "    struct = Structure.from_file(os.path.join(cif_dir, fname))\n",
    "    comp = struct.composition\n",
    "    n_atoms = comp.num_atoms\n",
    "\n",
    "    # Convert to ASE\n",
    "    ase_atoms = AseAtomsAdaptor.get_atoms(struct)\n",
    "\n",
    "    # Assign calculator and predict energy\n",
    "    ase_atoms.calc = calculator\n",
    "    energy_total = ase_atoms.get_potential_energy()  # eV (total)\n",
    "\n",
    "    # Reference energy from MACE chemical potentials\n",
    "    ref_total = sum(comp[el] * mu_mace[el.symbol] for el in comp.elements)\n",
    "\n",
    "    # Formation energy per atom\n",
    "    e_form = (energy_total - ref_total) / n_atoms\n",
    "\n",
    "    print(f\"{label:15s}:  E_form (MACE_T2_w2_it3) = {e_form: .6f} eV/atom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e647a58-c76d-4255-a2ca-4b4b88da0a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_2750347/3549858856.py:22: DeprecationWarning: Accessing summary data through MPRester.summary is deprecated. Please use MPRester.materials.summary instead.\n",
      "  docs = mpr.summary.search(\n"
     ]
    },
    {
     "ename": "MPRestError",
     "evalue": "invalid fields requested: ['energy']. Available fields: ['builder_meta', 'nsites', 'elements', 'nelements', 'composition', 'composition_reduced', 'formula_pretty', 'formula_anonymous', 'chemsys', 'volume', 'density', 'density_atomic', 'symmetry', 'property_name', 'material_id', 'deprecated', 'deprecation_reasons', 'last_updated', 'origins', 'warnings', 'structure', 'task_ids', 'uncorrected_energy_per_atom', 'energy_per_atom', 'formation_energy_per_atom', 'energy_above_hull', 'is_stable', 'equilibrium_reaction_energy_per_atom', 'decomposes_to', 'xas', 'grain_boundaries', 'band_gap', 'cbm', 'vbm', 'efermi', 'is_gap_direct', 'is_metal', 'es_source_calc_id', 'bandstructure', 'dos', 'dos_energy_up', 'dos_energy_down', 'is_magnetic', 'ordering', 'total_magnetization', 'total_magnetization_normalized_vol', 'total_magnetization_normalized_formula_units', 'num_magnetic_sites', 'num_unique_magnetic_sites', 'types_of_magnetic_species', 'bulk_modulus', 'shear_modulus', 'universal_anisotropy', 'homogeneous_poisson', 'e_total', 'e_ionic', 'e_electronic', 'n', 'e_ij_max', 'weighted_surface_energy_EV_PER_ANG2', 'weighted_surface_energy', 'weighted_work_function', 'surface_anisotropy', 'shape_factor', 'has_reconstructed', 'possible_species', 'has_props', 'theoretical', 'database_Ids']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMPRestError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m combo \u001b[38;5;129;01min\u001b[39;00m combinations(wanted_elements, r):\n\u001b[1;32m     21\u001b[0m     chemsys \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(combo)\n\u001b[0;32m---> 22\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[43mmpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchemsys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchemsys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaterial_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformula_pretty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstructure\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menergy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchemsys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m entries\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n",
      "File \u001b[0;32m~/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/mp_api/client/routes/materials/summary.py:367\u001b[0m, in \u001b[0;36mSummaryRester.search\u001b[0;34m(self, band_gap, chemsys, crystal_system, density, deprecated, e_electronic, e_ionic, e_total, efermi, elastic_anisotropy, elements, energy_above_hull, equilibrium_reaction_energy, exclude_elements, formation_energy, formula, g_reuss, g_voigt, g_vrh, has_props, has_reconstructed, is_gap_direct, is_metal, is_stable, k_reuss, k_voigt, k_vrh, magnetic_ordering, material_ids, n, num_elements, num_sites, num_magnetic_sites, num_unique_magnetic_sites, piezoelectric_modulus, poisson_ratio, possible_species, shape_factor, spacegroup_number, spacegroup_symbol, surface_energy_anisotropy, theoretical, total_energy, total_magnetization, total_magnetization_normalized_formula_units, total_magnetization_normalized_vol, uncorrected_energy, volume, weighted_surface_energy, weighted_work_function, include_gnome, num_chunks, chunk_size, all_fields, fields, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m     query_params\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_id_not_eq\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgnome_r2scan_statics\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m    361\u001b[0m query_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    362\u001b[0m     entry: query_params[entry]\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m query_params\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_params[entry] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    365\u001b[0m }\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/mp_api/client/core/client.py:1191\u001b[0m, in \u001b[0;36mBaseRester._search\u001b[0;34m(self, num_chunks, chunk_size, all_fields, fields, **kwargs)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A generic search method to retrieve documents matching specific parameters.\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \n\u001b[1;32m   1171\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;124;03m    A list of documents.\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# This method should be customized for each end point to give more user friendly,\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# documented kwargs.\u001b[39;00m\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_all_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/mp_api/client/core/client.py:1264\u001b[0m, in \u001b[0;36mBaseRester._get_all_documents\u001b[0;34m(self, query_params, all_fields, fields, chunk_size, num_chunks)\u001b[0m\n\u001b[1;32m   1250\u001b[0m list_entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m   1251\u001b[0m     (\n\u001b[1;32m   1252\u001b[0m         (key, \u001b[38;5;28mlen\u001b[39m(entry\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1260\u001b[0m )\n\u001b[1;32m   1262\u001b[0m chosen_param \u001b[38;5;241m=\u001b[39m list_entries[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_entries) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query_resource\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchosen_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/mp_api/client/core/client.py:464\u001b[0m, in \u001b[0;36mBaseRester._query_resource\u001b[0;34m(self, criteria, fields, suburl, use_document_model, parallel_param, num_chunks, chunk_size, timeout)\u001b[0m\n\u001b[1;32m    460\u001b[0m         invalid_fields \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    461\u001b[0m             f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_fields\n\u001b[1;32m    462\u001b[0m         ]\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m invalid_fields:\n\u001b[0;32m--> 464\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m MPRestError(\n\u001b[1;32m    465\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid fields requested: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_fields\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Available fields: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_fields\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m             )\n\u001b[1;32m    468\u001b[0m     criteria[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fields\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(fields)\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mMPRestError\u001b[0m: invalid fields requested: ['energy']. Available fields: ['builder_meta', 'nsites', 'elements', 'nelements', 'composition', 'composition_reduced', 'formula_pretty', 'formula_anonymous', 'chemsys', 'volume', 'density', 'density_atomic', 'symmetry', 'property_name', 'material_id', 'deprecated', 'deprecation_reasons', 'last_updated', 'origins', 'warnings', 'structure', 'task_ids', 'uncorrected_energy_per_atom', 'energy_per_atom', 'formation_energy_per_atom', 'energy_above_hull', 'is_stable', 'equilibrium_reaction_energy_per_atom', 'decomposes_to', 'xas', 'grain_boundaries', 'band_gap', 'cbm', 'vbm', 'efermi', 'is_gap_direct', 'is_metal', 'es_source_calc_id', 'bandstructure', 'dos', 'dos_energy_up', 'dos_energy_down', 'is_magnetic', 'ordering', 'total_magnetization', 'total_magnetization_normalized_vol', 'total_magnetization_normalized_formula_units', 'num_magnetic_sites', 'num_unique_magnetic_sites', 'types_of_magnetic_species', 'bulk_modulus', 'shear_modulus', 'universal_anisotropy', 'homogeneous_poisson', 'e_total', 'e_ionic', 'e_electronic', 'n', 'e_ij_max', 'weighted_surface_energy_EV_PER_ANG2', 'weighted_surface_energy', 'weighted_work_function', 'surface_anisotropy', 'shape_factor', 'has_reconstructed', 'possible_species', 'has_props', 'theoretical', 'database_Ids']"
     ]
    }
   ],
   "source": [
    "from mp_api.client import MPRester\n",
    "from pymatgen.core import Structure\n",
    "from ase.io import write\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "import os\n",
    "\n",
    "API_KEY = \"j3J85pX4nLw6asHG9E2lbbCHEKDKgrjc\"   # <-- put your Materials Project API key here\n",
    "adaptor = AseAtomsAdaptor()\n",
    "\n",
    "# Output directory\n",
    "os.makedirs(\"mp_li_la_zr_o_cifs\", exist_ok=True)\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "wanted_elements = [\"Li\", \"La\", \"Zr\", \"O\"]\n",
    "all_structures = []\n",
    "\n",
    "with MPRester(API_KEY) as mpr:\n",
    "    for r in [2, 3, 4]:  # binaries, ternaries, quaternaries\n",
    "        for combo in combinations(wanted_elements, r):\n",
    "            chemsys = \"-\".join(combo)\n",
    "            docs = mpr.summary.search(\n",
    "                chemsys=chemsys,\n",
    "                fields=[\"material_id\", \"formula_pretty\", \"structure\", \"energy\"]\n",
    "            )\n",
    "            print(f\"{chemsys}: {len(docs)} entries\")\n",
    "\n",
    "            for doc in docs:\n",
    "                struct = doc.structure\n",
    "                ase_atoms = adaptor.get_atoms(struct)\n",
    "                ase_atoms.info[\"energy\"] = doc.energy\n",
    "                all_structures.append(ase_atoms)\n",
    "\n",
    "                # Save CIF\n",
    "                cif_name = f\"mp_li_la_zr_o_cifs/{doc.material_id}_{doc.formula_pretty}.cif\"\n",
    "                struct.to(fmt=\"cif\", filename=cif_name)\n",
    "\n",
    "# Write replay dataset\n",
    "write(\"replay_li_la_zr_o.xyz\", all_structures)\n",
    "print(f\"Saved {len(all_structures)} structures into replay_li_la_zr_o.xyz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991c88bf-938d-4a38-868e-3cf3c6bb669d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1029839/567040639.py:19: DeprecationWarning: Accessing summary data through MPRester.summary is deprecated. Please use MPRester.materials.summary instead.\n",
      "  docs = mpr.summary.search(\n",
      "Retrieving SummaryDoc documents: 100%|██████████| 9/9 [00:00<00:00, 262144.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li: 9 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 3/3 [00:00<00:00, 82241.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La: 3 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 6/6 [00:00<00:00, 167772.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zr: 6 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 28/28 [00:00<00:00, 752823.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O: 28 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 3/3 [00:00<00:00, 85598.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li-La: 3 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 1/1 [00:00<00:00, 27413.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li-Zr: 1 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 23/23 [00:00<00:00, 647442.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li-O: 23 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 2/2 [00:00<00:00, 59074.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La-Zr: 2 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 12/12 [00:00<00:00, 305040.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La-O: 12 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 31/31 [00:00<00:00, 833483.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zr-O: 31 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li-La-Zr: 0 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 7/7 [00:00<00:00, 182361.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li-La-O: 7 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 16/16 [00:00<00:00, 296941.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li-Zr-O: 16 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 6/6 [00:00<00:00, 161319.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La-Zr-O: 6 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 3/3 [00:00<00:00, 37008.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li-La-Zr-O: 3 entries\n",
      "Saved 150 structures into replay_li_la_zr_o.xyz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mp_api.client import MPRester\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "from ase.io import write\n",
    "from itertools import combinations\n",
    "import os\n",
    "\n",
    "API_KEY = \"j3J85pX4nLw6asHG9E2lbbCHEKDKgrjc\"\n",
    "adaptor = AseAtomsAdaptor()\n",
    "\n",
    "os.makedirs(\"mp_li_la_zr_o_cifs\", exist_ok=True)\n",
    "wanted_elements = [\"Li\", \"La\", \"Zr\", \"O\"]\n",
    "\n",
    "structures = []\n",
    "\n",
    "with MPRester(API_KEY) as mpr:\n",
    "    for r in [1, 2, 3, 4]:  # elements, binaries, ternaries, quaternaries\n",
    "        for combo in combinations(wanted_elements, r):\n",
    "            chemsys = \"-\".join(combo)\n",
    "            docs = mpr.summary.search(\n",
    "                chemsys=chemsys,\n",
    "                fields=[\"material_id\", \"formula_pretty\", \"structure\",\n",
    "                        \"nsites\", \"energy_per_atom\"]\n",
    "            )\n",
    "            print(f\"{chemsys}: {len(docs)} entries\")\n",
    "\n",
    "            for doc in docs:\n",
    "                struct = doc.structure\n",
    "                ase_atoms = adaptor.get_atoms(struct)\n",
    "\n",
    "                # Convert per-atom energy to total energy\n",
    "                total_energy = doc.energy_per_atom * doc.nsites\n",
    "                ase_atoms.info[\"energy\"] = total_energy\n",
    "\n",
    "                structures.append(ase_atoms)\n",
    "\n",
    "                # Save CIF\n",
    "                cif_name = f\"mp_li_la_zr_o_cifs/{doc.material_id}_{doc.formula_pretty}.cif\"\n",
    "                struct.to(fmt=\"cif\", filename=cif_name)\n",
    "\n",
    "# Save replay dataset\n",
    "write(\"replay_li_la_zr_o_with_elem.xyz\", structures)\n",
    "print(f\"Saved {len(structures)} structures into replay_li_la_zr_o.xyz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef74717-4772-44b3-8df0-beec5f2a98bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: mace_run_train \\\n",
      "    --name \\\n",
      "    mace_T2_including_shortreplay_w2_welem \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --pt_train_file \\\n",
      "    /home/phanim/harshitrawat/summer/iteration_3-Copy1/replay_li_la_zr_o_with_elem.xyz \\\n",
      "    --atomic_numbers \\\n",
      "    [3,8,40,57] \\\n",
      "    --multiheads_finetuning \\\n",
      "    True \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz \\\n",
      "    --batch_size \\\n",
      "    2 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --forces_weight \\\n",
      "    0 \\\n",
      "    --energy_weight \\\n",
      "    10 \\\n",
      "    --stress_weight \\\n",
      "    0 \\\n",
      "    --lr \\\n",
      "    1 \\\n",
      "    --scheduler_patience \\\n",
      "    4 \\\n",
      "    --clip_grad \\\n",
      "    1 \\\n",
      "    --weight_decay \\\n",
      "    1e-8 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    1 \\\n",
      "    --E0s \\\n",
      "    {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549} \\\n",
      "    --seed \\\n",
      "    84 \\\n",
      "    --patience \\\n",
      "    8 \\\n",
      "    --restart_latest\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-25 22:31:42.907 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-08-25 22:31:42.908 INFO: MACE version: 0.3.14\n",
      "2025-08-25 22:31:43.425 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py:152: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-25 22:31:43.864 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-08-25 22:31:43.865 INFO: Multihead finetuning mode, setting learning rate to 0.0001 and EMA to True. To use a different learning rate, set --force_mh_ft_lr=True.\n",
      "2025-08-25 22:31:43.865 INFO: Using multiheads finetuning mode, setting learning rate to 0.0001 and EMA to True\n",
      "2025-08-25 22:31:43.865 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-08-25 22:31:43.865 INFO: Using heads: ['Default', 'pt_head']\n",
      "2025-08-25 22:31:43.865 INFO: Using the key specifications to parse data:\n",
      "2025-08-25 22:31:43.865 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-25 22:31:43.865 INFO: pt_head: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-25 22:31:43.865 INFO: =============    Processing head Default     ===========\n",
      "2025-08-25 22:31:49.212 INFO: Training set 1/1 [energy: 1612, stress: 0, virials: 0, dipole components: 0, head: 1612, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1612, charges: 0]\n",
      "2025-08-25 22:31:49.217 INFO: Total Training set [energy: 1612, stress: 0, virials: 0, dipole components: 0, head: 1612, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1612, charges: 0]\n",
      "2025-08-25 22:31:51.541 INFO: Validation set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-08-25 22:31:51.544 INFO: Total Validation set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-08-25 22:31:51.544 INFO: Total number of configurations: train=1612, valid=705, tests=[],\n",
      "2025-08-25 22:31:51.544 INFO: =============    Processing head pt_head     ===========\n",
      "2025-08-25 22:31:51.580 WARNING: No forces found with key 'REF_forces' in '/home/phanim/harshitrawat/summer/iteration_3-Copy1/replay_li_la_zr_o_with_elem.xyz'. If this is unexpected, Please change the key name in the command line arguments or ensure that the file contains the required data.\n",
      "2025-08-25 22:31:51.582 INFO: Training set 1/1 [energy: 150, stress: 0, virials: 0, dipole components: 0, head: 150, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-25 22:31:51.582 INFO: Total Training set [energy: 150, stress: 0, virials: 0, dipole components: 0, head: 150, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-25 22:31:51.582 INFO: No validation set provided, splitting training data instead.\n",
      "2025-08-25 22:31:51.586 INFO: Using random 10% of training set for validation with indices saved in: ./valid_indices_84.txt\n",
      "2025-08-25 22:31:51.587 INFO: Random Split Training set [energy: 135, stress: 0, virials: 0, dipole components: 0, head: 135, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-25 22:31:51.587 INFO: Random Split Validation set [energy: 15, stress: 0, virials: 0, dipole components: 0, head: 15, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-25 22:31:51.587 INFO: Total number of configurations: train=135, valid=15, tests=[],\n",
      "2025-08-25 22:31:51.587 INFO: ==================Using multiheads finetuning mode==================\n",
      "2025-08-25 22:31:51.587 INFO: Total number of configurations in pretraining: train=135, valid=15\n",
      "2025-08-25 22:31:51.587 INFO: Using atomic numbers from command line argument\n",
      "2025-08-25 22:31:51.587 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-08-25 22:31:51.587 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-08-25 22:31:51.590 INFO: Atomic Energies used (z: eV) for head Default: {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\n",
      "2025-08-25 22:31:51.590 INFO: Atomic Energies used (z: eV) for head pt_head: {3: -3.482100566595956, 8: -7.28459863421322, 40: -11.846857579882572, 57: -8.933684809576345}\n",
      "2025-08-25 22:31:51.590 INFO: Processing datasets for head 'Default'\n",
      "2025-08-25 22:32:10.868 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-08-25 22:32:19.422 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-08-25 22:32:19.422 INFO: Combined validation datasets for Default\n",
      "2025-08-25 22:32:19.422 INFO: Head 'Default' training dataset size: 1612\n",
      "2025-08-25 22:32:19.422 INFO: Processing datasets for head 'pt_head'\n",
      "2025-08-25 22:32:19.526 INFO: Combining 1 list datasets for head 'pt_head'\n",
      "2025-08-25 22:32:19.531 INFO: Head 'pt_head' training dataset size: 135\n",
      "2025-08-25 22:32:19.532 INFO: Average number of neighbors: 61.964672446250916\n",
      "2025-08-25 22:32:19.532 INFO: During training the following quantities will be reported: energy, forces, stress\n",
      "2025-08-25 22:32:19.532 INFO: ===========MODEL DETAILS===========\n",
      "2025-08-25 22:32:21.351 WARNING: Standard deviation of the scaling is zero, Changing to no scaling\n",
      "2025-08-25 22:32:21.353 INFO: Loading FOUNDATION model\n",
      "2025-08-25 22:32:21.354 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-08-25 22:32:21.354 INFO: Model configuration extracted from foundation model\n",
      "2025-08-25 22:32:21.354 INFO: Using universal loss function for fine-tuning\n",
      "2025-08-25 22:32:21.354 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-08-25 22:32:21.354 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-08-25 22:32:21.354 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-08-25 22:32:21.354 INFO: Distance transform for radial basis functions: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-25 22:32:23.074 INFO: Total number of parameters: 896586\n",
      "2025-08-25 22:32:23.074 INFO: \n",
      "2025-08-25 22:32:23.074 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-08-25 22:32:23.074 INFO: Using ADAM as parameter optimizer\n",
      "2025-08-25 22:32:23.074 INFO: Batch size: 2\n",
      "2025-08-25 22:32:23.074 INFO: Using Exponential Moving Average with decay: 0.99999\n",
      "2025-08-25 22:32:23.074 INFO: Number of gradient updates: 873\n",
      "2025-08-25 22:32:23.074 INFO: Learning rate: 0.0001, weight decay: 1e-08\n",
      "2025-08-25 22:32:23.074 INFO: UniversalLoss(energy_weight=10.000, forces_weight=0.000, stress_weight=0.000)\n",
      "2025-08-25 22:32:23.077 WARNING: Cannot find checkpoint with tag 'mace_T2_including_shortreplay_w2_welem_run-84' in './checkpoints'\n",
      "2025-08-25 22:32:23.078 INFO: Using gradient clipping with tolerance=1.000\n",
      "2025-08-25 22:32:23.078 INFO: \n",
      "2025-08-25 22:32:23.078 INFO: ===========TRAINING===========\n",
      "2025-08-25 22:32:23.078 INFO: Started training, reporting errors on validation set\n",
      "2025-08-25 22:32:23.078 INFO: Loss metrics on validation set\n",
      "2025-08-25 22:32:29.662 INFO: Initial: head: pt_head, loss=0.37186638, RMSE_E_per_atom= 8668.45 meV, RMSE_F=None meV / A, RMSE_stress=None meV / A^3\n",
      "2025-08-25 22:33:54.487 INFO: Initial: head: Default, loss=0.11800754, RMSE_E_per_atom= 1245.40 meV, RMSE_F=  959.54 meV / A, RMSE_stress=None meV / A^3\n",
      "2025-08-25 22:42:03.687 INFO: Epoch 0: head: pt_head, loss=0.33532765, RMSE_E_per_atom= 8131.42 meV, RMSE_F=None meV / A, RMSE_stress=None meV / A^3\n",
      "2025-08-25 22:43:28.385 INFO: Epoch 0: head: Default, loss=0.00686583, RMSE_E_per_atom=   90.07 meV, RMSE_F=  703.07 meV / A, RMSE_stress=None meV / A^3\n",
      "2025-08-25 22:43:28.477 INFO: Training complete\n",
      "2025-08-25 22:43:28.477 INFO: \n",
      "2025-08-25 22:43:28.477 INFO: ===========RESULTS===========\n",
      "2025-08-25 22:43:28.480 INFO: Loading checkpoint: ./checkpoints/mace_T2_including_shortreplay_w2_welem_run-84_epoch-0.pt\n",
      "2025-08-25 22:43:28.527 INFO: Loaded Stage one model from epoch 0 for evaluation\n",
      "2025-08-25 22:43:28.528 INFO: Saving model to checkpoints/mace_T2_including_shortreplay_w2_welem_run-84.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/checkpoint.py:187: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-25 22:43:28.742 INFO: Compiling model, saving metadata to mace_T2_including_shortreplay_w2_welem_compiled.model\n",
      "2025-08-25 22:43:29.580 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-08-25 22:43:29.581 INFO: Skipping evaluation for heads: ['pt_head']\n",
      "2025-08-25 22:43:29.581 INFO: Evaluating train_Default ...\n",
      "2025-08-25 22:46:16.822 INFO: Skipping evaluation of train_pt_head (in skip_heads list)\n",
      "2025-08-25 22:46:16.825 INFO: Evaluating valid_Default ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/bin/mace_run_train\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m1016\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    table_train_valid = create_error_table(\n",
      "        table_type=args.error_table,\n",
      "    ...<7 lines>...\n",
      "        skip_heads=skip_heads,\n",
      "    )\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/tables_utils.py\"\u001b[0m, line \u001b[35m117\u001b[0m, in \u001b[35mcreate_error_table\u001b[0m\n",
      "    _, metrics = \u001b[31mevaluate\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mmodel,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^\u001b[0m\n",
      "    ...<3 lines>...\n",
      "        \u001b[1;31mdevice=device,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/train.py\"\u001b[0m, line \u001b[35m557\u001b[0m, in \u001b[35mevaluate\u001b[0m\n",
      "    avg_loss, aux = \u001b[31mmetrics\u001b[0m\u001b[1;31m(batch, output)\u001b[0m\n",
      "                    \u001b[31m~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m1751\u001b[0m, in \u001b[35m_wrapped_call_impl\u001b[0m\n",
      "    return \u001b[31mself._call_impl\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m1762\u001b[0m, in \u001b[35m_call_impl\u001b[0m\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torchmetrics/metric.py\"\u001b[0m, line \u001b[35m313\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "    self._forward_cache = \u001b[31mself._forward_full_state_update\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "                          \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torchmetrics/metric.py\"\u001b[0m, line \u001b[35m328\u001b[0m, in \u001b[35m_forward_full_state_update\u001b[0m\n",
      "    \u001b[31mself.update\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torchmetrics/metric.py\"\u001b[0m, line \u001b[35m549\u001b[0m, in \u001b[35mwrapped_func\u001b[0m\n",
      "    \u001b[31mupdate\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/train.py\"\u001b[0m, line \u001b[35m603\u001b[0m, in \u001b[35mupdate\u001b[0m\n",
      "    loss = self.loss_fn(pred=output, ref=batch)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m1751\u001b[0m, in \u001b[35m_wrapped_call_impl\u001b[0m\n",
      "    return \u001b[31mself._call_impl\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m1762\u001b[0m, in \u001b[35m_call_impl\u001b[0m\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/modules/loss.py\"\u001b[0m, line \u001b[35m416\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "    configs_forces_weight = \u001b[31mtorch.repeat_interleave\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mref.forces_weight, ref.ptr[1:] - ref.ptr[:-1]\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m.unsqueeze(-1)\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "\u001b[1;35mKeyboardInterrupt\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     57\u001b[39m     subprocess.run(cmd, check=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m cmd = [\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmace_run_train\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# === General settings ===\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m--restart_latest\u001b[39m\u001b[33m\"\u001b[39m,                   \u001b[38;5;66;03m# Resumes from checkpoint if available\u001b[39;00m\n\u001b[32m     54\u001b[39m ]\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m.join(cmd), file=sys.stderr)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:558\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    560\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:1213\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1211\u001b[39m         stderr = \u001b[38;5;28mself\u001b[39m.stderr.read()\n\u001b[32m   1212\u001b[39m         \u001b[38;5;28mself\u001b[39m.stderr.close()\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1215\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:1276\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1274\u001b[39m     endtime = _time() + timeout\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1278\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1279\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1280\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:2068\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.returncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2067\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2068\u001b[39m (pid, sts) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[32m   2071\u001b[39m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[32m   2072\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pid == \u001b[38;5;28mself\u001b[39m.pid:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:2026\u001b[39m, in \u001b[36mPopen._try_wait\u001b[39m\u001b[34m(self, wait_flags)\u001b[39m\n\u001b[32m   2024\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[32m   2025\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2026\u001b[39m     (pid, sts) = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[32m   2028\u001b[39m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[32m   2029\u001b[39m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[32m   2030\u001b[39m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[32m   2031\u001b[39m     pid = \u001b[38;5;28mself\u001b[39m.pid\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Now universal MACE finetuning on T2\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    cmd = [\n",
    "        \"mace_run_train\",\n",
    "        # === General settings ===\n",
    "        \"--name\",              \"mace_T2_including_shortreplay_w2_welem\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "    # --- MP replay (pretraining head) ---\n",
    "        \"--pt_train_file\",\"/home/phanim/harshitrawat/summer/iteration_3-Copy1/replay_li_la_zr_o_with_elem.xyz\",              # <- MP replay shortcut\n",
    "        \"--atomic_numbers\",\"[3,8,40,57]\",    # Li, O, Zr, La\n",
    "        \"--multiheads_finetuning\",\"True\",\n",
    "\n",
    "        \"--train_file\", \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz\",\n",
    "        \"--valid_file\", \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz\",\n",
    "\n",
    "        \"--batch_size\",        \"2\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "\n",
    "        \"--device\",            \"cuda\",\n",
    "\n",
    "        # === Loss function weights ===\n",
    "        \"--forces_weight\",     \"0\",         # Increased force weight to balance energy better\n",
    "        \"--energy_weight\",     \"10\",   \n",
    "        \"--stress_weight\", \"0\",             # Reduced from 100 → avoid dominance + stabilize energy RMSE\n",
    "\n",
    "        # === Learning setup ===\n",
    "        \"--lr\",                \"1\",      # Explicit learning rate (0.0001 is too low → stagnation)\n",
    "        \"--scheduler_patience\",\"4\",          # Reduce LR if val loss doesn’t improve in 3 epochs\n",
    "        \"--clip_grad\",         \"1\",        # Avoid exploding gradients — essential when energy_weight is high\n",
    "        \"--weight_decay\",      \"1e-8\",       # Mild regularization to prevent overfitting\n",
    "\n",
    "        # === EMA helps smooth loss curve ===\n",
    "        #\"--ema_decay\",         \"0.999\",     # Smooths validation loss and helps final convergence\n",
    "\n",
    "        # === Domain + training settings ===\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"1\",\n",
    "        \"--E0s\",               \"{3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\",    # Still allowed — could optionally be replaced by manual E0s\n",
    "        \"--seed\",              \"84\",\n",
    "        \"--patience\",     \"8\",\n",
    "\n",
    "        \"--restart_latest\",                   # Resumes from checkpoint if available\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1c9c0-57d4-4252-8579-3e99101dafde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: mace_run_train \\\n",
      "    --name \\\n",
      "    mace_T2_including_shortreplay_w2_welem \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --pt_train_file \\\n",
      "    /home/phanim/harshitrawat/summer/iteration_3-Copy1/replay_li_la_zr_o_with_elem.xyz \\\n",
      "    --atomic_numbers \\\n",
      "    [3,8,40,57] \\\n",
      "    --multiheads_finetuning \\\n",
      "    True \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz \\\n",
      "    --batch_size \\\n",
      "    2 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --forces_weight \\\n",
      "    0 \\\n",
      "    --energy_weight \\\n",
      "    10 \\\n",
      "    --stress_weight \\\n",
      "    0 \\\n",
      "    --lr \\\n",
      "    0.1 \\\n",
      "    --scheduler_patience \\\n",
      "    4 \\\n",
      "    --clip_grad \\\n",
      "    1 \\\n",
      "    --weight_decay \\\n",
      "    1e-8 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    10 \\\n",
      "    --E0s \\\n",
      "    {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549} \\\n",
      "    --seed \\\n",
      "    84 \\\n",
      "    --patience \\\n",
      "    8 \\\n",
      "    --restart_latest\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-25 22:46:48.945 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-08-25 22:46:48.945 INFO: MACE version: 0.3.14\n",
      "2025-08-25 22:46:49.484 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py:152: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-25 22:46:50.114 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-08-25 22:46:50.117 INFO: Multihead finetuning mode, setting learning rate to 0.0001 and EMA to True. To use a different learning rate, set --force_mh_ft_lr=True.\n",
      "2025-08-25 22:46:50.117 INFO: Using multiheads finetuning mode, setting learning rate to 0.0001 and EMA to True\n",
      "2025-08-25 22:46:50.117 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-08-25 22:46:50.117 INFO: Using heads: ['Default', 'pt_head']\n",
      "2025-08-25 22:46:50.117 INFO: Using the key specifications to parse data:\n",
      "2025-08-25 22:46:50.117 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-25 22:46:50.118 INFO: pt_head: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-25 22:46:50.118 INFO: =============    Processing head Default     ===========\n",
      "2025-08-25 22:46:55.487 INFO: Training set 1/1 [energy: 1612, stress: 0, virials: 0, dipole components: 0, head: 1612, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1612, charges: 0]\n",
      "2025-08-25 22:46:55.492 INFO: Total Training set [energy: 1612, stress: 0, virials: 0, dipole components: 0, head: 1612, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1612, charges: 0]\n",
      "2025-08-25 22:46:57.815 INFO: Validation set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-08-25 22:46:57.817 INFO: Total Validation set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-08-25 22:46:57.817 INFO: Total number of configurations: train=1612, valid=705, tests=[],\n",
      "2025-08-25 22:46:57.817 INFO: =============    Processing head pt_head     ===========\n",
      "2025-08-25 22:46:57.844 WARNING: No forces found with key 'REF_forces' in '/home/phanim/harshitrawat/summer/iteration_3-Copy1/replay_li_la_zr_o_with_elem.xyz'. If this is unexpected, Please change the key name in the command line arguments or ensure that the file contains the required data.\n",
      "2025-08-25 22:46:57.846 INFO: Training set 1/1 [energy: 150, stress: 0, virials: 0, dipole components: 0, head: 150, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-25 22:46:57.846 INFO: Total Training set [energy: 150, stress: 0, virials: 0, dipole components: 0, head: 150, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-25 22:46:57.846 INFO: No validation set provided, splitting training data instead.\n",
      "2025-08-25 22:46:57.850 INFO: Using random 10% of training set for validation with indices saved in: ./valid_indices_84.txt\n",
      "2025-08-25 22:46:57.851 INFO: Random Split Training set [energy: 135, stress: 0, virials: 0, dipole components: 0, head: 135, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-25 22:46:57.851 INFO: Random Split Validation set [energy: 15, stress: 0, virials: 0, dipole components: 0, head: 15, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-08-25 22:46:57.851 INFO: Total number of configurations: train=135, valid=15, tests=[],\n",
      "2025-08-25 22:46:57.851 INFO: ==================Using multiheads finetuning mode==================\n",
      "2025-08-25 22:46:57.851 INFO: Total number of configurations in pretraining: train=135, valid=15\n",
      "2025-08-25 22:46:57.851 INFO: Using atomic numbers from command line argument\n",
      "2025-08-25 22:46:57.851 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-08-25 22:46:57.851 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-08-25 22:46:57.854 INFO: Atomic Energies used (z: eV) for head Default: {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\n",
      "2025-08-25 22:46:57.854 INFO: Atomic Energies used (z: eV) for head pt_head: {3: -3.482100566595956, 8: -7.28459863421322, 40: -11.846857579882572, 57: -8.933684809576345}\n",
      "2025-08-25 22:46:57.854 INFO: Processing datasets for head 'Default'\n",
      "2025-08-25 22:47:17.070 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-08-25 22:47:25.657 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-08-25 22:47:25.658 INFO: Combined validation datasets for Default\n",
      "2025-08-25 22:47:25.658 INFO: Head 'Default' training dataset size: 1612\n",
      "2025-08-25 22:47:25.658 INFO: Processing datasets for head 'pt_head'\n",
      "2025-08-25 22:47:25.761 INFO: Combining 1 list datasets for head 'pt_head'\n",
      "2025-08-25 22:47:25.766 INFO: Head 'pt_head' training dataset size: 135\n",
      "2025-08-25 22:47:25.766 INFO: Average number of neighbors: 61.964672446250916\n",
      "2025-08-25 22:47:25.766 INFO: During training the following quantities will be reported: energy, forces, stress\n",
      "2025-08-25 22:47:25.766 INFO: ===========MODEL DETAILS===========\n",
      "2025-08-25 22:47:27.315 WARNING: Standard deviation of the scaling is zero, Changing to no scaling\n",
      "2025-08-25 22:47:27.318 INFO: Loading FOUNDATION model\n",
      "2025-08-25 22:47:27.319 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-08-25 22:47:27.319 INFO: Model configuration extracted from foundation model\n",
      "2025-08-25 22:47:27.319 INFO: Using universal loss function for fine-tuning\n",
      "2025-08-25 22:47:27.319 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-08-25 22:47:27.319 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-08-25 22:47:27.319 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-08-25 22:47:27.319 INFO: Distance transform for radial basis functions: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/checkpoint.py:187: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-25 22:47:28.590 INFO: Total number of parameters: 896586\n",
      "2025-08-25 22:47:28.590 INFO: \n",
      "2025-08-25 22:47:28.590 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-08-25 22:47:28.590 INFO: Using ADAM as parameter optimizer\n",
      "2025-08-25 22:47:28.590 INFO: Batch size: 2\n",
      "2025-08-25 22:47:28.590 INFO: Using Exponential Moving Average with decay: 0.99999\n",
      "2025-08-25 22:47:28.590 INFO: Number of gradient updates: 8735\n",
      "2025-08-25 22:47:28.590 INFO: Learning rate: 0.0001, weight decay: 1e-08\n",
      "2025-08-25 22:47:28.590 INFO: UniversalLoss(energy_weight=10.000, forces_weight=0.000, stress_weight=0.000)\n",
      "2025-08-25 22:47:28.593 WARNING: No SWA checkpoint found, while SWA is enabled. Compare the swa_start parameter and the latest checkpoint.\n",
      "2025-08-25 22:47:28.593 INFO: Loading checkpoint: ./checkpoints/mace_T2_including_shortreplay_w2_welem_run-84_epoch-0.pt\n",
      "2025-08-25 22:47:28.629 INFO: Using gradient clipping with tolerance=1.000\n",
      "2025-08-25 22:47:28.629 INFO: \n",
      "2025-08-25 22:47:28.629 INFO: ===========TRAINING===========\n",
      "2025-08-25 22:47:28.629 INFO: Started training, reporting errors on validation set\n",
      "2025-08-25 22:47:28.629 INFO: Loss metrics on validation set\n",
      "2025-08-25 22:47:31.487 INFO: Initial: head: pt_head, loss=0.33532765, RMSE_E_per_atom= 8131.42 meV, RMSE_F=None meV / A, RMSE_stress=None meV / A^3\n"
     ]
    }
   ],
   "source": [
    "# Now universal MACE finetuning on T2\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    cmd = [\n",
    "        \"mace_run_train\",\n",
    "        # === General settings ===\n",
    "        \"--name\",              \"mace_T2_including_shortreplay_w2_welem\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "    # --- MP replay (pretraining head) ---\n",
    "        \"--pt_train_file\",\"/home/phanim/harshitrawat/summer/iteration_3-Copy1/replay_li_la_zr_o_with_elem.xyz\",              # <- MP replay shortcut\n",
    "        \"--atomic_numbers\",\"[3,8,40,57]\",    # Li, O, Zr, La\n",
    "        \"--multiheads_finetuning\",\"True\",\n",
    "\n",
    "        \"--train_file\", \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz\",\n",
    "        \"--valid_file\", \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz\",\n",
    "\n",
    "        \"--batch_size\",        \"2\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "\n",
    "        \"--device\",            \"cuda\",\n",
    "\n",
    "        # === Loss function weights ===\n",
    "        \"--forces_weight\",     \"0\",         # Increased force weight to balance energy better\n",
    "        \"--energy_weight\",     \"10\",   \n",
    "        \"--stress_weight\", \"0\",             # Reduced from 100 → avoid dominance + stabilize energy RMSE\n",
    "\n",
    "        # === Learning setup ===\n",
    "        \"--lr\",                \"0.1\",      # Explicit learning rate (0.0001 is too low → stagnation)\n",
    "        \"--scheduler_patience\",\"4\",          # Reduce LR if val loss doesn’t improve in 3 epochs\n",
    "        \"--clip_grad\",         \"1\",        # Avoid exploding gradients — essential when energy_weight is high\n",
    "        \"--weight_decay\",      \"1e-8\",       # Mild regularization to prevent overfitting\n",
    "\n",
    "        # === EMA helps smooth loss curve ===\n",
    "        #\"--ema_decay\",         \"0.999\",     # Smooths validation loss and helps final convergence\n",
    "\n",
    "        # === Domain + training settings ===\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"10\",\n",
    "        \"--E0s\",               \"{3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\",    # Still allowed — could optionally be replaced by manual E0s\n",
    "        \"--seed\",              \"84\",\n",
    "        \"--patience\",     \"8\",\n",
    "\n",
    "        \"--restart_latest\",                   # Resumes from checkpoint if available\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6de468a-a396-42b2-811f-257d8ffb41b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: mace_run_train \\\n",
      "    --name \\\n",
      "    mace_T2_including_replay_w2_combinations_4elems \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --pt_train_file \\\n",
      "    /home/phanim/harshitrawat/summer/replay_data/mp_finetuning-just_to_get_file_combinations_run-84.xyz \\\n",
      "    --atomic_numbers \\\n",
      "    [3,8,40,57] \\\n",
      "    --multiheads_finetuning \\\n",
      "    True \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz \\\n",
      "    --batch_size \\\n",
      "    2 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --forces_weight \\\n",
      "    0 \\\n",
      "    --energy_weight \\\n",
      "    10 \\\n",
      "    --stress_weight \\\n",
      "    0 \\\n",
      "    --lr \\\n",
      "    0.006 \\\n",
      "    --scheduler_patience \\\n",
      "    4 \\\n",
      "    --clip_grad \\\n",
      "    1 \\\n",
      "    --weight_decay \\\n",
      "    1e-8 \\\n",
      "    --ema_decay \\\n",
      "    0.999 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    78 \\\n",
      "    --E0s \\\n",
      "    {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549} \\\n",
      "    --seed \\\n",
      "    84 \\\n",
      "    --patience \\\n",
      "    8 \\\n",
      "    --restart_latest\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-03 04:18:46.304 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-09-03 04:18:46.305 INFO: MACE version: 0.3.14\n",
      "2025-09-03 04:18:46.720 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py:152: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-03 04:18:47.362 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-09-03 04:18:47.364 INFO: Multihead finetuning mode, setting learning rate to 0.0001 and EMA to True. To use a different learning rate, set --force_mh_ft_lr=True.\n",
      "2025-09-03 04:18:47.365 INFO: Using multiheads finetuning mode, setting learning rate to 0.0001 and EMA to True\n",
      "2025-09-03 04:18:47.365 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-09-03 04:18:47.365 INFO: Using heads: ['Default', 'pt_head']\n",
      "2025-09-03 04:18:47.365 INFO: Using the key specifications to parse data:\n",
      "2025-09-03 04:18:47.365 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-09-03 04:18:47.365 INFO: pt_head: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-09-03 04:18:47.365 INFO: =============    Processing head Default     ===========\n",
      "2025-09-03 04:18:52.788 INFO: Training set 1/1 [energy: 1612, stress: 0, virials: 0, dipole components: 0, head: 1612, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1612, charges: 0]\n",
      "2025-09-03 04:18:52.793 INFO: Total Training set [energy: 1612, stress: 0, virials: 0, dipole components: 0, head: 1612, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1612, charges: 0]\n",
      "2025-09-03 04:18:55.093 INFO: Validation set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-09-03 04:18:55.095 INFO: Total Validation set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-09-03 04:18:55.095 INFO: Total number of configurations: train=1612, valid=705, tests=[],\n",
      "2025-09-03 04:18:55.095 INFO: =============    Processing head pt_head     ===========\n",
      "2025-09-03 04:18:57.569 WARNING: No forces found with key 'REF_forces' in '/home/phanim/harshitrawat/summer/replay_data/mp_finetuning-just_to_get_file_combinations_run-84.xyz'. If this is unexpected, Please change the key name in the command line arguments or ensure that the file contains the required data.\n",
      "2025-09-03 04:18:57.896 INFO: Training set 1/1 [energy: 10000, stress: 0, virials: 0, dipole components: 0, head: 10000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-09-03 04:18:57.924 INFO: Total Training set [energy: 10000, stress: 0, virials: 0, dipole components: 0, head: 10000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-09-03 04:18:57.924 INFO: No validation set provided, splitting training data instead.\n",
      "2025-09-03 04:18:57.928 INFO: Using random 10% of training set for validation with indices saved in: ./valid_indices_84.txt\n",
      "2025-09-03 04:18:57.957 INFO: Random Split Training set [energy: 9000, stress: 0, virials: 0, dipole components: 0, head: 9000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-09-03 04:18:57.960 INFO: Random Split Validation set [energy: 1000, stress: 0, virials: 0, dipole components: 0, head: 1000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 0, charges: 0]\n",
      "2025-09-03 04:18:57.960 INFO: Total number of configurations: train=9000, valid=1000, tests=[],\n",
      "2025-09-03 04:18:57.960 INFO: ==================Using multiheads finetuning mode==================\n",
      "2025-09-03 04:18:57.960 INFO: Total number of configurations in pretraining: train=9000, valid=1000\n",
      "2025-09-03 04:18:57.960 INFO: Using atomic numbers from command line argument\n",
      "2025-09-03 04:18:57.977 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-09-03 04:18:57.977 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-09-03 04:18:57.980 INFO: Atomic Energies used (z: eV) for head Default: {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\n",
      "2025-09-03 04:18:57.980 INFO: Atomic Energies used (z: eV) for head pt_head: {3: -3.482100566595956, 8: -7.28459863421322, 40: -11.846857579882572, 57: -8.933684809576345}\n",
      "2025-09-03 04:18:57.980 INFO: Processing datasets for head 'Default'\n",
      "2025-09-03 04:19:17.160 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-09-03 04:19:25.758 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-09-03 04:19:25.758 INFO: Combined validation datasets for Default\n",
      "2025-09-03 04:19:25.758 INFO: Head 'Default' training dataset size: 1612\n",
      "2025-09-03 04:19:25.759 INFO: Processing datasets for head 'pt_head'\n",
      "2025-09-03 04:19:32.499 INFO: Combining 1 list datasets for head 'pt_head'\n",
      "2025-09-03 04:19:33.203 INFO: Head 'pt_head' training dataset size: 9000\n",
      "2025-09-03 04:19:33.203 INFO: Average number of neighbors: 61.964672446250916\n",
      "2025-09-03 04:19:33.203 INFO: During training the following quantities will be reported: energy, forces, stress\n",
      "2025-09-03 04:19:33.203 INFO: ===========MODEL DETAILS===========\n",
      "2025-09-03 04:19:39.600 WARNING: Standard deviation of the scaling is zero, Changing to no scaling\n",
      "2025-09-03 04:19:39.612 INFO: Loading FOUNDATION model\n",
      "2025-09-03 04:19:39.613 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-09-03 04:19:39.613 INFO: Model configuration extracted from foundation model\n",
      "2025-09-03 04:19:39.613 INFO: Using universal loss function for fine-tuning\n",
      "2025-09-03 04:19:39.613 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-09-03 04:19:39.614 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-09-03 04:19:39.614 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-09-03 04:19:39.614 INFO: Distance transform for radial basis functions: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/checkpoint.py:187: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-03 04:19:40.913 INFO: Total number of parameters: 896586\n",
      "2025-09-03 04:19:40.913 INFO: \n",
      "2025-09-03 04:19:40.914 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-09-03 04:19:40.914 INFO: Using ADAM as parameter optimizer\n",
      "2025-09-03 04:19:40.914 INFO: Batch size: 2\n",
      "2025-09-03 04:19:40.914 INFO: Using Exponential Moving Average with decay: 0.99999\n",
      "2025-09-03 04:19:40.914 INFO: Number of gradient updates: 413868\n",
      "2025-09-03 04:19:40.914 INFO: Learning rate: 0.0001, weight decay: 1e-08\n",
      "2025-09-03 04:19:40.914 INFO: UniversalLoss(energy_weight=10.000, forces_weight=0.000, stress_weight=0.000)\n",
      "2025-09-03 04:19:40.934 WARNING: No SWA checkpoint found, while SWA is enabled. Compare the swa_start parameter and the latest checkpoint.\n",
      "2025-09-03 04:19:40.935 INFO: Loading checkpoint: ./checkpoints/mace_T2_including_replay_w2_combinations_4elems_run-84_epoch-76.pt\n",
      "2025-09-03 04:19:40.986 INFO: Using gradient clipping with tolerance=1.000\n",
      "2025-09-03 04:19:40.986 INFO: \n",
      "2025-09-03 04:19:40.986 INFO: ===========TRAINING===========\n",
      "2025-09-03 04:19:40.986 INFO: Started training, reporting errors on validation set\n",
      "2025-09-03 04:19:40.986 INFO: Loss metrics on validation set\n",
      "2025-09-03 04:20:20.674 INFO: Initial: head: pt_head, loss=0.00000166, RMSE_E_per_atom=    0.58 meV, RMSE_F=None meV / A, RMSE_stress=None meV / A^3\n",
      "2025-09-03 04:21:47.071 INFO: Initial: head: Default, loss=0.00016676, RMSE_E_per_atom=    5.93 meV, RMSE_F= 1300.56 meV / A, RMSE_stress=None meV / A^3\n",
      "2025-09-03 04:35:01.626 INFO: Epoch 76: head: pt_head, loss=0.00000862, RMSE_E_per_atom=    1.31 meV, RMSE_F=None meV / A, RMSE_stress=None meV / A^3\n",
      "2025-09-03 04:36:27.958 INFO: Epoch 76: head: Default, loss=0.00016285, RMSE_E_per_atom=    5.85 meV, RMSE_F= 1298.79 meV / A, RMSE_stress=None meV / A^3\n",
      "2025-09-03 04:49:38.746 INFO: Epoch 77: head: pt_head, loss=0.00000354, RMSE_E_per_atom=    0.84 meV, RMSE_F=None meV / A, RMSE_stress=None meV / A^3\n",
      "2025-09-03 04:51:05.033 INFO: Epoch 77: head: Default, loss=0.00016262, RMSE_E_per_atom=    5.84 meV, RMSE_F= 1294.14 meV / A, RMSE_stress=None meV / A^3\n",
      "2025-09-03 04:51:05.138 INFO: Training complete\n",
      "2025-09-03 04:51:05.139 INFO: \n",
      "2025-09-03 04:51:05.139 INFO: ===========RESULTS===========\n",
      "2025-09-03 04:51:05.142 INFO: Loading checkpoint: ./checkpoints/mace_T2_including_replay_w2_combinations_4elems_run-84_epoch-77.pt\n",
      "2025-09-03 04:51:05.171 INFO: Loaded Stage one model from epoch 77 for evaluation\n",
      "2025-09-03 04:51:05.171 INFO: Saving model to checkpoints/mace_T2_including_replay_w2_combinations_4elems_run-84.model\n",
      "2025-09-03 04:51:05.403 INFO: Compiling model, saving metadata to mace_T2_including_replay_w2_combinations_4elems_compiled.model\n",
      "2025-09-03 04:51:05.986 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-09-03 04:51:05.986 INFO: Skipping evaluation for heads: ['pt_head']\n",
      "2025-09-03 04:51:05.986 INFO: Evaluating train_Default ...\n",
      "2025-09-03 04:53:56.003 INFO: Skipping evaluation of train_pt_head (in skip_heads list)\n",
      "2025-09-03 04:53:56.006 INFO: Evaluating valid_Default ...\n"
     ]
    }
   ],
   "source": [
    "# Now universal MACE finetuning on T2\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    cmd = [\n",
    "        \"mace_run_train\",\n",
    "        # === General settings ===\n",
    "        \"--name\",              \"mace_T2_including_replay_w2_combinations_4elems\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "    # --- MP replay (pretraining head) ---\n",
    "        \"--pt_train_file\",\"/home/phanim/harshitrawat/summer/replay_data/mp_finetuning-just_to_get_file_combinations_run-84.xyz\",              # <- MP replay shortcut\n",
    "        \"--atomic_numbers\",\"[3,8,40,57]\",    # Li, O, Zr, La\n",
    "        \"--multiheads_finetuning\",\"True\",\n",
    "\n",
    "        \"--train_file\",\"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz\",\n",
    "        \"--valid_file\",\"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz\",\n",
    "\n",
    "        \"--batch_size\",        \"2\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "\n",
    "        \"--device\",            \"cuda\",\n",
    "\n",
    "        # === Loss function weights ===\n",
    "        \"--forces_weight\",     \"0\",         # Increased force weight to balance energy better\n",
    "        \"--energy_weight\",     \"10\",   \n",
    "        \"--stress_weight\", \"0\",             # Reduced from 100 → avoid dominance + stabilize energy RMSE\n",
    "\n",
    "        # === Learning setup ===\n",
    "        \"--lr\",                \"0.006\",      # Explicit learning rate (0.0001 is too low → stagnation)\n",
    "        \"--scheduler_patience\",\"4\",          # Reduce LR if val loss doesn’t improve in 3 epochs\n",
    "        \"--clip_grad\",         \"1\",        # Avoid exploding gradients — essential when energy_weight is high\n",
    "        \"--weight_decay\",      \"1e-8\",       # Mild regularization to prevent overfitting\n",
    "\n",
    "        # === EMA helps smooth loss curve ===\n",
    "        \"--ema_decay\",         \"0.999\",     # Smooths validation loss and helps final convergence\n",
    "\n",
    "        # === Domain + training settings ===\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"78\",\n",
    "        \"--E0s\",               \"{3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\",    # Still allowed — could optionally be replaced by manual E0s\n",
    "        \"--seed\",              \"84\",\n",
    "        \"--patience\",     \"8\",\n",
    "\n",
    "        \"--restart_latest\",                   # Resumes from checkpoint if available\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb6ad75-bf11-4eba-992e-b4b59b75213a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ScaleShiftMACE:\n\tsize mismatch for atomic_numbers: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([86]).\n\tsize mismatch for node_embedding.linear.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([11008]).\n\tsize mismatch for atomic_energies_fn.atomic_energies: copying a param with shape torch.Size([2, 4]) from checkpoint, the shape in current model is torch.Size([2, 86]).\n\tsize mismatch for interactions.0.skip_tp.weight: copying a param with shape torch.Size([65536]) from checkpoint, the shape in current model is torch.Size([1409024]).\n\tsize mismatch for interactions.1.skip_tp.weight: copying a param with shape torch.Size([65536]) from checkpoint, the shape in current model is torch.Size([1409024]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.0.weights_max: copying a param with shape torch.Size([4, 23, 128]) from checkpoint, the shape in current model is torch.Size([86, 23, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.0.weights.0: copying a param with shape torch.Size([4, 4, 128]) from checkpoint, the shape in current model is torch.Size([86, 4, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.0.weights.1: copying a param with shape torch.Size([4, 1, 128]) from checkpoint, the shape in current model is torch.Size([86, 1, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.1.weights_max: copying a param with shape torch.Size([4, 51, 128]) from checkpoint, the shape in current model is torch.Size([86, 51, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.1.weights.0: copying a param with shape torch.Size([4, 6, 128]) from checkpoint, the shape in current model is torch.Size([86, 6, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.1.weights.1: copying a param with shape torch.Size([4, 1, 128]) from checkpoint, the shape in current model is torch.Size([86, 1, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.2.weights_max: copying a param with shape torch.Size([4, 65, 128]) from checkpoint, the shape in current model is torch.Size([86, 65, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.2.weights.0: copying a param with shape torch.Size([4, 7, 128]) from checkpoint, the shape in current model is torch.Size([86, 7, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.2.weights.1: copying a param with shape torch.Size([4, 1, 128]) from checkpoint, the shape in current model is torch.Size([86, 1, 128]).\n\tsize mismatch for products.1.symmetric_contractions.contractions.0.weights_max: copying a param with shape torch.Size([4, 23, 128]) from checkpoint, the shape in current model is torch.Size([86, 23, 128]).\n\tsize mismatch for products.1.symmetric_contractions.contractions.0.weights.0: copying a param with shape torch.Size([4, 4, 128]) from checkpoint, the shape in current model is torch.Size([86, 4, 128]).\n\tsize mismatch for products.1.symmetric_contractions.contractions.0.weights.1: copying a param with shape torch.Size([4, 1, 128]) from checkpoint, the shape in current model is torch.Size([86, 1, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m model = torch.load(template_model_path, map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Inject weights from checkpoint\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m missing, unexpected = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmissing keys:\u001b[39m\u001b[33m\"\u001b[39m, missing)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33munexpected keys:\u001b[39m\u001b[33m\"\u001b[39m, unexpected)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for ScaleShiftMACE:\n\tsize mismatch for atomic_numbers: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([86]).\n\tsize mismatch for node_embedding.linear.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([11008]).\n\tsize mismatch for atomic_energies_fn.atomic_energies: copying a param with shape torch.Size([2, 4]) from checkpoint, the shape in current model is torch.Size([2, 86]).\n\tsize mismatch for interactions.0.skip_tp.weight: copying a param with shape torch.Size([65536]) from checkpoint, the shape in current model is torch.Size([1409024]).\n\tsize mismatch for interactions.1.skip_tp.weight: copying a param with shape torch.Size([65536]) from checkpoint, the shape in current model is torch.Size([1409024]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.0.weights_max: copying a param with shape torch.Size([4, 23, 128]) from checkpoint, the shape in current model is torch.Size([86, 23, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.0.weights.0: copying a param with shape torch.Size([4, 4, 128]) from checkpoint, the shape in current model is torch.Size([86, 4, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.0.weights.1: copying a param with shape torch.Size([4, 1, 128]) from checkpoint, the shape in current model is torch.Size([86, 1, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.1.weights_max: copying a param with shape torch.Size([4, 51, 128]) from checkpoint, the shape in current model is torch.Size([86, 51, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.1.weights.0: copying a param with shape torch.Size([4, 6, 128]) from checkpoint, the shape in current model is torch.Size([86, 6, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.1.weights.1: copying a param with shape torch.Size([4, 1, 128]) from checkpoint, the shape in current model is torch.Size([86, 1, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.2.weights_max: copying a param with shape torch.Size([4, 65, 128]) from checkpoint, the shape in current model is torch.Size([86, 65, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.2.weights.0: copying a param with shape torch.Size([4, 7, 128]) from checkpoint, the shape in current model is torch.Size([86, 7, 128]).\n\tsize mismatch for products.0.symmetric_contractions.contractions.2.weights.1: copying a param with shape torch.Size([4, 1, 128]) from checkpoint, the shape in current model is torch.Size([86, 1, 128]).\n\tsize mismatch for products.1.symmetric_contractions.contractions.0.weights_max: copying a param with shape torch.Size([4, 23, 128]) from checkpoint, the shape in current model is torch.Size([86, 23, 128]).\n\tsize mismatch for products.1.symmetric_contractions.contractions.0.weights.0: copying a param with shape torch.Size([4, 4, 128]) from checkpoint, the shape in current model is torch.Size([86, 4, 128]).\n\tsize mismatch for products.1.symmetric_contractions.contractions.0.weights.1: copying a param with shape torch.Size([4, 1, 128]) from checkpoint, the shape in current model is torch.Size([86, 1, 128])."
     ]
    }
   ],
   "source": [
    "from mace.calculators import MACECalculator\n",
    "mace_calc = MACECalculator(model_paths=[\"/home/phanim/harshitrawat/summer/iteration_3-Copy1/mace_T2_including_replay_w2_combinations_4elems_compiled.model\"], device=\"cpu\")  # or \"cpu\"\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "from pymatgen.core import Structure\n",
    "adaptor = AseAtomsAdaptor()\n",
    "\n",
    "pmg_structure = Structure.from_file(\"/home/phanim/harshitrawat/summer/formation_energy/cifs/Li.cif\")  # e.g. for Li\n",
    "ase_atoms = adaptor.get_atoms(pmg_structure)\n",
    "ase_atoms.calc = mace_calc\n",
    "total_energy = ase_atoms.get_potential_energy()\n",
    "mu_model_Li = total_energy / len(ase_atoms)\n",
    "print(f\"Li: μ_model = {mu_model_Li:.6f} eV/atom\")\n",
    "# Let us do this for La, Zr, and O as well\n",
    "pmg_structure = Structure.from_file(\"/home/phanim/harshitrawat/summer/formation_energy/cifs/La.cif\")\n",
    "ase_atoms = adaptor.get_atoms(pmg_structure)\n",
    "ase_atoms.calc = mace_calc\n",
    "total_energy = ase_atoms.get_potential_energy()\n",
    "mu_model_La = total_energy / len(ase_atoms)\n",
    "print(f\"La: μ_model = {mu_model_La:.6f} eV/atom\")\n",
    "pmg_structure = Structure.from_file(\"/home/phanim/harshitrawat/summer/formation_energy/cifs/Zr.cif\")\n",
    "ase_atoms = adaptor.get_atoms(pmg_structure)\n",
    "ase_atoms.calc = mace_calc\n",
    "total_energy = ase_atoms.get_potential_energy()\n",
    "mu_model_Zr = total_energy / len(ase_atoms)\n",
    "print(f\"Zr: μ_model = {mu_model_Zr:.6f} eV/atom\")\n",
    "pmg_structure = Structure.from_file(\"/home/phanim/harshitrawat/summer/formation_energy/cifs/O2.cif\")  # Needs to be a periodic solid O2 structure\n",
    "ase_atoms = adaptor.get_atoms(pmg_structure)\n",
    "ase_atoms.calc = mace_calc\n",
    "total_energy = ase_atoms.get_potential_energy()\n",
    "mu_model_O = total_energy / len(ase_atoms)\n",
    "print(f\"O: μ_model = {mu_model_O:.6f} eV/atom\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7437e-b02c-4ecd-8147-ea255c5ee281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "mu_mace = {\n",
    "    \"Li\": mu_model_Li,\n",
    "    \"La\": mu_model_La,\n",
    "    \"Zr\": mu_model_Zr,\n",
    "    \"O\":  mu_model_O,\n",
    "}\n",
    "\n",
    "# ---- 3. CIF files ------------------------------------------------------\n",
    "cif_dir = \"./cifs\"\n",
    "compounds = {\n",
    "    \"mp-841.cif\": \"Li2O2\",\n",
    "    \"mp-1960.cif\": \"Li2O\",\n",
    "    \"mp-942733.cif\": \"Li7La3Zr2O12\",\n",
    "    \"mp-2858.cif\": \"ZrO2\",\n",
    "    \"mp-1968.cif\": \"La2O3\",\n",
    "}\n",
    "\n",
    "# ---- 4. Predict formation energy per atom -----------------------------\n",
    "for fname, label in compounds.items():\n",
    "    struct = Structure.from_file(os.path.join(cif_dir, fname))\n",
    "    comp = struct.composition\n",
    "    n_atoms = comp.num_atoms\n",
    "\n",
    "    # Convert to ASE\n",
    "    ase_atoms = AseAtomsAdaptor.get_atoms(struct)\n",
    "\n",
    "    # Assign calculator and predict energy\n",
    "    ase_atoms.calc = mace_calc\n",
    "    energy_total = ase_atoms.get_potential_energy()  # eV (total)\n",
    "\n",
    "    # Reference energy from MACE chemical potentials\n",
    "    ref_total = sum(comp[el] * mu_mace[el.symbol] for el in comp.elements)\n",
    "\n",
    "    # Formation energy per atom\n",
    "    e_form = (energy_total - ref_total) / n_atoms\n",
    "\n",
    "    print(f\"{label:15s}:  E_form (mace-omat-0-medium) = {e_form: .6f} eV/atom\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969cc92-0be5-41bc-b049-30e28b9fc044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61641833-122a-482c-bdbe-2b6354c6e15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
