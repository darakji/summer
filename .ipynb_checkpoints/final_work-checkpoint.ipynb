{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc28fb93-7a79-4ef7-b935-8c8bba72dee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ase.io import read, write\n",
    "from mace.calculators import MACECalculator\n",
    "from chgnet.model.model import CHGNet\n",
    "from chgnet.model.dynamics import CHGNetCalculator\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a55885-bcdf-483b-ac64-c2063d212a98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/chgnet/model/model.py:673: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHGNet v0.3.0 initialized with 412,525 parameters\n",
      "CHGNet will run on cpu\n",
      "CHGNet will run on cuda:0\n",
      "\n",
      "📂 Labeling: /home/phanim/harshitrawat/summer/md/mdcifs\n",
      "✅ 1/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0000.cif\n",
      "✅ 2/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0001.cif\n",
      "✅ 3/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0002.cif\n",
      "✅ 4/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0003.cif\n",
      "✅ 5/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0004.cif\n",
      "✅ 6/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0005.cif\n",
      "✅ 7/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0006.cif\n",
      "✅ 8/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0007.cif\n",
      "✅ 9/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0008.cif\n",
      "✅ 10/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0009.cif\n",
      "✅ 11/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0010.cif\n",
      "✅ 12/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0011.cif\n",
      "✅ 13/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0012.cif\n",
      "✅ 14/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0013.cif\n",
      "✅ 15/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0014.cif\n",
      "✅ 16/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0015.cif\n",
      "✅ 17/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0016.cif\n",
      "✅ 18/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0017.cif\n",
      "✅ 19/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0018.cif\n",
      "✅ 20/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0019.cif\n",
      "✅ 21/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0020.cif\n",
      "✅ 22/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0021.cif\n",
      "✅ 23/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0022.cif\n",
      "✅ 24/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0023.cif\n",
      "✅ 25/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0024.cif\n",
      "✅ 26/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0025.cif\n",
      "✅ 27/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0026.cif\n",
      "✅ 28/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0027.cif\n",
      "✅ 29/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0028.cif\n",
      "✅ 30/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0029.cif\n",
      "✅ 31/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0030.cif\n",
      "✅ 32/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0031.cif\n",
      "✅ 33/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0032.cif\n",
      "✅ 34/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0033.cif\n",
      "✅ 35/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0034.cif\n",
      "✅ 36/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0035.cif\n",
      "✅ 37/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0036.cif\n",
      "✅ 38/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0037.cif\n",
      "✅ 39/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0038.cif\n",
      "✅ 40/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0039.cif\n",
      "✅ 41/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0040.cif\n",
      "✅ 42/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0041.cif\n",
      "✅ 43/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0042.cif\n",
      "✅ 44/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0043.cif\n",
      "✅ 45/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0044.cif\n",
      "✅ 46/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0045.cif\n",
      "✅ 47/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0046.cif\n",
      "✅ 48/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0047.cif\n",
      "✅ 49/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0048.cif\n",
      "✅ 50/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0049.cif\n",
      "✅ 51/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0050.cif\n",
      "✅ 52/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0051.cif\n",
      "✅ 53/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0052.cif\n",
      "✅ 54/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0053.cif\n",
      "✅ 55/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0054.cif\n",
      "✅ 56/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0055.cif\n",
      "✅ 57/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0056.cif\n",
      "✅ 58/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0057.cif\n",
      "✅ 59/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0058.cif\n",
      "✅ 60/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0059.cif\n",
      "✅ 61/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0060.cif\n",
      "✅ 62/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0061.cif\n",
      "✅ 63/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0062.cif\n",
      "✅ 64/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0063.cif\n",
      "✅ 65/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0064.cif\n",
      "✅ 66/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0065.cif\n",
      "✅ 67/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0066.cif\n",
      "✅ 68/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0067.cif\n",
      "✅ 69/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0068.cif\n",
      "✅ 70/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0069.cif\n",
      "✅ 71/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0070.cif\n",
      "✅ 72/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0071.cif\n",
      "✅ 73/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0072.cif\n",
      "✅ 74/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0073.cif\n",
      "✅ 75/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0074.cif\n",
      "✅ 76/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0075.cif\n",
      "✅ 77/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0076.cif\n",
      "✅ 78/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0077.cif\n",
      "✅ 79/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0078.cif\n",
      "✅ 80/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0079.cif\n",
      "✅ 81/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0080.cif\n",
      "✅ 82/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0081.cif\n",
      "✅ 83/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0082.cif\n",
      "✅ 84/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0083.cif\n",
      "✅ 85/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0084.cif\n",
      "✅ 86/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0085.cif\n",
      "✅ 87/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0086.cif\n",
      "✅ 88/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0087.cif\n",
      "✅ 89/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0088.cif\n",
      "✅ 90/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0089.cif\n",
      "✅ 91/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0090.cif\n",
      "✅ 92/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0091.cif\n",
      "✅ 93/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0092.cif\n",
      "✅ 94/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0093.cif\n",
      "✅ 95/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0094.cif\n",
      "✅ 96/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0095.cif\n",
      "✅ 97/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0096.cif\n",
      "✅ 98/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0097.cif\n",
      "✅ 99/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0098.cif\n",
      "✅ 100/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0099.cif\n",
      "✅ 101/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0100.cif\n",
      "✅ 102/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0101.cif\n",
      "✅ 103/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0102.cif\n",
      "✅ 104/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0103.cif\n",
      "✅ 105/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0104.cif\n",
      "✅ 106/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0105.cif\n",
      "✅ 107/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0106.cif\n",
      "✅ 108/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0107.cif\n",
      "✅ 109/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0108.cif\n",
      "✅ 110/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0109.cif\n",
      "✅ 111/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0110.cif\n",
      "✅ 112/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0111.cif\n",
      "✅ 113/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0112.cif\n",
      "✅ 114/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0113.cif\n",
      "✅ 115/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0114.cif\n",
      "✅ 116/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0115.cif\n",
      "✅ 117/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0116.cif\n",
      "✅ 118/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0117.cif\n",
      "✅ 119/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0118.cif\n",
      "✅ 120/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0119.cif\n",
      "✅ 121/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0120.cif\n",
      "✅ 122/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0121.cif\n",
      "✅ 123/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0122.cif\n",
      "✅ 124/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0123.cif\n",
      "✅ 125/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0124.cif\n",
      "✅ 126/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0125.cif\n",
      "✅ 127/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0126.cif\n",
      "✅ 128/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0127.cif\n",
      "✅ 129/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0128.cif\n",
      "✅ 130/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0129.cif\n",
      "✅ 131/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0130.cif\n",
      "✅ 132/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0131.cif\n",
      "✅ 133/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0132.cif\n",
      "✅ 134/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0133.cif\n",
      "✅ 135/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0134.cif\n",
      "✅ 136/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0135.cif\n",
      "✅ 137/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0136.cif\n",
      "✅ 138/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0137.cif\n",
      "✅ 139/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0138.cif\n",
      "✅ 140/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0139.cif\n",
      "✅ 141/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0140.cif\n",
      "✅ 142/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0141.cif\n",
      "✅ 143/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0142.cif\n",
      "✅ 144/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0143.cif\n",
      "✅ 145/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0144.cif\n",
      "✅ 146/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0145.cif\n",
      "✅ 147/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0146.cif\n",
      "✅ 148/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0147.cif\n",
      "✅ 149/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0148.cif\n",
      "✅ 150/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0149.cif\n",
      "✅ 151/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0150.cif\n",
      "✅ 152/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0151.cif\n",
      "✅ 153/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0152.cif\n",
      "✅ 154/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0153.cif\n",
      "✅ 155/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0154.cif\n",
      "✅ 156/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0155.cif\n",
      "✅ 157/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0156.cif\n",
      "✅ 158/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0157.cif\n",
      "✅ 159/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0158.cif\n",
      "✅ 160/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0159.cif\n",
      "✅ 161/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0160.cif\n",
      "✅ 162/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0161.cif\n",
      "✅ 163/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0162.cif\n",
      "✅ 164/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0163.cif\n",
      "✅ 165/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0164.cif\n",
      "✅ 166/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0165.cif\n",
      "✅ 167/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0166.cif\n",
      "✅ 168/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0167.cif\n",
      "✅ 169/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0168.cif\n",
      "✅ 170/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0169.cif\n",
      "✅ 171/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0170.cif\n",
      "✅ 172/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0171.cif\n",
      "✅ 173/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0172.cif\n",
      "✅ 174/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0173.cif\n",
      "✅ 175/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0174.cif\n",
      "✅ 176/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0175.cif\n",
      "✅ 177/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0176.cif\n",
      "✅ 178/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0177.cif\n",
      "✅ 179/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0178.cif\n",
      "✅ 180/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0179.cif\n",
      "✅ 181/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0180.cif\n",
      "✅ 182/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0181.cif\n",
      "✅ 183/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0182.cif\n",
      "✅ 184/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0183.cif\n",
      "✅ 185/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0184.cif\n",
      "✅ 186/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0185.cif\n",
      "✅ 187/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0186.cif\n",
      "✅ 188/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0187.cif\n",
      "✅ 189/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0188.cif\n",
      "✅ 190/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0189.cif\n",
      "✅ 191/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0190.cif\n",
      "✅ 192/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0191.cif\n",
      "✅ 193/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0192.cif\n",
      "✅ 194/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0193.cif\n",
      "✅ 195/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0194.cif\n",
      "✅ 196/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0195.cif\n",
      "✅ 197/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0196.cif\n",
      "✅ 198/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0197.cif\n",
      "✅ 199/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0198.cif\n",
      "✅ 200/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0199.cif\n",
      "✅ 201/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T300_0200.cif\n",
      "✅ 202/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0000.cif\n",
      "✅ 203/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0001.cif\n",
      "✅ 204/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0002.cif\n",
      "✅ 205/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0003.cif\n",
      "✅ 206/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0004.cif\n",
      "✅ 207/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0005.cif\n",
      "✅ 208/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0006.cif\n",
      "✅ 209/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0007.cif\n",
      "✅ 210/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0008.cif\n",
      "✅ 211/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0009.cif\n",
      "✅ 212/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0010.cif\n",
      "✅ 213/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0011.cif\n",
      "✅ 214/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0012.cif\n",
      "✅ 215/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0013.cif\n",
      "✅ 216/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0014.cif\n",
      "✅ 217/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0015.cif\n",
      "✅ 218/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0016.cif\n",
      "✅ 219/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0017.cif\n",
      "✅ 220/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0018.cif\n",
      "✅ 221/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0019.cif\n",
      "✅ 222/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0020.cif\n",
      "✅ 223/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0021.cif\n",
      "✅ 224/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0022.cif\n",
      "✅ 225/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0023.cif\n",
      "✅ 226/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0024.cif\n",
      "✅ 227/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0025.cif\n",
      "✅ 228/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0026.cif\n",
      "✅ 229/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0027.cif\n",
      "✅ 230/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0028.cif\n",
      "✅ 231/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0029.cif\n",
      "✅ 232/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0030.cif\n",
      "✅ 233/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0031.cif\n",
      "✅ 234/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0032.cif\n",
      "✅ 235/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0033.cif\n",
      "✅ 236/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0034.cif\n",
      "✅ 237/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0035.cif\n",
      "✅ 238/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0036.cif\n",
      "✅ 239/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0037.cif\n",
      "✅ 240/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0038.cif\n",
      "✅ 241/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0039.cif\n",
      "✅ 242/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0040.cif\n",
      "✅ 243/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0041.cif\n",
      "✅ 244/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0042.cif\n",
      "✅ 245/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0043.cif\n",
      "✅ 246/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0044.cif\n",
      "✅ 247/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0045.cif\n",
      "✅ 248/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0046.cif\n",
      "✅ 249/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0047.cif\n",
      "✅ 250/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0048.cif\n",
      "✅ 251/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0049.cif\n",
      "✅ 252/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0050.cif\n",
      "✅ 253/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0051.cif\n",
      "✅ 254/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0052.cif\n",
      "✅ 255/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0053.cif\n",
      "✅ 256/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0054.cif\n",
      "✅ 257/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0055.cif\n",
      "✅ 258/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0056.cif\n",
      "✅ 259/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0057.cif\n",
      "✅ 260/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0058.cif\n",
      "✅ 261/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0059.cif\n",
      "✅ 262/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0060.cif\n",
      "✅ 263/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0061.cif\n",
      "✅ 264/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0062.cif\n",
      "✅ 265/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0063.cif\n",
      "✅ 266/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0064.cif\n",
      "✅ 267/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0065.cif\n",
      "✅ 268/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0066.cif\n",
      "✅ 269/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0067.cif\n",
      "✅ 270/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0068.cif\n",
      "✅ 271/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0069.cif\n",
      "✅ 272/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0070.cif\n",
      "✅ 273/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0071.cif\n",
      "✅ 274/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0072.cif\n",
      "✅ 275/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0073.cif\n",
      "✅ 276/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0074.cif\n",
      "✅ 277/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0075.cif\n",
      "✅ 278/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0076.cif\n",
      "✅ 279/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0077.cif\n",
      "✅ 280/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0078.cif\n",
      "✅ 281/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0079.cif\n",
      "✅ 282/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0080.cif\n",
      "✅ 283/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0081.cif\n",
      "✅ 284/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0082.cif\n",
      "✅ 285/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0083.cif\n",
      "✅ 286/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0084.cif\n",
      "✅ 287/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0085.cif\n",
      "✅ 288/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0086.cif\n",
      "✅ 289/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0087.cif\n",
      "✅ 290/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0088.cif\n",
      "✅ 291/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0089.cif\n",
      "✅ 292/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0090.cif\n",
      "✅ 293/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0091.cif\n",
      "✅ 294/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0092.cif\n",
      "✅ 295/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0093.cif\n",
      "✅ 296/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0094.cif\n",
      "✅ 297/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0095.cif\n",
      "✅ 298/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0096.cif\n",
      "✅ 299/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0097.cif\n",
      "✅ 300/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0098.cif\n",
      "✅ 301/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0099.cif\n",
      "✅ 302/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0100.cif\n",
      "✅ 303/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0101.cif\n",
      "✅ 304/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0102.cif\n",
      "✅ 305/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0103.cif\n",
      "✅ 306/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0104.cif\n",
      "✅ 307/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0105.cif\n",
      "✅ 308/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0106.cif\n",
      "✅ 309/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0107.cif\n",
      "✅ 310/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0108.cif\n",
      "✅ 311/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0109.cif\n",
      "✅ 312/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0110.cif\n",
      "✅ 313/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0111.cif\n",
      "✅ 314/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0112.cif\n",
      "✅ 315/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0113.cif\n",
      "✅ 316/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0114.cif\n",
      "✅ 317/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0115.cif\n",
      "✅ 318/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0116.cif\n",
      "✅ 319/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0117.cif\n",
      "✅ 320/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0118.cif\n",
      "✅ 321/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0119.cif\n",
      "✅ 322/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0120.cif\n",
      "✅ 323/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0121.cif\n",
      "✅ 324/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0122.cif\n",
      "✅ 325/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0123.cif\n",
      "✅ 326/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0124.cif\n",
      "✅ 327/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0125.cif\n",
      "✅ 328/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0126.cif\n",
      "✅ 329/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0127.cif\n",
      "✅ 330/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0128.cif\n",
      "✅ 331/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0129.cif\n",
      "✅ 332/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0130.cif\n",
      "✅ 333/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0131.cif\n",
      "✅ 334/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0132.cif\n",
      "✅ 335/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0133.cif\n",
      "✅ 336/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0134.cif\n",
      "✅ 337/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0135.cif\n",
      "✅ 338/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0136.cif\n",
      "✅ 339/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0137.cif\n",
      "✅ 340/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0138.cif\n",
      "✅ 341/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0139.cif\n",
      "✅ 342/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0140.cif\n",
      "✅ 343/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0141.cif\n",
      "✅ 344/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0142.cif\n",
      "✅ 345/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0143.cif\n",
      "✅ 346/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0144.cif\n",
      "✅ 347/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0145.cif\n",
      "✅ 348/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0146.cif\n",
      "✅ 349/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0147.cif\n",
      "✅ 350/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0148.cif\n",
      "✅ 351/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0149.cif\n",
      "✅ 352/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0150.cif\n",
      "✅ 353/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0151.cif\n",
      "✅ 354/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0152.cif\n",
      "✅ 355/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0153.cif\n",
      "✅ 356/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0154.cif\n",
      "✅ 357/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0155.cif\n",
      "✅ 358/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0156.cif\n",
      "✅ 359/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0157.cif\n",
      "✅ 360/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0158.cif\n",
      "✅ 361/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0159.cif\n",
      "✅ 362/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0160.cif\n",
      "✅ 363/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0161.cif\n",
      "✅ 364/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0162.cif\n",
      "✅ 365/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0163.cif\n",
      "✅ 366/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0164.cif\n",
      "✅ 367/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0165.cif\n",
      "✅ 368/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0166.cif\n",
      "✅ 369/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0167.cif\n",
      "✅ 370/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0168.cif\n",
      "✅ 371/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0169.cif\n",
      "✅ 372/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0170.cif\n",
      "✅ 373/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0171.cif\n",
      "✅ 374/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0172.cif\n",
      "✅ 375/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0173.cif\n",
      "✅ 376/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0174.cif\n",
      "✅ 377/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0175.cif\n",
      "✅ 378/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0176.cif\n",
      "✅ 379/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0177.cif\n",
      "✅ 380/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0178.cif\n",
      "✅ 381/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0179.cif\n",
      "✅ 382/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0180.cif\n",
      "✅ 383/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0181.cif\n",
      "✅ 384/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0182.cif\n",
      "✅ 385/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0183.cif\n",
      "✅ 386/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0184.cif\n",
      "✅ 387/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0185.cif\n",
      "✅ 388/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0186.cif\n",
      "✅ 389/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0187.cif\n",
      "✅ 390/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0188.cif\n",
      "✅ 391/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0189.cif\n",
      "✅ 392/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0190.cif\n",
      "✅ 393/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0191.cif\n",
      "✅ 394/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0192.cif\n",
      "✅ 395/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0193.cif\n",
      "✅ 396/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0194.cif\n",
      "✅ 397/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0195.cif\n",
      "✅ 398/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0196.cif\n",
      "✅ 399/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0197.cif\n",
      "✅ 400/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0198.cif\n",
      "✅ 401/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0199.cif\n",
      "✅ 402/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_100_slab_heavy_T450_0200.cif\n",
      "✅ 403/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0000.cif\n",
      "✅ 404/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0001.cif\n",
      "✅ 405/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0002.cif\n",
      "✅ 406/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0003.cif\n",
      "✅ 407/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0004.cif\n",
      "✅ 408/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0005.cif\n",
      "✅ 409/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0006.cif\n",
      "✅ 410/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0007.cif\n",
      "✅ 411/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0008.cif\n",
      "✅ 412/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0009.cif\n",
      "✅ 413/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0010.cif\n",
      "✅ 414/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0011.cif\n",
      "✅ 415/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0012.cif\n",
      "✅ 416/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0013.cif\n",
      "✅ 417/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0014.cif\n",
      "✅ 418/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0015.cif\n",
      "✅ 419/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0016.cif\n",
      "✅ 420/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0017.cif\n",
      "✅ 421/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0018.cif\n",
      "✅ 422/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0019.cif\n",
      "✅ 423/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0020.cif\n",
      "✅ 424/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0021.cif\n",
      "✅ 425/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0022.cif\n",
      "✅ 426/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0023.cif\n",
      "✅ 427/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0024.cif\n",
      "✅ 428/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0025.cif\n",
      "✅ 429/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0026.cif\n",
      "✅ 430/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0027.cif\n",
      "✅ 431/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0028.cif\n",
      "✅ 432/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0029.cif\n",
      "✅ 433/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0030.cif\n",
      "✅ 434/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0031.cif\n",
      "✅ 435/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0032.cif\n",
      "✅ 436/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0033.cif\n",
      "✅ 437/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0034.cif\n",
      "✅ 438/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0035.cif\n",
      "✅ 439/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0036.cif\n",
      "✅ 440/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0037.cif\n",
      "✅ 441/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0038.cif\n",
      "✅ 442/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0039.cif\n",
      "✅ 443/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0040.cif\n",
      "✅ 444/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0041.cif\n",
      "✅ 445/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0042.cif\n",
      "✅ 446/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0043.cif\n",
      "✅ 447/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0044.cif\n",
      "✅ 448/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0045.cif\n",
      "✅ 449/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0046.cif\n",
      "✅ 450/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0047.cif\n",
      "✅ 451/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0048.cif\n",
      "✅ 452/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0049.cif\n",
      "✅ 453/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0050.cif\n",
      "✅ 454/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0051.cif\n",
      "✅ 455/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0052.cif\n",
      "✅ 456/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0053.cif\n",
      "✅ 457/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0054.cif\n",
      "✅ 458/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0055.cif\n",
      "✅ 459/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0056.cif\n",
      "✅ 460/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0057.cif\n",
      "✅ 461/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0058.cif\n",
      "✅ 462/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0059.cif\n",
      "✅ 463/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0060.cif\n",
      "✅ 464/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0061.cif\n",
      "✅ 465/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0062.cif\n",
      "✅ 466/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0063.cif\n",
      "✅ 467/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0064.cif\n",
      "✅ 468/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0065.cif\n",
      "✅ 469/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0066.cif\n",
      "✅ 470/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0067.cif\n",
      "✅ 471/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0068.cif\n",
      "✅ 472/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0069.cif\n",
      "✅ 473/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0070.cif\n",
      "✅ 474/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0071.cif\n",
      "✅ 475/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0072.cif\n",
      "✅ 476/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0073.cif\n",
      "✅ 477/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0074.cif\n",
      "✅ 478/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0075.cif\n",
      "✅ 479/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0076.cif\n",
      "✅ 480/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0077.cif\n",
      "✅ 481/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0078.cif\n",
      "✅ 482/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0079.cif\n",
      "✅ 483/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0080.cif\n",
      "✅ 484/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0081.cif\n",
      "✅ 485/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0082.cif\n",
      "✅ 486/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0083.cif\n",
      "✅ 487/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0084.cif\n",
      "✅ 488/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0085.cif\n",
      "✅ 489/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0086.cif\n",
      "✅ 490/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0087.cif\n",
      "✅ 491/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0088.cif\n",
      "✅ 492/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0089.cif\n",
      "✅ 493/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0090.cif\n",
      "✅ 494/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0091.cif\n",
      "✅ 495/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0092.cif\n",
      "✅ 496/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0093.cif\n",
      "✅ 497/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0094.cif\n",
      "✅ 498/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0095.cif\n",
      "✅ 499/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0096.cif\n",
      "✅ 500/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0097.cif\n",
      "✅ 501/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0098.cif\n",
      "✅ 502/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0099.cif\n",
      "✅ 503/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0100.cif\n",
      "✅ 504/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0101.cif\n",
      "✅ 505/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0102.cif\n",
      "✅ 506/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0103.cif\n",
      "✅ 507/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0104.cif\n",
      "✅ 508/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0105.cif\n",
      "✅ 509/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0106.cif\n",
      "✅ 510/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0107.cif\n",
      "✅ 511/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0108.cif\n",
      "✅ 512/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0109.cif\n",
      "✅ 513/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0110.cif\n",
      "✅ 514/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0111.cif\n",
      "✅ 515/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0112.cif\n",
      "✅ 516/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0113.cif\n",
      "✅ 517/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0114.cif\n",
      "✅ 518/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0115.cif\n",
      "✅ 519/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0116.cif\n",
      "✅ 520/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0117.cif\n",
      "✅ 521/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0118.cif\n",
      "✅ 522/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0119.cif\n",
      "✅ 523/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0120.cif\n",
      "✅ 524/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0121.cif\n",
      "✅ 525/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0122.cif\n",
      "✅ 526/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0123.cif\n",
      "✅ 527/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0124.cif\n",
      "✅ 528/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0125.cif\n",
      "✅ 529/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0126.cif\n",
      "✅ 530/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0127.cif\n",
      "✅ 531/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0128.cif\n",
      "✅ 532/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0129.cif\n",
      "✅ 533/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0130.cif\n",
      "✅ 534/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0131.cif\n",
      "✅ 535/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0132.cif\n",
      "✅ 536/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0133.cif\n",
      "✅ 537/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0134.cif\n",
      "✅ 538/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0135.cif\n",
      "✅ 539/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0136.cif\n",
      "✅ 540/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0137.cif\n",
      "✅ 541/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0138.cif\n",
      "✅ 542/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0139.cif\n",
      "✅ 543/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0140.cif\n",
      "✅ 544/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0141.cif\n",
      "✅ 545/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0142.cif\n",
      "✅ 546/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0143.cif\n",
      "✅ 547/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0144.cif\n",
      "✅ 548/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0145.cif\n",
      "✅ 549/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0146.cif\n",
      "✅ 550/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0147.cif\n",
      "✅ 551/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0148.cif\n",
      "✅ 552/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0149.cif\n",
      "✅ 553/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0150.cif\n",
      "✅ 554/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0151.cif\n",
      "✅ 555/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0152.cif\n",
      "✅ 556/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0153.cif\n",
      "✅ 557/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0154.cif\n",
      "✅ 558/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0155.cif\n",
      "✅ 559/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0156.cif\n",
      "✅ 560/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0157.cif\n",
      "✅ 561/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0158.cif\n",
      "✅ 562/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0159.cif\n",
      "✅ 563/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0160.cif\n",
      "✅ 564/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0161.cif\n",
      "✅ 565/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0162.cif\n",
      "✅ 566/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0163.cif\n",
      "✅ 567/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0164.cif\n",
      "✅ 568/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0165.cif\n",
      "✅ 569/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0166.cif\n",
      "✅ 570/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0167.cif\n",
      "✅ 571/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0168.cif\n",
      "✅ 572/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0169.cif\n",
      "✅ 573/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0170.cif\n",
      "✅ 574/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0171.cif\n",
      "✅ 575/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0172.cif\n",
      "✅ 576/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0173.cif\n",
      "✅ 577/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0174.cif\n",
      "✅ 578/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0175.cif\n",
      "✅ 579/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0176.cif\n",
      "✅ 580/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0177.cif\n",
      "✅ 581/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0178.cif\n",
      "✅ 582/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0179.cif\n",
      "✅ 583/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0180.cif\n",
      "✅ 584/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0181.cif\n",
      "✅ 585/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0182.cif\n",
      "✅ 586/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0183.cif\n",
      "✅ 587/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0184.cif\n",
      "✅ 588/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0185.cif\n",
      "✅ 589/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0186.cif\n",
      "✅ 590/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0187.cif\n",
      "✅ 591/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0188.cif\n",
      "✅ 592/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0189.cif\n",
      "✅ 593/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0190.cif\n",
      "✅ 594/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0191.cif\n",
      "✅ 595/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0192.cif\n",
      "✅ 596/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0193.cif\n",
      "✅ 597/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0194.cif\n",
      "✅ 598/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0195.cif\n",
      "✅ 599/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0196.cif\n",
      "✅ 600/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0197.cif\n",
      "✅ 601/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0198.cif\n",
      "✅ 602/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0199.cif\n",
      "✅ 603/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T300_0200.cif\n",
      "✅ 604/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0000.cif\n",
      "✅ 605/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0001.cif\n",
      "✅ 606/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0002.cif\n",
      "✅ 607/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0003.cif\n",
      "✅ 608/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0004.cif\n",
      "✅ 609/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0005.cif\n",
      "✅ 610/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0006.cif\n",
      "✅ 611/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0007.cif\n",
      "✅ 612/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0008.cif\n",
      "✅ 613/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0009.cif\n",
      "✅ 614/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0010.cif\n",
      "✅ 615/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0011.cif\n",
      "✅ 616/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0012.cif\n",
      "✅ 617/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0013.cif\n",
      "✅ 618/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0014.cif\n",
      "✅ 619/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0015.cif\n",
      "✅ 620/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0016.cif\n",
      "✅ 621/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0017.cif\n",
      "✅ 622/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0018.cif\n",
      "✅ 623/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0019.cif\n",
      "✅ 624/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0020.cif\n",
      "✅ 625/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0021.cif\n",
      "✅ 626/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0022.cif\n",
      "✅ 627/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0023.cif\n",
      "✅ 628/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0024.cif\n",
      "✅ 629/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0025.cif\n",
      "✅ 630/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0026.cif\n",
      "✅ 631/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0027.cif\n",
      "✅ 632/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0028.cif\n",
      "✅ 633/6030 — cellrelaxed_LLZO_001_Zr_code93_sto__Li_110_slab_heavy_T450_0029.cif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from ase.io import read\n",
    "from chgnet.model.model import CHGNet\n",
    "from chgnet.model.dynamics import CHGNetCalculator\n",
    "import torch\n",
    "\n",
    "# === Setup ===\n",
    "folders = {\n",
    "    \"/home/phanim/harshitrawat/summer/md/mdcifs\": \"/home/phanim/harshitrawat/summer/final_work/mdinfo_chgnet_predictions_forces.json\",\n",
    "    \"/home/phanim/harshitrawat/summer/md/mdcifs_strained_perturbed\": \"/home/phanim/harshitrawat/summer/final_work/strain_perturb_chgnet_predictions_forces.json\"\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\")  # or whichever MIG slice is active\n",
    "\n",
    "# === Load CHGNet ===\n",
    "model = CHGNet.load(use_device=\"cpu\", verbose=True)\n",
    "model = model.to(device)\n",
    "calc = CHGNetCalculator(model=model, use_device=device)\n",
    "\n",
    "def extract_info_from_cif(cif_path):\n",
    "    try:\n",
    "        atoms = read(cif_path)\n",
    "        atoms.calc = calc\n",
    "        return {\n",
    "            \"file\": os.path.basename(cif_path),\n",
    "            \"energy_eV\": atoms.get_potential_energy(),\n",
    "            \"forces_per_atom_eV_per_A\": atoms.get_forces().tolist(),\n",
    "            \"stress_tensor\": atoms.get_stress(voigt=False).tolist(),\n",
    "            \"magmom_total\": atoms.get_magnetic_moment() if \"magmom\" in atoms.arrays else None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"file\": os.path.basename(cif_path),\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# === Label and Save ===\n",
    "for folder, out_json in folders.items():\n",
    "    print(f\"\\n📂 Labeling: {folder}\")\n",
    "    results = []\n",
    "    cif_files = sorted([f for f in os.listdir(folder) if f.endswith(\".cif\")])\n",
    "\n",
    "    for i, fname in enumerate(cif_files):\n",
    "        full_path = os.path.join(folder, fname)\n",
    "        result = extract_info_from_cif(full_path)\n",
    "        results.append(result)\n",
    "        if \"error\" in result:\n",
    "            print(f\"❌ {fname} — {result['error']}\")\n",
    "        else:\n",
    "            print(f\"✅ {i+1}/{len(cif_files)} — {fname}\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_json), exist_ok=True)\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"🧾 Saved {len(results)} entries to: {out_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32bf1b6c-e0ee-4e80-8d3c-060ae1479d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np\n",
    "import pandas as pd\n",
    "from ase.io import read, write\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === Paths ===\n",
    "json_paths = [\n",
    "    \"/home/phanim/harshitrawat/summer/final_work/mdinfo_chgnet_predictions_forces.json\",\n",
    "    \"/home/phanim/harshitrawat/summer/final_work/strain_perturb_chgnet_predictions_forces.json\"\n",
    "]\n",
    "base_cif_dir = \"/home/phanim/harshitrawat/summer/md/mdcifs\"\n",
    "pert_cif_dir = \"/home/phanim/harshitrawat/summer/md/mdcifs_strained_perturbed\"\n",
    "out_folder = \"/home/phanim/harshitrawat/summer/final_work\"\n",
    "\n",
    "# === Load JSON and match with CIFs ===\n",
    "entries = []\n",
    "for path in json_paths:\n",
    "    with open(path) as f:\n",
    "        entries.extend(json.load(f))\n",
    "\n",
    "entries = [e for e in entries if \"error\" not in e]\n",
    "\n",
    "# === Split into T1 and T2 ===\n",
    "train_entries, val_entries = train_test_split(entries, test_size=0.1, random_state=42)\n",
    "\n",
    "def make_extxyz(entries, outfile):\n",
    "    print(\"extxyz process started\")\n",
    "    atoms_list = []\n",
    "    for entry in entries:\n",
    "        fname = entry[\"file\"]\n",
    "        cif_path = os.path.join(pert_cif_dir if \"perturbed\" in fname else base_cif_dir, fname)\n",
    "\n",
    "        try:\n",
    "            atoms = read(cif_path)\n",
    "            atoms.info[\"REF_energy\"] = entry[\"energy_eV\"]\n",
    "            atoms.arrays[\"REF_forces\"] = np.array(entry[\"forces_per_atom_eV_per_A\"])\n",
    "            atoms.info[\"file\"] = fname\n",
    "            atoms_list.append(atoms)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed on {fname}: {e}\")\n",
    "\n",
    "    write(outfile, atoms_list, format=\"extxyz\", write_info=True)\n",
    "    print(f\"✅ Wrote {len(atoms_list)} to: {outfile}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4dfaba-821c-41d4-8c13-c496f855a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extxyz process started\n",
      "✅ Wrote 6337 to: /home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz\n",
      "extxyz process started\n",
      "✅ Wrote 705 to: /home/phanim/harshitrawat/summer/final_work/T2_chgnet_labeled.extxyz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Write EXTXYZs ===\n",
    "make_extxyz(train_entries, os.path.join(out_folder, \"T1_chgnet_labeled.extxyz\"))\n",
    "make_extxyz(val_entries, os.path.join(out_folder, \"T2_chgnet_labeled.extxyz\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65edb595-554c-45ba-a62d-e30394fd9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save splits as Excel ===\n",
    "pd.DataFrame(train_entries).to_excel(os.path.join(out_folder, \"T1_split.xlsx\"), index=False)\n",
    "pd.DataFrame(val_entries).to_excel(os.path.join(out_folder, \"T2_split.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f598aab-6d5a-46ad-b36e-974c0c516847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0723 18:58:00.289000 794575 site-packages/torch/distributed/run.py:766] \n",
      "W0723 18:58:00.289000 794575 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "W0723 18:58:00.289000 794575 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0723 18:58:00.289000 794575 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "/home/phanim/harshitrawat/miniconda3/bin/python: can't open file '/home/phanim/harshitrawat/summer/python': [Errno 2] No such file or directory\n",
      "/home/phanim/harshitrawat/miniconda3/bin/python: can't open file '/home/phanim/harshitrawat/summer/python': [Errno 2] No such file or directory\n",
      "E0723 18:58:00.631000 794575 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 2) local_rank: 0 (pid: 794742) of binary: /home/phanim/harshitrawat/miniconda3/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/bin/torchrun\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\"\u001b[0m, line \u001b[35m355\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return f(*args, **kwargs)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m892\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m883\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31melastic_launch(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mconfig=config,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mentrypoint=cmd,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "    \u001b[31m)\u001b[0m\u001b[1;31m(*cmd_args)\u001b[0m\n",
      "    \u001b[31m~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m139\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m270\u001b[0m, in \u001b[35mlaunch_agent\u001b[0m\n",
      "    raise ChildFailedError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "\u001b[1;35mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError\u001b[0m: \u001b[35m\n",
      "============================================================\n",
      "python FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2025-07-23_18:58:00\n",
      "  host      : cn10.cds.iisc.in\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 2 (pid: 794743)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-07-23_18:58:00\n",
      "  host      : cn10.cds.iisc.in\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 2 (pid: 794742)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!bash /home/phanim/harshitrawat/summer/run_mace.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5070e52c-753f-45ff-bc00-fb41b6ff30ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_dataset' from 'mace.data.utils' (/home/phanim/harshitrawat/summer/mace/mace/data/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_default_dtype\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_dataset' from 'mace.data.utils' (/home/phanim/harshitrawat/summer/mace/mace/data/utils.py)"
     ]
    }
   ],
   "source": [
    "from mace.tools import train\n",
    "from mace.modules import models\n",
    "from mace.data.utils import load_dataset\n",
    "from mace.tools.utils import get_default_dtype\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.set_default_dtype(torch.float32)  # force everything to float32\n",
    "\n",
    "# === Path setup ===\n",
    "train_file = \"/home/phanim/harshitrawat/summer/final_work/T1_float32_fixed.extxyz\"\n",
    "test_file  = \"/home/phanim/harshitrawat/summer/final_work/T2_float32_fixed.extxyz\"\n",
    "model_path = \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-64-L2_epoch-199.model\"\n",
    "\n",
    "# === Load dataset manually and cast\n",
    "train_data = load_dataset(train_file, \"float32\")\n",
    "test_data  = load_dataset(test_file,  \"float32\")\n",
    "\n",
    "# === Load model\n",
    "model = torch.load(model_path, map_location=\"cpu\")\n",
    "model = model.to(\"cuda\").float()  # ✅ make model float32\n",
    "\n",
    "# === Prepare training args\n",
    "args = dict(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    val_dataset=test_data,\n",
    "    forces_weight=100.0,\n",
    "    energy_weight=1.0,\n",
    "    loss=\"weighted\",\n",
    "    learning_rate=0.001,\n",
    "    ema_decay=0.99,\n",
    "    num_epochs=300,\n",
    "    batch_size=4,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# === Launch training\n",
    "train.train(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e230758-abea-495e-ac98-206357c8a718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "ERROR:root:Failed to initialize distributed environment: 'SLURM_NTASKS_PER_NODE'\n"
     ]
    }
   ],
   "source": [
    "export PYTHONPATH=/home/phanim/harshitrawat/summer/mace\n",
    "\n",
    "torchrun --nproc_per_node=2 \\\n",
    "         --nnodes=1 \\\n",
    "         --rdzv_backend=c10d \\\n",
    "         --rdzv_endpoint=localhost:0 \\\n",
    "         --master_port=29501 \\\n",
    "         -m mace.commands.train \\\n",
    "         --distributed \\\n",
    "         --launcher torchrun \\\n",
    "         --name mace_T1_finetune \\\n",
    "         --model MACE \\\n",
    "         --train_file /home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz \\\n",
    "         --test_file  /home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz \\\n",
    "         --foundation_model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
    "         --foundation_model_readout \\\n",
    "         --device cuda \\\n",
    "         --batch_size 2 \\\n",
    "         --valid_batch_size 1 \\\n",
    "         --default_dtype float64 \\\n",
    "         --valid_fraction 0.005 \\\n",
    "         --max_num_epochs 5 \\\n",
    "         --forces_weight 100.0 \\\n",
    "         --energy_weight 1.0 \\\n",
    "         --r_max 5.0 \\\n",
    "         --E0s \"{3:-201.7093,8:-431.6112,40:-1275.9529,57:-857.6754}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f134baf-0506-4764-bfad-40cbd5bbdab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0723 23:39:42.779000 1107440 site-packages/torch/distributed/run.py:766] \n",
      "W0723 23:39:42.779000 1107440 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "W0723 23:39:42.779000 1107440 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0723 23:39:42.779000 1107440 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "2025-07-23 23:39:50.327 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-23 23:39:50.330 INFO: ===========VERIFYING SETTINGS===========\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m383\u001b[0m, in \u001b[35m_lazy_init\u001b[0m\n",
      "[rank0]:     \u001b[31mqueued_call\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m252\u001b[0m, in \u001b[35m_check_capability\u001b[0m\n",
      "[rank0]:     capability = get_device_capability(d)\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m560\u001b[0m, in \u001b[35mget_device_capability\u001b[0m\n",
      "[rank0]:     prop = get_device_properties(device)\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m580\u001b[0m, in \u001b[35mget_device_properties\u001b[0m\n",
      "[rank0]:     return _get_device_properties(device)  # type: ignore[name-defined]\n",
      "[rank0]: \u001b[1;35mRuntimeError\u001b[0m: \u001b[35mdevice >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":52, please report a bug to PyTorch. device=1, num_gpus=1\u001b[0m\n",
      "\n",
      "[rank0]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "[rank0]:     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "[rank0]:     \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m108\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "[rank0]:     \u001b[31mtorch.cuda.set_device\u001b[0m\u001b[1;31m(local_rank)\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m529\u001b[0m, in \u001b[35mset_device\u001b[0m\n",
      "[rank0]:     \u001b[31mtorch._C._cuda_setDevice\u001b[0m\u001b[1;31m(device)\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m_lazy_init\u001b[0m\n",
      "[rank0]:     raise DeferredCudaCallError(msg) from e\n",
      "[rank0]: \u001b[1;35mtorch.cuda.DeferredCudaCallError\u001b[0m: \u001b[35mCUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":52, please report a bug to PyTorch. device=1, num_gpus=1\n",
      "\n",
      "[rank0]: CUDA call was originally invoked at:\n",
      "\n",
      "[rank0]:   File \"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\", line 19, in <module>\n",
      "[rank0]:     import torch.distributed\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "[rank0]:   File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "[rank0]:   File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/__init__.py\", line 2064, in <module>\n",
      "[rank0]:     _C._initExtension(_manager_path())\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "[rank0]:   File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "[rank0]:   File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\", line 317, in <module>\n",
      "[rank0]:     _lazy_call(_check_capability)\n",
      "[rank0]:   File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\", line 314, in _lazy_call\n",
      "[rank0]:     _queued_calls.append((callable, traceback.format_stack()))\n",
      "[rank0]: \u001b[0m\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m383\u001b[0m, in \u001b[35m_lazy_init\u001b[0m\n",
      "[rank0]:     \u001b[31mqueued_call\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m252\u001b[0m, in \u001b[35m_check_capability\u001b[0m\n",
      "[rank0]:     capability = get_device_capability(d)\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m560\u001b[0m, in \u001b[35mget_device_capability\u001b[0m\n",
      "[rank0]:     prop = get_device_properties(device)\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m580\u001b[0m, in \u001b[35mget_device_properties\u001b[0m\n",
      "[rank0]:     return _get_device_properties(device)  # type: ignore[name-defined]\n",
      "[rank0]: \u001b[1;35mRuntimeError\u001b[0m: \u001b[35mdevice >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":52, please report a bug to PyTorch. device=1, num_gpus=1\u001b[0m\n",
      "\n",
      "[rank0]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "[rank0]:     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "[rank0]:     \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m108\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "[rank0]:     \u001b[31mtorch.cuda.set_device\u001b[0m\u001b[1;31m(local_rank)\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m529\u001b[0m, in \u001b[35mset_device\u001b[0m\n",
      "[rank0]:     \u001b[31mtorch._C._cuda_setDevice\u001b[0m\u001b[1;31m(device)\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m_lazy_init\u001b[0m\n",
      "[rank0]:     raise DeferredCudaCallError(msg) from e\n",
      "[rank0]: \u001b[1;35mtorch.cuda.DeferredCudaCallError\u001b[0m: \u001b[35mCUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":52, please report a bug to PyTorch. device=1, num_gpus=1\n",
      "\n",
      "[rank0]: CUDA call was originally invoked at:\n",
      "\n",
      "[rank0]:   File \"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\", line 19, in <module>\n",
      "[rank0]:     import torch.distributed\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "[rank0]:   File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "[rank0]:   File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/__init__.py\", line 2064, in <module>\n",
      "[rank0]:     _C._initExtension(_manager_path())\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "[rank0]:   File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n",
      "[rank0]:   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "[rank0]:   File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\", line 317, in <module>\n",
      "[rank0]:     _lazy_call(_check_capability)\n",
      "[rank0]:   File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\", line 314, in _lazy_call\n",
      "[rank0]:     _queued_calls.append((callable, traceback.format_stack()))\n",
      "[rank0]: \u001b[0m\n",
      "[rank0]:[W723 23:39:51.114867692 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank0]:[W723 23:39:51.118366432 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0723 23:39:51.515000 1107440 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1107482 closing signal SIGTERM\n",
      "E0723 23:39:51.679000 1107440 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1107481) of binary: /home/phanim/harshitrawat/miniconda3/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/bin/torchrun\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\"\u001b[0m, line \u001b[35m355\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return f(*args, **kwargs)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m892\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m883\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31melastic_launch(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mconfig=config,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mentrypoint=cmd,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "    \u001b[31m)\u001b[0m\u001b[1;31m(*cmd_args)\u001b[0m\n",
      "    \u001b[31m~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m139\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m270\u001b[0m, in \u001b[35mlaunch_agent\u001b[0m\n",
      "    raise ChildFailedError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "\u001b[1;35mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError\u001b[0m: \u001b[35m\n",
      "============================================================\n",
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-07-23_23:39:51\n",
      "  host      : cn10.cds.iisc.in\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1107481)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=/home/phanim/harshitrawat/mace/mace \\\n",
    "torchrun \\\n",
    "  --standalone \\\n",
    "  --nnodes=1 \\\n",
    "  --nproc_per_node=2 \\\n",
    "  /home/phanim/harshitrawat/mace/mace/cli/run_train.py \\\n",
    "    --name='mace_T1_finetune' \\\n",
    "    --model='MACE' \\\n",
    "    --num_interactions=2 \\\n",
    "    --num_channels=128 \\\n",
    "    --max_L=2 \\\n",
    "    --correlation=3 \\\n",
    "    --E0s=\"{3:-201.7093,8:-431.6112,40:-1275.9529,57:-857.6754}\" \\\n",
    "    --r_max=5.0 \\\n",
    "    --train_file='/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.h5' \\\n",
    "    --valid_file='/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.h5' \\\n",
    "    --statistics_file='/home/phanim/harshitrawat/summer/final_work/statistics.json' \\\n",
    "    --num_workers=4 \\\n",
    "    --batch_size=2 \\\n",
    "    --valid_batch_size=1 \\\n",
    "    --max_num_epochs=5 \\\n",
    "    --loss='weighted' \\\n",
    "    --error_table='PerAtomRMSE' \\\n",
    "    --default_dtype='float64' \\\n",
    "    --device='cuda' \\\n",
    "    --distributed \\\n",
    "    --seed=42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c5ce0f3-8cd8-41b5-8de2-78741e948ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1751150545.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    torchrun --standalone \\\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1,3\n",
    "\n",
    "PYTHONPATH='/home/phanim/harshitrawat/mace/mace' \\\n",
    "torchrun --standalone \\\n",
    "         --nnodes=1 \\\n",
    "         --nproc_per_node=2 \\\n",
    "         /home/phanim/harshitrawat/mace/mace/cli/run_train.py \\\n",
    "  --name=\"mace_T1_finetune\" \\\n",
    "  --model=\"MACE\" \\\n",
    "  --train_file=\"/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz\" \\\n",
    "  --test_file=\"/home/phanim/harshitrawat/summer/final_work/T2_chgnet_labeled.extxyz\" \\\n",
    "  --foundation_model=\"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\" \\\n",
    "  --foundation_model_readout \\\n",
    "  --device=\"cuda\" \\\n",
    "  --batch_size=2 \\\n",
    "  --valid_batch_size=1 \\\n",
    "  --max_num_epochs=5 \\\n",
    "  --r_max=5.0 \\\n",
    "  --E0s=\"{3:-201.7093,8:-431.6112,40:-1275.9529,57:-857.6754}\" \\\n",
    "  --seed=42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d1667bb-f64f-4032-89d7-584bca7e4977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0724 00:21:12.310000 1272055 site-packages/torch/distributed/run.py:766] \n",
      "W0724 00:21:12.310000 1272055 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "W0724 00:21:12.310000 1272055 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0724 00:21:12.310000 1272055 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "2025-07-24 00:21:17.805 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-24 00:21:17.805 INFO: MACE version: 0.3.14\n",
      "2025-07-24 00:21:17.805 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-24 00:21:17.805 INFO: MACE version: 0.3.14\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m383\u001b[0m, in \u001b[35m_lazy_init\u001b[0m\n",
      "    \u001b[31mqueued_call\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m252\u001b[0m, in \u001b[35m_check_capability\u001b[0m\n",
      "    capability = get_device_capability(d)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m560\u001b[0m, in \u001b[35mget_device_capability\u001b[0m\n",
      "    prop = get_device_properties(device)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m580\u001b[0m, in \u001b[35mget_device_properties\u001b[0m\n",
      "    return _get_device_properties(device)  # type: ignore[name-defined]\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mdevice >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":52, please report a bug to PyTorch. device=1, num_gpus=1\u001b[0m\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m119\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    device = tools.init_device(args.device)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/torch_tools.py\"\u001b[0m, line \u001b[35m59\u001b[0m, in \u001b[35minit_device\u001b[0m\n",
      "    f\"CUDA version: {torch.version.cuda}, CUDA device: {\u001b[31mtorch.cuda.current_device\u001b[0m\u001b[1;31m()\u001b[0m}\"\n",
      "                                                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m1026\u001b[0m, in \u001b[35mcurrent_device\u001b[0m\n",
      "    \u001b[31m_lazy_init\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m_lazy_init\u001b[0m\n",
      "    raise DeferredCudaCallError(msg) from e\n",
      "\u001b[1;35mtorch.cuda.DeferredCudaCallError\u001b[0m: \u001b[35mCUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":52, please report a bug to PyTorch. device=1, num_gpus=1\n",
      "\n",
      "CUDA call was originally invoked at:\n",
      "\n",
      "  File \"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\", line 19, in <module>\n",
      "    import torch.distributed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/__init__.py\", line 2064, in <module>\n",
      "    _C._initExtension(_manager_path())\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\", line 317, in <module>\n",
      "    _lazy_call(_check_capability)\n",
      "  File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\", line 314, in _lazy_call\n",
      "    _queued_calls.append((callable, traceback.format_stack()))\n",
      "\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m383\u001b[0m, in \u001b[35m_lazy_init\u001b[0m\n",
      "    \u001b[31mqueued_call\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m252\u001b[0m, in \u001b[35m_check_capability\u001b[0m\n",
      "    capability = get_device_capability(d)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m560\u001b[0m, in \u001b[35mget_device_capability\u001b[0m\n",
      "    prop = get_device_properties(device)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m580\u001b[0m, in \u001b[35mget_device_properties\u001b[0m\n",
      "    return _get_device_properties(device)  # type: ignore[name-defined]\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mdevice >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":52, please report a bug to PyTorch. device=1, num_gpus=1\u001b[0m\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m119\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    device = tools.init_device(args.device)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/torch_tools.py\"\u001b[0m, line \u001b[35m59\u001b[0m, in \u001b[35minit_device\u001b[0m\n",
      "    f\"CUDA version: {torch.version.cuda}, CUDA device: {\u001b[31mtorch.cuda.current_device\u001b[0m\u001b[1;31m()\u001b[0m}\"\n",
      "                                                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m1026\u001b[0m, in \u001b[35mcurrent_device\u001b[0m\n",
      "    \u001b[31m_lazy_init\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m_lazy_init\u001b[0m\n",
      "    raise DeferredCudaCallError(msg) from e\n",
      "\u001b[1;35mtorch.cuda.DeferredCudaCallError\u001b[0m: \u001b[35mCUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":52, please report a bug to PyTorch. device=1, num_gpus=1\n",
      "\n",
      "CUDA call was originally invoked at:\n",
      "\n",
      "  File \"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\", line 19, in <module>\n",
      "    import torch.distributed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/__init__.py\", line 2064, in <module>\n",
      "    _C._initExtension(_manager_path())\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\", line 317, in <module>\n",
      "    _lazy_call(_check_capability)\n",
      "  File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\", line 314, in _lazy_call\n",
      "    _queued_calls.append((callable, traceback.format_stack()))\n",
      "\u001b[0m\n",
      "W0724 00:21:19.170000 1272055 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1272142 closing signal SIGTERM\n",
      "E0724 00:21:19.234000 1272055 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1272139) of binary: /home/phanim/harshitrawat/miniconda3/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/bin/torchrun\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\"\u001b[0m, line \u001b[35m355\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return f(*args, **kwargs)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m892\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m883\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31melastic_launch(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mconfig=config,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mentrypoint=cmd,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "    \u001b[31m)\u001b[0m\u001b[1;31m(*cmd_args)\u001b[0m\n",
      "    \u001b[31m~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m139\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m270\u001b[0m, in \u001b[35mlaunch_agent\u001b[0m\n",
      "    raise ChildFailedError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "\u001b[1;35mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError\u001b[0m: \u001b[35m\n",
      "============================================================\n",
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-07-24_00:21:19\n",
      "  host      : cn10.cds.iisc.in\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1272139)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Expose both MIG GPUs (1 and 3) to torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"PYTHONPATH\"] = \"/home/phanim/harshitrawat/mace/mace\"\n",
    "\n",
    "!torchrun --standalone \\\n",
    "         --nnodes=1 \\\n",
    "         --nproc_per_node=2 \\\n",
    "         /home/phanim/harshitrawat/mace/mace/cli/run_train.py \\\n",
    "  --name=\"mace_T1_finetune\" \\\n",
    "  --model=\"MACE\" \\\n",
    "  --train_file=\"/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz\" \\\n",
    "  --test_file=\"/home/phanim/harshitrawat/summer/final_work/T2_chgnet_labeled.extxyz\" \\\n",
    "  --foundation_model=\"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\" \\\n",
    "  --foundation_model_readout \\\n",
    "  --device=\"cuda\" \\\n",
    "  --batch_size=2 \\\n",
    "  --valid_batch_size=1 \\\n",
    "  --valid_fraction=0.005 \\\n",
    "  --max_num_epochs=5 \\\n",
    "  --r_max=5.0 \\\n",
    "  --E0s=\"{3:-201.7093,8:-431.6112,40:-1275.9529,57:-857.6754}\" \\\n",
    "  --seed=42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f5eb4ea-51ba-494d-b791-682c6ada4049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0724 00:08:07.618000 1142400 site-packages/torch/distributed/run.py:766] \n",
      "W0724 00:08:07.618000 1142400 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "W0724 00:08:07.618000 1142400 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0724 00:08:07.618000 1142400 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "2025-07-24 00:08:14.213 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-24 00:08:14.213 INFO: MACE version: 0.3.14\n",
      "2025-07-24 00:08:14.233 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-24 00:08:14.233 INFO: MACE version: 0.3.14\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m383\u001b[0m, in \u001b[35m_lazy_init\u001b[0m\n",
      "    \u001b[31mqueued_call\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m252\u001b[0m, in \u001b[35m_check_capability\u001b[0m\n",
      "    capability = get_device_capability(d)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m560\u001b[0m, in \u001b[35mget_device_capability\u001b[0m\n",
      "    prop = get_device_properties(device)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m580\u001b[0m, in \u001b[35mget_device_properties\u001b[0m\n",
      "    return _get_device_properties(device)  # type: ignore[name-defined]\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mdevice >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":52, please report a bug to PyTorch. device=1, num_gpus=1\u001b[0m\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m383\u001b[0m, in \u001b[35m_lazy_init\u001b[0m\n",
      "    \u001b[31mqueued_call\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m252\u001b[0m, in \u001b[35m_check_capability\u001b[0m\n",
      "    capability = get_device_capability(d)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m560\u001b[0m, in \u001b[35mget_device_capability\u001b[0m\n",
      "    prop = get_device_properties(device)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m580\u001b[0m, in \u001b[35mget_device_properties\u001b[0m\n",
      "    return _get_device_properties(device)  # type: ignore[name-defined]\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mdevice >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":52, please report a bug to PyTorch. device=1, num_gpus=1\u001b[0m\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m119\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    device = tools.init_device(args.device)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/torch_tools.py\"\u001b[0m, line \u001b[35m59\u001b[0m, in \u001b[35minit_device\u001b[0m\n",
      "    f\"CUDA version: {torch.version.cuda}, CUDA device: {\u001b[31mtorch.cuda.current_device\u001b[0m\u001b[1;31m()\u001b[0m}\"\n",
      "                                                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m1026\u001b[0m, in \u001b[35mcurrent_device\u001b[0m\n",
      "    \u001b[31m_lazy_init\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m_lazy_init\u001b[0m\n",
      "    raise DeferredCudaCallError(msg) from e\n",
      "\u001b[1;35mtorch.cuda.DeferredCudaCallError\u001b[0m: \u001b[35mCUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":52, please report a bug to PyTorch. device=1, num_gpus=1\n",
      "\n",
      "CUDA call was originally invoked at:\n",
      "\n",
      "  File \"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\", line 19, in <module>\n",
      "    import torch.distributed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/__init__.py\", line 2064, in <module>\n",
      "    _C._initExtension(_manager_path())\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\", line 317, in <module>\n",
      "    _lazy_call(_check_capability)\n",
      "  File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\", line 314, in _lazy_call\n",
      "    _queued_calls.append((callable, traceback.format_stack()))\n",
      "\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m119\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    device = tools.init_device(args.device)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/torch_tools.py\"\u001b[0m, line \u001b[35m59\u001b[0m, in \u001b[35minit_device\u001b[0m\n",
      "    f\"CUDA version: {torch.version.cuda}, CUDA device: {\u001b[31mtorch.cuda.current_device\u001b[0m\u001b[1;31m()\u001b[0m}\"\n",
      "                                                        \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m1026\u001b[0m, in \u001b[35mcurrent_device\u001b[0m\n",
      "    \u001b[31m_lazy_init\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m_lazy_init\u001b[0m\n",
      "    raise DeferredCudaCallError(msg) from e\n",
      "\u001b[1;35mtorch.cuda.DeferredCudaCallError\u001b[0m: \u001b[35mCUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"/pytorch/aten/src/ATen/cuda/CUDAContext.cpp\":52, please report a bug to PyTorch. device=1, num_gpus=1\n",
      "\n",
      "CUDA call was originally invoked at:\n",
      "\n",
      "  File \"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\", line 19, in <module>\n",
      "    import torch.distributed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/__init__.py\", line 2064, in <module>\n",
      "    _C._initExtension(_manager_path())\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\", line 317, in <module>\n",
      "    _lazy_call(_check_capability)\n",
      "  File \"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/cuda/__init__.py\", line 314, in _lazy_call\n",
      "    _queued_calls.append((callable, traceback.format_stack()))\n",
      "\u001b[0m\n",
      "W0724 00:08:15.513000 1142400 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1142512 closing signal SIGTERM\n",
      "E0724 00:08:15.778000 1142400 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1142511) of binary: /home/phanim/harshitrawat/miniconda3/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/bin/torchrun\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\"\u001b[0m, line \u001b[35m355\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return f(*args, **kwargs)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m892\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m883\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31melastic_launch(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mconfig=config,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mentrypoint=cmd,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "    \u001b[31m)\u001b[0m\u001b[1;31m(*cmd_args)\u001b[0m\n",
      "    \u001b[31m~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m139\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m270\u001b[0m, in \u001b[35mlaunch_agent\u001b[0m\n",
      "    raise ChildFailedError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "\u001b[1;35mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError\u001b[0m: \u001b[35m\n",
      "============================================================\n",
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-07-24_00:08:15\n",
      "  host      : cn10.cds.iisc.in\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1142511)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,3\"\n",
    "os.environ[\"PYTHONPATH\"] = \"/home/phanim/harshitrawat/mace/mace\"\n",
    "\n",
    "!torchrun --standalone \\\n",
    "         --nnodes=1 \\\n",
    "         --nproc_per_node=2 \\\n",
    "         /home/phanim/harshitrawat/mace/mace/cli/run_train.py \\\n",
    "  --name=\"mace_T1_finetune\" \\\n",
    "  --model=\"MACE\" \\\n",
    "  --train_file=\"/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz\" \\\n",
    "  --test_file=\"/home/phanim/harshitrawat/summer/final_work/T2_chgnet_labeled.extxyz\" \\\n",
    "  --foundation_model=\"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\" \\\n",
    "  --foundation_model_readout \\\n",
    "  --device=\"cuda\" \\\n",
    "  --batch_size=2 \\\n",
    "  --valid_batch_size=1 \\\n",
    "  --max_num_epochs=5 \\\n",
    "  --r_max=5.0 \\\n",
    "  --E0s=\"{3:-201.7093,8:-431.6112,40:-1275.9529,57:-857.6754}\" \\\n",
    "  --seed=42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4ecc6b4-8b53-48b9-9c95-318f700176e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "2025-07-24 00:22:22.775 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-24 00:22:22.775 INFO: MACE version: 0.3.14\n",
      "2025-07-24 00:22:23.300 INFO: CUDA version: 12.6, CUDA device: 0\n",
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py:146: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n",
      "2025-07-24 00:22:23.805 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-07-24 00:22:23.806 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-07-24 00:22:23.806 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-07-24 00:22:23.806 INFO: Using heads: ['Default']\n",
      "2025-07-24 00:22:23.806 INFO: Using the key specifications to parse data:\n",
      "2025-07-24 00:22:23.806 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-07-24 00:22:23.806 INFO: =============    Processing head Default     ===========\n",
      "2025-07-24 00:22:44.801 INFO: Training set 1/1 [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 00:22:44.820 INFO: Total Training set [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 00:22:44.820 INFO: No validation set provided, splitting training data instead.\n",
      "2025-07-24 00:22:44.824 INFO: Using random 0% of training set for validation with indices saved in: ./valid_indices_42.txt\n",
      "2025-07-24 00:22:44.846 INFO: Random Split Training set [energy: 6306, stress: 0, virials: 0, dipole components: 0, head: 6306, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6306, charges: 0]\n",
      "2025-07-24 00:22:44.846 INFO: Random Split Validation set [energy: 31, stress: 0, virials: 0, dipole components: 0, head: 31, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 31, charges: 0]\n",
      "2025-07-24 00:22:47.501 INFO: Test set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 00:22:47.503 INFO: Total Test set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 00:22:47.503 INFO: Total number of configurations: train=6306, valid=31, tests=[Default_Default: 705],\n",
      "2025-07-24 00:22:47.786 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-07-24 00:22:47.786 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-07-24 00:22:47.786 INFO: Atomic Energies used (z: eV) for head Default: {3: -201.7093, 8: -431.6112, 40: -1275.9529, 57: -857.6754}\n",
      "2025-07-24 00:22:47.786 INFO: Processing datasets for head 'Default'\n",
      "2025-07-24 00:24:10.593 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-07-24 00:24:11.069 INFO: Head 'Default' training dataset size: 6306\n",
      "2025-07-24 00:24:11.070 INFO: Computing average number of neighbors\n",
      "2025-07-24 00:24:19.387 INFO: Average number of neighbors: 68.47692474069387\n",
      "2025-07-24 00:24:19.388 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-07-24 00:24:19.388 INFO: ===========MODEL DETAILS===========\n",
      "2025-07-24 00:24:26.900 INFO: Loading FOUNDATION model\n",
      "2025-07-24 00:24:26.901 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-07-24 00:24:26.901 INFO: Model configuration extracted from foundation model\n",
      "2025-07-24 00:24:26.901 INFO: Using weighted loss function for fine-tuning\n",
      "2025-07-24 00:24:26.901 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-07-24 00:24:26.901 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-07-24 00:24:26.901 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-07-24 00:24:26.901 INFO: Distance transform for radial basis functions: None\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "2025-07-24 00:24:28.265 INFO: Total number of parameters: 894362\n",
      "2025-07-24 00:24:28.265 INFO: \n",
      "2025-07-24 00:24:28.265 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-07-24 00:24:28.265 INFO: Using ADAM as parameter optimizer\n",
      "2025-07-24 00:24:28.265 INFO: Batch size: 2\n",
      "2025-07-24 00:24:28.265 INFO: Number of gradient updates: 15765\n",
      "2025-07-24 00:24:28.265 INFO: Learning rate: 0.01, weight decay: 5e-07\n",
      "2025-07-24 00:24:28.265 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-07-24 00:24:28.265 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-07-24 00:24:28.265 INFO: \n",
      "2025-07-24 00:24:28.265 INFO: ===========TRAINING===========\n",
      "2025-07-24 00:24:28.265 INFO: Started training, reporting errors on validation set\n",
      "2025-07-24 00:24:28.265 INFO: Loss metrics on validation set\n",
      "2025-07-24 00:24:36.880 INFO: Initial: head: Default, loss=83458.08978851, RMSE_E_per_atom=288641.89 meV, RMSE_F= 1267.25 meV / A\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m767\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mtools.train\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mmodel=model,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "    ...<23 lines>...\n",
      "        \u001b[1;31mrank=rank,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m222\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "    \u001b[31mtrain_one_epoch\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mmodel=model,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "    ...<11 lines>...\n",
      "        \u001b[1;31mrank=rank,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m370\u001b[0m, in \u001b[35mtrain_one_epoch\u001b[0m\n",
      "    _, opt_metrics = \u001b[31mtake_step\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                     \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mmodel=model_to_train,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<6 lines>...\n",
      "        \u001b[1;31mdevice=device,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m416\u001b[0m, in \u001b[35mtake_step\u001b[0m\n",
      "    loss = closure()\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m410\u001b[0m, in \u001b[35mclosure\u001b[0m\n",
      "    \u001b[31mloss.backward\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/_tensor.py\"\u001b[0m, line \u001b[35m648\u001b[0m, in \u001b[35mbackward\u001b[0m\n",
      "    \u001b[31mtorch.autograd.backward\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mself, gradient, retain_graph, create_graph, inputs=inputs\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35mbackward\u001b[0m\n",
      "    \u001b[31m_engine_run_backward\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mtensors,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^\u001b[0m\n",
      "    ...<5 lines>...\n",
      "        \u001b[1;31maccumulate_grad=True,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py\"\u001b[0m, line \u001b[35m824\u001b[0m, in \u001b[35m_engine_run_backward\u001b[0m\n",
      "    return \u001b[31mVariable._execution_engine.run_backward\u001b[0m\u001b[1;31m(  # Calls into the C++ engine to run the backward pass\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        \u001b[1;31mt_outputs, *args, **kwargs\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m  # Calls into the C++ engine to run the backward pass\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "\u001b[1;35mtorch.OutOfMemoryError\u001b[0m: \u001b[35mCUDA out of memory. Tried to allocate 4.66 GiB. GPU 0 has a total capacity of 69.50 GiB of which 3.35 GiB is free. Including non-PyTorch memory, this process has 66.09 GiB memory in use. Of the allocated memory 65.19 GiB is allocated by PyTorch, and 487.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\u001b[0m\n",
      "E0724 00:24:47.698000 1273280 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1273332) of binary: /home/phanim/harshitrawat/miniconda3/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/bin/torchrun\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\"\u001b[0m, line \u001b[35m355\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return f(*args, **kwargs)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m892\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m883\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31melastic_launch(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mconfig=config,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mentrypoint=cmd,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "    \u001b[31m)\u001b[0m\u001b[1;31m(*cmd_args)\u001b[0m\n",
      "    \u001b[31m~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m139\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m270\u001b[0m, in \u001b[35mlaunch_agent\u001b[0m\n",
      "    raise ChildFailedError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "\u001b[1;35mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError\u001b[0m: \u001b[35m\n",
      "============================================================\n",
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-07-24_00:24:47\n",
      "  host      : cn10.cds.iisc.in\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1273332)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Only expose ONE MIG device to torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # or \"3\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# PYTHONPATH optional if already in sys.path\n",
    "os.environ[\"PYTHONPATH\"] = \"/home/phanim/harshitrawat/mace/mace\"\n",
    "\n",
    "!torchrun --standalone \\\n",
    "         --nnodes=1 \\\n",
    "         --nproc_per_node=1 \\\n",
    "         /home/phanim/harshitrawat/mace/mace/cli/run_train.py \\\n",
    "  --name=\"mace_T1_finetune\" \\\n",
    "  --model=\"MACE\" \\\n",
    "  --train_file=\"/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz\" \\\n",
    "  --test_file=\"/home/phanim/harshitrawat/summer/final_work/T2_chgnet_labeled.extxyz\" \\\n",
    "  --foundation_model=\"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\" \\\n",
    "  --foundation_model_readout \\\n",
    "  --device=\"cuda\" \\\n",
    "  --batch_size=2 \\\n",
    "  --valid_batch_size=1 \\\n",
    "  --valid_fraction 0.005 \\\n",
    "  --max_num_epochs=5 \\\n",
    "  --r_max=5.0 \\\n",
    "  --E0s=\"{3:-201.7093,8:-431.6112,40:-1275.9529,57:-857.6754}\" \\\n",
    "  --seed=42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12380ffe-58cf-49cf-a65e-3ab9edbc9e2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0724 00:40:27.565000 1353925 site-packages/torch/distributed/run.py:766] \n",
      "W0724 00:40:27.565000 1353925 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "W0724 00:40:27.565000 1353925 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0724 00:40:27.565000 1353925 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "2025-07-24 00:40:33.499 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-24 00:40:33.518 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-24 00:40:33.637 INFO: Process group initialized: True\n",
      "2025-07-24 00:40:33.637 INFO: Processes: 1\n",
      "2025-07-24 00:40:33.637 INFO: MACE version: 0.3.14\n",
      "2025-07-24 00:40:33.637 INFO: CUDA version: 12.6, CUDA device: 0\n",
      "2025-07-24 00:40:33.639 INFO: Process group initialized: True\n",
      "2025-07-24 00:40:33.639 INFO: Processes: 1\n",
      "2025-07-24 00:40:33.639 INFO: MACE version: 0.3.14\n",
      "2025-07-24 00:40:33.639 INFO: CUDA version: 12.6, CUDA device: 0\n",
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py:146: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n",
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py:146: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n",
      "2025-07-24 00:40:34.650 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-07-24 00:40:34.651 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-07-24 00:40:34.652 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-07-24 00:40:34.652 INFO: Using heads: ['Default']\n",
      "2025-07-24 00:40:34.651 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-07-24 00:40:34.652 INFO: Using the key specifications to parse data:\n",
      "2025-07-24 00:40:34.652 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-07-24 00:40:34.652 INFO: =============    Processing head Default     ===========\n",
      "2025-07-24 00:40:34.653 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-07-24 00:40:34.653 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-07-24 00:40:34.653 INFO: Using heads: ['Default']\n",
      "2025-07-24 00:40:34.653 INFO: Using the key specifications to parse data:\n",
      "2025-07-24 00:40:34.653 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-07-24 00:40:34.653 INFO: =============    Processing head Default     ===========\n",
      "2025-07-24 00:40:55.709 INFO: Training set 1/1 [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 00:40:55.722 INFO: Training set 1/1 [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 00:40:55.729 INFO: Total Training set [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 00:40:55.729 INFO: No validation set provided, splitting training data instead.\n",
      "2025-07-24 00:40:55.733 INFO: Using random 0% of training set for validation with indices saved in: ./valid_indices_42.txt\n",
      "2025-07-24 00:40:55.741 INFO: Total Training set [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 00:40:55.741 INFO: No validation set provided, splitting training data instead.\n",
      "2025-07-24 00:40:55.745 INFO: Using random 0% of training set for validation with indices saved in: ./valid_indices_42.txt\n",
      "2025-07-24 00:40:55.756 INFO: Random Split Training set [energy: 6306, stress: 0, virials: 0, dipole components: 0, head: 6306, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6306, charges: 0]\n",
      "2025-07-24 00:40:55.756 INFO: Random Split Validation set [energy: 31, stress: 0, virials: 0, dipole components: 0, head: 31, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 31, charges: 0]\n",
      "2025-07-24 00:40:55.768 INFO: Random Split Training set [energy: 6306, stress: 0, virials: 0, dipole components: 0, head: 6306, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6306, charges: 0]\n",
      "2025-07-24 00:40:55.768 INFO: Random Split Validation set [energy: 31, stress: 0, virials: 0, dipole components: 0, head: 31, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 31, charges: 0]\n",
      "2025-07-24 00:40:58.339 INFO: Test set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 00:40:58.341 INFO: Total Test set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 00:40:58.341 INFO: Total number of configurations: train=6306, valid=31, tests=[Default_Default: 705],\n",
      "2025-07-24 00:40:58.350 INFO: Test set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 00:40:58.352 INFO: Total Test set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 00:40:58.352 INFO: Total number of configurations: train=6306, valid=31, tests=[Default_Default: 705],\n",
      "2025-07-24 00:40:58.609 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-07-24 00:40:58.609 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-07-24 00:40:58.609 INFO: Atomic Energies used (z: eV) for head Default: {3: -201.7093, 8: -431.6112, 40: -1275.9529, 57: -857.6754}\n",
      "2025-07-24 00:40:58.609 INFO: Processing datasets for head 'Default'\n",
      "2025-07-24 00:40:58.631 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-07-24 00:40:58.631 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-07-24 00:40:58.631 INFO: Atomic Energies used (z: eV) for head Default: {3: -201.7093, 8: -431.6112, 40: -1275.9529, 57: -857.6754}\n",
      "2025-07-24 00:40:58.631 INFO: Processing datasets for head 'Default'\n",
      "2025-07-24 00:42:19.148 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-07-24 00:42:19.262 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-07-24 00:42:19.615 INFO: Head 'Default' training dataset size: 6306\n",
      "2025-07-24 00:42:19.616 INFO: Computing average number of neighbors\n",
      "2025-07-24 00:42:19.727 INFO: Head 'Default' training dataset size: 6306\n",
      "2025-07-24 00:42:19.728 INFO: Computing average number of neighbors\n",
      "2025-07-24 00:42:43.436 INFO: Average number of neighbors: 68.47692474069387\n",
      "2025-07-24 00:42:43.436 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-07-24 00:42:43.436 INFO: ===========MODEL DETAILS===========\n",
      "2025-07-24 00:42:43.675 INFO: Average number of neighbors: 68.47692474069387\n",
      "2025-07-24 00:42:43.675 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-07-24 00:42:43.675 INFO: ===========MODEL DETAILS===========\n",
      "2025-07-24 00:42:55.107 INFO: Loading FOUNDATION model\n",
      "2025-07-24 00:42:55.109 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-07-24 00:42:55.109 INFO: Model configuration extracted from foundation model\n",
      "2025-07-24 00:42:55.109 INFO: Using weighted loss function for fine-tuning\n",
      "2025-07-24 00:42:55.109 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-07-24 00:42:55.109 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-07-24 00:42:55.109 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-07-24 00:42:55.109 INFO: Distance transform for radial basis functions: None\n",
      "2025-07-24 00:42:55.112 INFO: Loading FOUNDATION model\n",
      "2025-07-24 00:42:55.114 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-07-24 00:42:55.114 INFO: Model configuration extracted from foundation model\n",
      "2025-07-24 00:42:55.114 INFO: Using weighted loss function for fine-tuning\n",
      "2025-07-24 00:42:55.114 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-07-24 00:42:55.114 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-07-24 00:42:55.114 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-07-24 00:42:55.114 INFO: Distance transform for radial basis functions: None\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "Using reduced CG: False\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "2025-07-24 00:42:56.361 INFO: Total number of parameters: 894362\n",
      "2025-07-24 00:42:56.362 INFO: \n",
      "2025-07-24 00:42:56.362 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-07-24 00:42:56.362 INFO: Using ADAM as parameter optimizer\n",
      "2025-07-24 00:42:56.362 INFO: Batch size: 1\n",
      "2025-07-24 00:42:56.362 INFO: Total number of parameters: 894362\n",
      "2025-07-24 00:42:56.362 INFO: Number of gradient updates: 31530\n",
      "2025-07-24 00:42:56.362 INFO: \n",
      "2025-07-24 00:42:56.362 INFO: Learning rate: 0.01, weight decay: 5e-07\n",
      "2025-07-24 00:42:56.362 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-07-24 00:42:56.362 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-07-24 00:42:56.362 INFO: Using ADAM as parameter optimizer\n",
      "2025-07-24 00:42:56.362 INFO: Batch size: 1\n",
      "2025-07-24 00:42:56.362 INFO: Number of gradient updates: 31530\n",
      "2025-07-24 00:42:56.362 INFO: Learning rate: 0.01, weight decay: 5e-07\n",
      "2025-07-24 00:42:56.362 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-07-24 00:42:56.372 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-07-24 00:42:56.372 INFO: \n",
      "2025-07-24 00:42:56.372 INFO: ===========TRAINING===========\n",
      "2025-07-24 00:42:56.372 INFO: Started training, reporting errors on validation set\n",
      "2025-07-24 00:42:56.372 INFO: Loss metrics on validation set\n",
      "2025-07-24 00:42:56.372 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-07-24 00:42:56.372 INFO: \n",
      "2025-07-24 00:42:56.372 INFO: ===========TRAINING===========\n",
      "2025-07-24 00:42:56.372 INFO: Started training, reporting errors on validation set\n",
      "2025-07-24 00:42:56.372 INFO: Loss metrics on validation set\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "2025-07-24 00:43:12.023 INFO: Initial: head: Default, loss=83458.08978851, RMSE_E_per_atom=288641.89 meV, RMSE_F= 1267.25 meV / A\n",
      "2025-07-24 00:43:12.040 INFO: Initial: head: Default, loss=83458.08978851, RMSE_E_per_atom=288641.89 meV, RMSE_F= 1267.25 meV / A\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "[rank0]:     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "[rank0]:     \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m767\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "[rank0]:     \u001b[31mtools.train\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<23 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m222\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "[rank0]:     \u001b[31mtrain_one_epoch\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<11 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m370\u001b[0m, in \u001b[35mtrain_one_epoch\u001b[0m\n",
      "[rank0]:     _, opt_metrics = \u001b[31mtake_step\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:                      \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model_to_train,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<6 lines>...\n",
      "[rank0]:         \u001b[1;31mdevice=device,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m416\u001b[0m, in \u001b[35mtake_step\u001b[0m\n",
      "[rank0]:     loss = closure()\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m410\u001b[0m, in \u001b[35mclosure\u001b[0m\n",
      "[rank0]:     \u001b[31mloss.backward\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/_tensor.py\"\u001b[0m, line \u001b[35m648\u001b[0m, in \u001b[35mbackward\u001b[0m\n",
      "[rank0]:     \u001b[31mtorch.autograd.backward\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mself, gradient, retain_graph, create_graph, inputs=inputs\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35mbackward\u001b[0m\n",
      "[rank0]:     \u001b[31m_engine_run_backward\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mtensors,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<5 lines>...\n",
      "[rank0]:         \u001b[1;31maccumulate_grad=True,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py\"\u001b[0m, line \u001b[35m824\u001b[0m, in \u001b[35m_engine_run_backward\u001b[0m\n",
      "[rank0]:     return \u001b[31mVariable._execution_engine.run_backward\u001b[0m\u001b[1;31m(  # Calls into the C++ engine to run the backward pass\u001b[0m\n",
      "[rank0]:            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mt_outputs, *args, **kwargs\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m  # Calls into the C++ engine to run the backward pass\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]: \u001b[1;35mtorch.OutOfMemoryError\u001b[0m: \u001b[35mCUDA out of memory. Tried to allocate 434.00 MiB. GPU 0 has a total capacity of 69.50 GiB of which 312.31 MiB is free. Including non-PyTorch memory, this process has 33.68 GiB memory in use. Process 1353933 has 35.45 GiB memory in use. Of the allocated memory 32.36 GiB is allocated by PyTorch, and 350.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\u001b[0m\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "[rank0]:     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "[rank0]:     \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m767\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "[rank0]:     \u001b[31mtools.train\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<23 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m222\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "[rank0]:     \u001b[31mtrain_one_epoch\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<11 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m370\u001b[0m, in \u001b[35mtrain_one_epoch\u001b[0m\n",
      "[rank0]:     _, opt_metrics = \u001b[31mtake_step\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:                      \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model_to_train,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<6 lines>...\n",
      "[rank0]:         \u001b[1;31mdevice=device,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m416\u001b[0m, in \u001b[35mtake_step\u001b[0m\n",
      "[rank0]:     loss = closure()\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m410\u001b[0m, in \u001b[35mclosure\u001b[0m\n",
      "[rank0]:     \u001b[31mloss.backward\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/_tensor.py\"\u001b[0m, line \u001b[35m648\u001b[0m, in \u001b[35mbackward\u001b[0m\n",
      "[rank0]:     \u001b[31mtorch.autograd.backward\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mself, gradient, retain_graph, create_graph, inputs=inputs\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35mbackward\u001b[0m\n",
      "[rank0]:     \u001b[31m_engine_run_backward\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mtensors,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<5 lines>...\n",
      "[rank0]:         \u001b[1;31maccumulate_grad=True,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py\"\u001b[0m, line \u001b[35m824\u001b[0m, in \u001b[35m_engine_run_backward\u001b[0m\n",
      "[rank0]:     return \u001b[31mVariable._execution_engine.run_backward\u001b[0m\u001b[1;31m(  # Calls into the C++ engine to run the backward pass\u001b[0m\n",
      "[rank0]:            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mt_outputs, *args, **kwargs\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m  # Calls into the C++ engine to run the backward pass\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]: \u001b[1;35mtorch.OutOfMemoryError\u001b[0m: \u001b[35mCUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacity of 69.50 GiB of which 574.31 MiB is free. Process 1353934 has 33.68 GiB memory in use. Including non-PyTorch memory, this process has 35.20 GiB memory in use. Of the allocated memory 33.84 GiB is allocated by PyTorch, and 395.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\u001b[0m\n",
      "[rank0]:[W724 00:43:24.446356667 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank0]:[W724 00:43:24.473217973 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0724 00:43:26.742000 1353925 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1353933 closing signal SIGTERM\n",
      "E0724 00:43:27.066000 1353925 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 1 (pid: 1353934) of binary: /home/phanim/harshitrawat/miniconda3/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/bin/torchrun\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\"\u001b[0m, line \u001b[35m355\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return f(*args, **kwargs)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m892\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m883\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31melastic_launch(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mconfig=config,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mentrypoint=cmd,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "    \u001b[31m)\u001b[0m\u001b[1;31m(*cmd_args)\u001b[0m\n",
      "    \u001b[31m~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m139\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m270\u001b[0m, in \u001b[35mlaunch_agent\u001b[0m\n",
      "    raise ChildFailedError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "\u001b[1;35mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError\u001b[0m: \u001b[35m\n",
      "============================================================\n",
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-07-24_00:43:26\n",
      "  host      : cn10.cds.iisc.in\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 1353934)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Only expose ONE MIG device to torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # or \"3\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# PYTHONPATH optional if already in sys.path\n",
    "os.environ[\"PYTHONPATH\"] = \"/home/phanim/harshitrawat/mace/mace\"\n",
    "!torchrun --standalone \\\n",
    "  --nnodes=1 \\\n",
    "  --nproc_per_node=2 \\\n",
    "  /home/phanim/harshitrawat/mace/mace/cli/run_train.py \\\n",
    "  --name=\"mace_T1_finetune\" \\\n",
    "  --model=\"MACE\" \\\n",
    "  --train_file=\"/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz\" \\\n",
    "  --test_file=\"/home/phanim/harshitrawat/summer/final_work/T2_chgnet_labeled.extxyz\" \\\n",
    "  --foundation_model=\"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\" \\\n",
    "  --foundation_model_readout \\\n",
    "  --batch_size=1 \\\n",
    "  --valid_batch_size=1 \\\n",
    "  --valid_fraction=0.005 \\\n",
    "  --device=\"cuda\" \\\n",
    "  --max_num_epochs=5 \\\n",
    "  --r_max=5.0 \\\n",
    "  --E0s=\"{3:-201.7093,8:-431.6112,40:-1275.9529,57:-857.6754}\" \\\n",
    "  --seed=42 \\\n",
    "  --distributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b33a28-a3d5-402b-96b8-835d90f663d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0724 00:48:35.478000 1362799 site-packages/torch/distributed/run.py:766] \n",
      "W0724 00:48:35.478000 1362799 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "W0724 00:48:35.478000 1362799 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0724 00:48:35.478000 1362799 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "2025-07-24 00:48:39.760 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-24 00:48:39.837 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-24 00:48:39.923 INFO: Process group initialized: True\n",
      "2025-07-24 00:48:39.924 INFO: Processes: 1\n",
      "2025-07-24 00:48:39.924 INFO: MACE version: 0.3.14\n",
      "2025-07-24 00:48:39.924 INFO: CUDA version: 12.6, CUDA device: 0\n",
      "2025-07-24 00:48:39.954 INFO: Process group initialized: True\n",
      "2025-07-24 00:48:39.954 INFO: Processes: 1\n",
      "2025-07-24 00:48:39.954 INFO: MACE version: 0.3.14\n",
      "2025-07-24 00:48:39.954 INFO: CUDA version: 12.6, CUDA device: 0\n",
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py:146: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n",
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py:146: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n",
      "2025-07-24 00:48:40.872 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-07-24 00:48:40.873 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-07-24 00:48:40.873 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-07-24 00:48:40.873 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-07-24 00:48:40.873 INFO: Using heads: ['Default']\n",
      "2025-07-24 00:48:40.873 INFO: Using the key specifications to parse data:\n",
      "2025-07-24 00:48:40.874 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-07-24 00:48:40.874 INFO: =============    Processing head Default     ===========\n",
      "2025-07-24 00:48:40.874 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-07-24 00:48:40.874 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-07-24 00:48:40.874 INFO: Using heads: ['Default']\n",
      "2025-07-24 00:48:40.874 INFO: Using the key specifications to parse data:\n",
      "2025-07-24 00:48:40.874 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-07-24 00:48:40.874 INFO: =============    Processing head Default     ===========\n",
      "2025-07-24 00:49:01.895 INFO: Training set 1/1 [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 00:49:01.904 INFO: Training set 1/1 [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 00:49:01.915 INFO: Total Training set [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 00:49:01.915 INFO: No validation set provided, splitting training data instead.\n",
      "2025-07-24 00:49:01.918 INFO: Using random 0% of training set for validation with indices saved in: ./valid_indices_42.txt\n",
      "2025-07-24 00:49:01.926 INFO: Total Training set [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 00:49:01.926 INFO: No validation set provided, splitting training data instead.\n",
      "2025-07-24 00:49:01.929 INFO: Using random 0% of training set for validation with indices saved in: ./valid_indices_42.txt\n",
      "2025-07-24 00:49:01.941 INFO: Random Split Training set [energy: 6306, stress: 0, virials: 0, dipole components: 0, head: 6306, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6306, charges: 0]\n",
      "2025-07-24 00:49:01.941 INFO: Random Split Validation set [energy: 31, stress: 0, virials: 0, dipole components: 0, head: 31, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 31, charges: 0]\n",
      "2025-07-24 00:49:01.952 INFO: Random Split Training set [energy: 6306, stress: 0, virials: 0, dipole components: 0, head: 6306, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6306, charges: 0]\n",
      "2025-07-24 00:49:01.952 INFO: Random Split Validation set [energy: 31, stress: 0, virials: 0, dipole components: 0, head: 31, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 31, charges: 0]\n",
      "2025-07-24 00:49:04.541 INFO: Test set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 00:49:04.543 INFO: Total Test set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 00:49:04.544 INFO: Total number of configurations: train=6306, valid=31, tests=[Default_Default: 705],\n",
      "2025-07-24 00:49:04.578 INFO: Test set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 00:49:04.580 INFO: Total Test set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 00:49:04.581 INFO: Total number of configurations: train=6306, valid=31, tests=[Default_Default: 705],\n",
      "2025-07-24 00:49:04.821 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-07-24 00:49:04.821 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-07-24 00:49:04.821 INFO: Atomic Energies used (z: eV) for head Default: {3: -201.7093, 8: -431.6112, 40: -1275.9529, 57: -857.6754}\n",
      "2025-07-24 00:49:04.821 INFO: Processing datasets for head 'Default'\n",
      "2025-07-24 00:49:04.858 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-07-24 00:49:04.858 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-07-24 00:49:04.858 INFO: Atomic Energies used (z: eV) for head Default: {3: -201.7093, 8: -431.6112, 40: -1275.9529, 57: -857.6754}\n",
      "2025-07-24 00:49:04.858 INFO: Processing datasets for head 'Default'\n",
      "2025-07-24 00:50:25.418 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-07-24 00:50:25.605 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-07-24 00:50:25.890 INFO: Head 'Default' training dataset size: 6306\n",
      "2025-07-24 00:50:25.890 INFO: Computing average number of neighbors\n",
      "2025-07-24 00:50:26.074 INFO: Head 'Default' training dataset size: 6306\n",
      "2025-07-24 00:50:26.075 INFO: Computing average number of neighbors\n",
      "2025-07-24 00:50:49.542 INFO: Average number of neighbors: 68.47692474069387\n",
      "2025-07-24 00:50:49.543 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-07-24 00:50:49.543 INFO: ===========MODEL DETAILS===========\n",
      "2025-07-24 00:50:49.706 INFO: Average number of neighbors: 68.47692474069387\n",
      "2025-07-24 00:50:49.706 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-07-24 00:50:49.706 INFO: ===========MODEL DETAILS===========\n",
      "2025-07-24 00:51:00.733 INFO: Loading FOUNDATION model\n",
      "2025-07-24 00:51:00.735 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-07-24 00:51:00.735 INFO: Model configuration extracted from foundation model\n",
      "2025-07-24 00:51:00.735 INFO: Using weighted loss function for fine-tuning\n",
      "2025-07-24 00:51:00.735 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-07-24 00:51:00.735 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-07-24 00:51:00.735 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-07-24 00:51:00.735 INFO: Distance transform for radial basis functions: None\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "2025-07-24 00:51:01.231 INFO: Loading FOUNDATION model\n",
      "2025-07-24 00:51:01.233 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-07-24 00:51:01.233 INFO: Model configuration extracted from foundation model\n",
      "2025-07-24 00:51:01.233 INFO: Using weighted loss function for fine-tuning\n",
      "2025-07-24 00:51:01.233 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-07-24 00:51:01.233 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-07-24 00:51:01.233 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-07-24 00:51:01.233 INFO: Distance transform for radial basis functions: None\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "2025-07-24 00:51:01.978 INFO: Total number of parameters: 894362\n",
      "2025-07-24 00:51:01.978 INFO: \n",
      "2025-07-24 00:51:01.978 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-07-24 00:51:01.978 INFO: Using ADAM as parameter optimizer\n",
      "2025-07-24 00:51:01.979 INFO: Batch size: 1\n",
      "2025-07-24 00:51:01.979 INFO: Number of gradient updates: 31530\n",
      "2025-07-24 00:51:01.979 INFO: Learning rate: 0.01, weight decay: 5e-07\n",
      "2025-07-24 00:51:01.979 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "2025-07-24 00:51:01.988 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-07-24 00:51:01.988 INFO: \n",
      "2025-07-24 00:51:01.988 INFO: ===========TRAINING===========\n",
      "2025-07-24 00:51:01.988 INFO: Started training, reporting errors on validation set\n",
      "2025-07-24 00:51:01.988 INFO: Loss metrics on validation set\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "2025-07-24 00:51:02.378 INFO: Total number of parameters: 894362\n",
      "2025-07-24 00:51:02.378 INFO: \n",
      "2025-07-24 00:51:02.378 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-07-24 00:51:02.378 INFO: Using ADAM as parameter optimizer\n",
      "2025-07-24 00:51:02.378 INFO: Batch size: 1\n",
      "2025-07-24 00:51:02.378 INFO: Number of gradient updates: 31530\n",
      "2025-07-24 00:51:02.378 INFO: Learning rate: 0.01, weight decay: 5e-07\n",
      "2025-07-24 00:51:02.378 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-07-24 00:51:02.389 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-07-24 00:51:02.389 INFO: \n",
      "2025-07-24 00:51:02.389 INFO: ===========TRAINING===========\n",
      "2025-07-24 00:51:02.389 INFO: Started training, reporting errors on validation set\n",
      "2025-07-24 00:51:02.389 INFO: Loss metrics on validation set\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "2025-07-24 00:51:17.250 INFO: Initial: head: Default, loss=83458.08978851, RMSE_E_per_atom=288641.89 meV, RMSE_F= 1267.25 meV / A\n",
      "2025-07-24 00:51:17.251 INFO: Initial: head: Default, loss=83458.08978851, RMSE_E_per_atom=288641.89 meV, RMSE_F= 1267.25 meV / A\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "[rank0]:     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "[rank0]:     \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m767\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "[rank0]:     \u001b[31mtools.train\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<23 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m222\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "[rank0]:     \u001b[31mtrain_one_epoch\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<11 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m370\u001b[0m, in \u001b[35mtrain_one_epoch\u001b[0m\n",
      "[rank0]:     _, opt_metrics = \u001b[31mtake_step\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:                      \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model_to_train,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<6 lines>...\n",
      "[rank0]:         \u001b[1;31mdevice=device,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m416\u001b[0m, in \u001b[35mtake_step\u001b[0m\n",
      "[rank0]:     loss = closure()\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m402\u001b[0m, in \u001b[35mclosure\u001b[0m\n",
      "[rank0]:     output = model(\n",
      "[rank0]:         batch_dict,\n",
      "[rank0]:     ...<3 lines>...\n",
      "[rank0]:         compute_stress=output_args[\"stress\"],\n",
      "[rank0]:     )\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m1751\u001b[0m, in \u001b[35m_wrapped_call_impl\u001b[0m\n",
      "[rank0]:     return \u001b[31mself._call_impl\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "[rank0]:            \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m1762\u001b[0m, in \u001b[35m_call_impl\u001b[0m\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/parallel/distributed.py\"\u001b[0m, line \u001b[35m1637\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "[rank0]:     else \u001b[31mself._run_ddp_forward\u001b[0m\u001b[1;31m(*inputs, **kwargs)\u001b[0m\n",
      "[rank0]:          \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/parallel/distributed.py\"\u001b[0m, line \u001b[35m1464\u001b[0m, in \u001b[35m_run_ddp_forward\u001b[0m\n",
      "[rank0]:     return \u001b[31mself.module\u001b[0m\u001b[1;31m(*inputs, **kwargs)\u001b[0m  # type: ignore[index]\n",
      "[rank0]:            \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m1751\u001b[0m, in \u001b[35m_wrapped_call_impl\u001b[0m\n",
      "[rank0]:     return \u001b[31mself._call_impl\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "[rank0]:            \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m1762\u001b[0m, in \u001b[35m_call_impl\u001b[0m\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/modules/models.py\"\u001b[0m, line \u001b[35m565\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "[rank0]:     forces, virials, stress, hessian, edge_forces = \u001b[31mget_outputs\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:                                                     \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31menergy=inter_e,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<9 lines>...\n",
      "[rank0]:         \u001b[1;31mcompute_edge_forces=compute_edge_forces or compute_atomic_stresses,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/modules/utils.py\"\u001b[0m, line \u001b[35m195\u001b[0m, in \u001b[35mget_outputs\u001b[0m\n",
      "[rank0]:     \u001b[31mcompute_forces\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31menergy=energy,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mpositions=positions,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mtraining=(training or compute_hessian or compute_edge_forces),\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m,\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/modules/utils.py\"\u001b[0m, line \u001b[35m26\u001b[0m, in \u001b[35mcompute_forces\u001b[0m\n",
      "[rank0]:     gradient = \u001b[31mtorch.autograd.grad\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:                \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31moutputs=[energy],  # [n_graphs, ]\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<4 lines>...\n",
      "[rank0]:         \u001b[1;31mallow_unused=True,  # For complete dissociation turn to true\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m[\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py\"\u001b[0m, line \u001b[35m502\u001b[0m, in \u001b[35mgrad\u001b[0m\n",
      "[rank0]:     result = _engine_run_backward(\n",
      "[rank0]:         outputs,\n",
      "[rank0]:     ...<5 lines>...\n",
      "[rank0]:         accumulate_grad=False,\n",
      "[rank0]:     )\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py\"\u001b[0m, line \u001b[35m824\u001b[0m, in \u001b[35m_engine_run_backward\u001b[0m\n",
      "[rank0]:     return \u001b[31mVariable._execution_engine.run_backward\u001b[0m\u001b[1;31m(  # Calls into the C++ engine to run the backward pass\u001b[0m\n",
      "[rank0]:            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mt_outputs, *args, **kwargs\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m  # Calls into the C++ engine to run the backward pass\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]: \u001b[1;35mtorch.OutOfMemoryError\u001b[0m: \u001b[35mCUDA out of memory. Tried to allocate 2.96 GiB. GPU 0 has a total capacity of 69.50 GiB of which 1.88 GiB is free. Process 1362808 has 36.96 GiB memory in use. Including non-PyTorch memory, this process has 30.60 GiB memory in use. Of the allocated memory 29.36 GiB is allocated by PyTorch, and 274.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\u001b[0m\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "[rank0]:     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "[rank0]:     \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m767\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "[rank0]:     \u001b[31mtools.train\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<23 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m222\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "[rank0]:     \u001b[31mtrain_one_epoch\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<11 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m370\u001b[0m, in \u001b[35mtrain_one_epoch\u001b[0m\n",
      "[rank0]:     _, opt_metrics = \u001b[31mtake_step\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:                      \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model_to_train,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<6 lines>...\n",
      "[rank0]:         \u001b[1;31mdevice=device,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m416\u001b[0m, in \u001b[35mtake_step\u001b[0m\n",
      "[rank0]:     loss = closure()\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m410\u001b[0m, in \u001b[35mclosure\u001b[0m\n",
      "[rank0]:     \u001b[31mloss.backward\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/_tensor.py\"\u001b[0m, line \u001b[35m648\u001b[0m, in \u001b[35mbackward\u001b[0m\n",
      "[rank0]:     \u001b[31mtorch.autograd.backward\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mself, gradient, retain_graph, create_graph, inputs=inputs\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35mbackward\u001b[0m\n",
      "[rank0]:     \u001b[31m_engine_run_backward\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mtensors,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<5 lines>...\n",
      "[rank0]:         \u001b[1;31maccumulate_grad=True,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py\"\u001b[0m, line \u001b[35m824\u001b[0m, in \u001b[35m_engine_run_backward\u001b[0m\n",
      "[rank0]:     return \u001b[31mVariable._execution_engine.run_backward\u001b[0m\u001b[1;31m(  # Calls into the C++ engine to run the backward pass\u001b[0m\n",
      "[rank0]:            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mt_outputs, *args, **kwargs\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m  # Calls into the C++ engine to run the backward pass\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]: \u001b[1;35mtorch.OutOfMemoryError\u001b[0m: \u001b[35mCUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacity of 69.50 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 37.76 GiB memory in use. Process 1362809 has 30.60 GiB memory in use. Of the allocated memory 36.37 GiB is allocated by PyTorch, and 419.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\u001b[0m\n",
      "[rank0]:[W724 00:51:29.693594190 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank0]:[W724 00:51:29.701161116 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0724 00:51:32.010000 1362799 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1362809 closing signal SIGTERM\n",
      "E0724 00:51:32.276000 1362799 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1362808) of binary: /home/phanim/harshitrawat/miniconda3/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/bin/torchrun\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\"\u001b[0m, line \u001b[35m355\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return f(*args, **kwargs)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m892\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m883\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31melastic_launch(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mconfig=config,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mentrypoint=cmd,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "    \u001b[31m)\u001b[0m\u001b[1;31m(*cmd_args)\u001b[0m\n",
      "    \u001b[31m~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m139\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m270\u001b[0m, in \u001b[35mlaunch_agent\u001b[0m\n",
      "    raise ChildFailedError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "\u001b[1;35mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError\u001b[0m: \u001b[35m\n",
      "============================================================\n",
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-07-24_00:51:32\n",
      "  host      : cn10.cds.iisc.in\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1362808)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Only expose ONE MIG device to torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # or \"3\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# PYTHONPATH optional if already in sys.path\n",
    "os.environ[\"PYTHONPATH\"] = \"/home/phanim/harshitrawat/mace/mace\"\n",
    "!torchrun --standalone \\\n",
    "  --nnodes=1 \\\n",
    "  --nproc_per_node=2 \\\n",
    "  /home/phanim/harshitrawat/mace/mace/cli/run_train.py \\\n",
    "  --name=\"mace_T1_finetune\" \\\n",
    "  --model=\"MACE\" \\\n",
    "  --num_interactions=1 \\\n",
    "  --hidden_irreps=\"64x0e+64x1o+64x2e\" \\\n",
    "  --train_file=\"/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz\" \\\n",
    "  --test_file=\"/home/phanim/harshitrawat/summer/final_work/T2_chgnet_labeled.extxyz\" \\\n",
    "  --foundation_model=\"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\" \\\n",
    "  --foundation_model_readout \\\n",
    "  --batch_size=1 \\\n",
    "  --valid_batch_size=1 \\\n",
    "  --valid_fraction=0.005 \\\n",
    "  --device=\"cuda\" \\\n",
    "  --max_num_epochs=5 \\\n",
    "  --r_max=5.0 \\\n",
    "  --E0s=\"{3:-201.7093,8:-431.6112,40:-1275.9529,57:-857.6754}\" \\\n",
    "  --seed=42 \\\n",
    "  --distributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3a5d378-079b-402a-bd8f-0380345ce1a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0724 01:32:57.517000 1415701 site-packages/torch/distributed/run.py:766] \n",
      "W0724 01:32:57.517000 1415701 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "W0724 01:32:57.517000 1415701 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0724 01:32:57.517000 1415701 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "2025-07-24 01:33:04.560 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-24 01:33:04.565 INFO: Process group initialized: True\n",
      "2025-07-24 01:33:04.570 INFO: Processes: 1\n",
      "2025-07-24 01:33:04.570 INFO: MACE version: 0.3.14\n",
      "2025-07-24 01:33:04.571 INFO: CUDA version: 12.6, CUDA device: 0\n",
      "2025-07-24 01:33:04.576 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-24 01:33:04.591 INFO: Process group initialized: True\n",
      "2025-07-24 01:33:04.591 INFO: Processes: 1\n",
      "2025-07-24 01:33:04.591 INFO: MACE version: 0.3.14\n",
      "2025-07-24 01:33:04.592 INFO: CUDA version: 12.6, CUDA device: 0\n",
      "2025-07-24 01:33:04.828 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-07-24 01:33:04.829 INFO: Using heads: ['Default']\n",
      "2025-07-24 01:33:04.829 INFO: Using the key specifications to parse data:\n",
      "2025-07-24 01:33:04.829 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-07-24 01:33:04.829 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-07-24 01:33:04.829 INFO: =============    Processing head Default     ===========\n",
      "2025-07-24 01:33:04.829 INFO: Using heads: ['Default']\n",
      "2025-07-24 01:33:04.829 INFO: Using the key specifications to parse data:\n",
      "2025-07-24 01:33:04.829 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-07-24 01:33:04.829 INFO: =============    Processing head Default     ===========\n",
      "2025-07-24 01:33:25.892 INFO: Training set 1/1 [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 01:33:25.911 INFO: Total Training set [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 01:33:25.912 INFO: No validation set provided, splitting training data instead.\n",
      "2025-07-24 01:33:25.914 INFO: Using random 0% of training set for validation with indices saved in: ./valid_indices_42.txt\n",
      "2025-07-24 01:33:25.925 INFO: Training set 1/1 [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 01:33:25.936 INFO: Random Split Training set [energy: 6306, stress: 0, virials: 0, dipole components: 0, head: 6306, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6306, charges: 0]\n",
      "2025-07-24 01:33:25.936 INFO: Random Split Validation set [energy: 31, stress: 0, virials: 0, dipole components: 0, head: 31, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 31, charges: 0]\n",
      "2025-07-24 01:33:25.944 INFO: Total Training set [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 01:33:25.944 INFO: No validation set provided, splitting training data instead.\n",
      "2025-07-24 01:33:25.948 INFO: Using random 0% of training set for validation with indices saved in: ./valid_indices_42.txt\n",
      "2025-07-24 01:33:25.970 INFO: Random Split Training set [energy: 6306, stress: 0, virials: 0, dipole components: 0, head: 6306, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6306, charges: 0]\n",
      "2025-07-24 01:33:25.970 INFO: Random Split Validation set [energy: 31, stress: 0, virials: 0, dipole components: 0, head: 31, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 31, charges: 0]\n",
      "2025-07-24 01:33:28.542 INFO: Test set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 01:33:28.544 INFO: Total Test set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 01:33:28.544 INFO: Total number of configurations: train=6306, valid=31, tests=[Default_Default: 705],\n",
      "2025-07-24 01:33:28.569 INFO: Test set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 01:33:28.571 INFO: Total Test set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 01:33:28.571 INFO: Total number of configurations: train=6306, valid=31, tests=[Default_Default: 705],\n",
      "2025-07-24 01:33:28.822 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-07-24 01:33:28.822 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-07-24 01:33:28.822 INFO: Atomic Energies used (z: eV) for head Default: {3: -201.7093, 8: -431.6112, 40: -1275.9529, 57: -857.6754}\n",
      "2025-07-24 01:33:28.822 INFO: Processing datasets for head 'Default'\n",
      "2025-07-24 01:33:28.851 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-07-24 01:33:28.851 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-07-24 01:33:28.851 INFO: Atomic Energies used (z: eV) for head Default: {3: -201.7093, 8: -431.6112, 40: -1275.9529, 57: -857.6754}\n",
      "2025-07-24 01:33:28.851 INFO: Processing datasets for head 'Default'\n",
      "2025-07-24 01:34:24.153 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-07-24 01:34:24.329 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-07-24 01:34:24.472 INFO: Head 'Default' training dataset size: 6306\n",
      "2025-07-24 01:34:24.473 INFO: Computing average number of neighbors\n",
      "2025-07-24 01:34:24.650 INFO: Head 'Default' training dataset size: 6306\n",
      "2025-07-24 01:34:24.650 INFO: Computing average number of neighbors\n",
      "2025-07-24 01:34:37.924 INFO: Average number of neighbors: 38.70344595488896\n",
      "2025-07-24 01:34:37.924 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-07-24 01:34:37.924 INFO: ===========MODEL DETAILS===========\n",
      "2025-07-24 01:34:37.934 INFO: Average number of neighbors: 38.70344595488896\n",
      "2025-07-24 01:34:37.934 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-07-24 01:34:37.934 INFO: ===========MODEL DETAILS===========\n",
      "2025-07-24 01:34:43.645 INFO: Building model\n",
      "2025-07-24 01:34:43.646 INFO: Message passing with 64 channels and max_L=2 (64x0e+64x1o+64x2e)\n",
      "2025-07-24 01:34:43.646 INFO: 1 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-07-24 01:34:43.646 INFO: 8 radial and 5 basis functions\n",
      "2025-07-24 01:34:43.646 INFO: Radial cutoff: 5.0 A (total receptive field for each atom: 5.0 A)\n",
      "2025-07-24 01:34:43.646 INFO: Distance transform for radial basis functions: None\n",
      "2025-07-24 01:34:43.646 INFO: Hidden irreps: 64x0e+64x1o+64x2e\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "2025-07-24 01:34:43.720 INFO: Building model\n",
      "2025-07-24 01:34:43.721 INFO: Message passing with 64 channels and max_L=2 (64x0e+64x1o+64x2e)\n",
      "2025-07-24 01:34:43.721 INFO: 1 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-07-24 01:34:43.721 INFO: 8 radial and 5 basis functions\n",
      "2025-07-24 01:34:43.721 INFO: Radial cutoff: 5.0 A (total receptive field for each atom: 5.0 A)\n",
      "2025-07-24 01:34:43.721 INFO: Distance transform for radial basis functions: None\n",
      "2025-07-24 01:34:43.721 INFO: Hidden irreps: 64x0e+64x1o+64x2e\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "2025-07-24 01:34:44.366 INFO: Total number of parameters: 164416\n",
      "2025-07-24 01:34:44.366 INFO: \n",
      "2025-07-24 01:34:44.366 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-07-24 01:34:44.366 INFO: Using ADAM as parameter optimizer\n",
      "2025-07-24 01:34:44.366 INFO: Batch size: 4\n",
      "2025-07-24 01:34:44.366 INFO: Number of gradient updates: 7882\n",
      "2025-07-24 01:34:44.366 INFO: Learning rate: 0.01, weight decay: 5e-07\n",
      "2025-07-24 01:34:44.366 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-07-24 01:34:44.370 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-07-24 01:34:44.370 INFO: \n",
      "2025-07-24 01:34:44.370 INFO: ===========TRAINING===========\n",
      "2025-07-24 01:34:44.370 INFO: Started training, reporting errors on validation set\n",
      "2025-07-24 01:34:44.370 INFO: Loss metrics on validation set\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "2025-07-24 01:34:44.419 INFO: Total number of parameters: 164416\n",
      "2025-07-24 01:34:44.419 INFO: \n",
      "2025-07-24 01:34:44.419 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-07-24 01:34:44.419 INFO: Using ADAM as parameter optimizer\n",
      "2025-07-24 01:34:44.419 INFO: Batch size: 4\n",
      "2025-07-24 01:34:44.419 INFO: Number of gradient updates: 7882\n",
      "2025-07-24 01:34:44.419 INFO: Learning rate: 0.01, weight decay: 5e-07\n",
      "2025-07-24 01:34:44.419 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-07-24 01:34:44.425 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-07-24 01:34:44.425 INFO: \n",
      "2025-07-24 01:34:44.425 INFO: ===========TRAINING===========\n",
      "2025-07-24 01:34:44.425 INFO: Started training, reporting errors on validation set\n",
      "2025-07-24 01:34:44.425 INFO: Loss metrics on validation set\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "2025-07-24 01:34:47.947 INFO: Initial: head: Default, loss=84131.06202039, RMSE_E_per_atom=290030.84 meV, RMSE_F=  365.66 meV / A\n",
      "2025-07-24 01:34:47.948 INFO: Initial: head: Default, loss=84131.06202039, RMSE_E_per_atom=290030.84 meV, RMSE_F=  365.66 meV / A\n",
      "2025-07-24 01:44:04.849 INFO: Epoch 0: head: Default, loss=5426.93807002, RMSE_E_per_atom=17866.21 meV, RMSE_F= 7246.69 meV / A\n",
      "2025-07-24 01:44:04.900 INFO: Epoch 0: head: Default, loss=6507.78962434, RMSE_E_per_atom=29279.82 meV, RMSE_F= 7531.38 meV / A\n",
      "^C\n",
      "W0724 01:46:08.328000 1415701 site-packages/torch/distributed/elastic/agent/server/api.py:719] Received 2 death signal, shutting down workers\n",
      "W0724 01:46:08.331000 1415701 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1415709 closing signal SIGINT\n",
      "W0724 01:46:08.333000 1415701 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1415710 closing signal SIGINT\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "[rank0]:     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "[rank0]:     \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m767\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "[rank0]:     \u001b[31mtools.train\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<23 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m222\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "[rank0]:     \u001b[31mtrain_one_epoch\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<11 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m370\u001b[0m, in \u001b[35mtrain_one_epoch\u001b[0m\n",
      "[rank0]:     _, opt_metrics = \u001b[31mtake_step\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:                      \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model_to_train,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<6 lines>...\n",
      "[rank0]:         \u001b[1;31mdevice=device,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m416\u001b[0m, in \u001b[35mtake_step\u001b[0m\n",
      "[rank0]:     loss = closure()\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m409\u001b[0m, in \u001b[35mclosure\u001b[0m\n",
      "[rank0]:     loss = loss_fn(pred=output, ref=batch)\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m1751\u001b[0m, in \u001b[35m_wrapped_call_impl\u001b[0m\n",
      "[rank0]:     return \u001b[31mself._call_impl\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "[rank0]:            \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m1762\u001b[0m, in \u001b[35m_call_impl\u001b[0m\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/modules/loss.py\"\u001b[0m, line \u001b[35m241\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "[rank0]:     loss_forces = mean_squared_error_forces(ref, pred, ddp)\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/modules/loss.py\"\u001b[0m, line \u001b[35m127\u001b[0m, in \u001b[35mmean_squared_error_forces\u001b[0m\n",
      "[rank0]:     configs_forces_weight = \u001b[31mtorch.repeat_interleave\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:                             \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mref.forces_weight, ref.ptr[1:] - ref.ptr[:-1]\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m.unsqueeze(-1)\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]: \u001b[1;35mKeyboardInterrupt\u001b[0m\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "[rank0]:     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "[rank0]:     \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m767\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "[rank0]:     \u001b[31mtools.train\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<23 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m222\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "[rank0]:     \u001b[31mtrain_one_epoch\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<11 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m370\u001b[0m, in \u001b[35mtrain_one_epoch\u001b[0m\n",
      "[rank0]:     _, opt_metrics = \u001b[31mtake_step\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:                      \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model_to_train,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<6 lines>...\n",
      "[rank0]:         \u001b[1;31mdevice=device,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m416\u001b[0m, in \u001b[35mtake_step\u001b[0m\n",
      "[rank0]:     loss = closure()\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m409\u001b[0m, in \u001b[35mclosure\u001b[0m\n",
      "[rank0]:     loss = loss_fn(pred=output, ref=batch)\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m1751\u001b[0m, in \u001b[35m_wrapped_call_impl\u001b[0m\n",
      "[rank0]:     return \u001b[31mself._call_impl\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "[rank0]:            \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m1762\u001b[0m, in \u001b[35m_call_impl\u001b[0m\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/modules/loss.py\"\u001b[0m, line \u001b[35m241\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "[rank0]:     loss_forces = mean_squared_error_forces(ref, pred, ddp)\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/modules/loss.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mmean_squared_error_forces\u001b[0m\n",
      "[rank0]:     configs_weight = \u001b[31mtorch.repeat_interleave\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:                      \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mref.weight, ref.ptr[1:] - ref.ptr[:-1]\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m.unsqueeze(-1)\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]: \u001b[1;35mKeyboardInterrupt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Only expose ONE MIG device to torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"MIG-98d798d2-0f1a-500f-acdd-1a3ae2bc68d3, MIG-f3bd2aad-c585-52fe-b36a-42380cc6cfc6\"  # or \"3\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# PYTHONPATH optional if already in sys.path\n",
    "os.environ[\"PYTHONPATH\"] = \"/home/phanim/harshitrawat/mace/mace\"\n",
    "!torchrun --standalone \\\n",
    "  --nnodes=1 \\\n",
    "  --nproc_per_node=2 \\\n",
    "  /home/phanim/harshitrawat/mace/mace/cli/run_train.py \\\n",
    "  --name=\"mace_T1_finetune\" \\\n",
    "  --model=\"MACE\" \\\n",
    "  --num_interactions=1 \\\n",
    "  --hidden_irreps=\"64x0e+64x1o+64x2e\" \\\n",
    "  --train_file=\"/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz\" \\\n",
    "  --test_file=\"/home/phanim/harshitrawat/summer/final_work/T2_chgnet_labeled.extxyz\" \\\n",
    "  --batch_size=4 \\\n",
    "  --valid_batch_size=1 \\\n",
    "  --valid_fraction=0.005 \\\n",
    "  --device=\"cuda\" \\\n",
    "  --max_num_epochs=5 \\\n",
    "  --r_max=5.0 \\\n",
    "  --E0s=\"{3:-201.7093,8:-431.6112,40:-1275.9529,57:-857.6754}\" \\\n",
    "  --seed=42 \\\n",
    "  --distributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de7696d9-730a-4481-99f6-9d45c11df6ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0724 02:46:55.678000 1500316 site-packages/torch/distributed/run.py:766] \n",
      "W0724 02:46:55.678000 1500316 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "W0724 02:46:55.678000 1500316 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0724 02:46:55.678000 1500316 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "2025-07-24 02:47:02.400 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-24 02:47:02.405 INFO: Process group initialized: True\n",
      "2025-07-24 02:47:02.405 INFO: Processes: 1\n",
      "2025-07-24 02:47:02.405 INFO: MACE version: 0.3.14\n",
      "2025-07-24 02:47:02.405 INFO: CUDA version: 12.6, CUDA device: 0\n",
      "2025-07-24 02:47:02.465 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-24 02:47:02.473 INFO: Process group initialized: True\n",
      "2025-07-24 02:47:02.473 INFO: Processes: 1\n",
      "2025-07-24 02:47:02.473 INFO: MACE version: 0.3.14\n",
      "2025-07-24 02:47:02.474 INFO: CUDA version: 12.6, CUDA device: 0\n",
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py:146: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n",
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py:146: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n",
      "2025-07-24 02:47:03.090 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-07-24 02:47:03.091 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-07-24 02:47:03.091 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-07-24 02:47:03.091 INFO: Using heads: ['Default']\n",
      "2025-07-24 02:47:03.091 INFO: Using the key specifications to parse data:\n",
      "2025-07-24 02:47:03.091 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-07-24 02:47:03.091 INFO: =============    Processing head Default     ===========\n",
      "2025-07-24 02:47:03.099 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-07-24 02:47:03.139 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-07-24 02:47:03.139 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-07-24 02:47:03.139 INFO: Using heads: ['Default']\n",
      "2025-07-24 02:47:03.139 INFO: Using the key specifications to parse data:\n",
      "2025-07-24 02:47:03.139 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-07-24 02:47:03.140 INFO: =============    Processing head Default     ===========\n",
      "2025-07-24 02:47:24.174 INFO: Training set 1/1 [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 02:47:24.196 INFO: Total Training set [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 02:47:24.196 INFO: No validation set provided, splitting training data instead.\n",
      "2025-07-24 02:47:24.200 INFO: Using random 0% of training set for validation with indices saved in: ./valid_indices_42.txt\n",
      "2025-07-24 02:47:24.224 INFO: Random Split Training set [energy: 6306, stress: 0, virials: 0, dipole components: 0, head: 6306, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6306, charges: 0]\n",
      "2025-07-24 02:47:24.224 INFO: Random Split Validation set [energy: 31, stress: 0, virials: 0, dipole components: 0, head: 31, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 31, charges: 0]\n",
      "2025-07-24 02:47:24.278 INFO: Training set 1/1 [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 02:47:24.297 INFO: Total Training set [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6337, charges: 0]\n",
      "2025-07-24 02:47:24.297 INFO: No validation set provided, splitting training data instead.\n",
      "2025-07-24 02:47:24.301 INFO: Using random 0% of training set for validation with indices saved in: ./valid_indices_42.txt\n",
      "2025-07-24 02:47:24.324 INFO: Random Split Training set [energy: 6306, stress: 0, virials: 0, dipole components: 0, head: 6306, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 6306, charges: 0]\n",
      "2025-07-24 02:47:24.324 INFO: Random Split Validation set [energy: 31, stress: 0, virials: 0, dipole components: 0, head: 31, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 31, charges: 0]\n",
      "2025-07-24 02:47:26.825 INFO: Test set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 02:47:26.828 INFO: Total Test set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 02:47:26.828 INFO: Total number of configurations: train=6306, valid=31, tests=[Default_Default: 705],\n",
      "2025-07-24 02:47:26.973 INFO: Test set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 02:47:26.975 INFO: Total Test set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-07-24 02:47:26.975 INFO: Total number of configurations: train=6306, valid=31, tests=[Default_Default: 705],\n",
      "2025-07-24 02:47:27.095 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-07-24 02:47:27.095 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-07-24 02:47:27.095 INFO: Atomic Energies used (z: eV) for head Default: {3: -201.7093, 8: -431.6112, 40: -1275.9529, 57: -857.6754}\n",
      "2025-07-24 02:47:27.095 INFO: Processing datasets for head 'Default'\n",
      "2025-07-24 02:47:27.243 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-07-24 02:47:27.244 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-07-24 02:47:27.244 INFO: Atomic Energies used (z: eV) for head Default: {3: -201.7093, 8: -431.6112, 40: -1275.9529, 57: -857.6754}\n",
      "2025-07-24 02:47:27.244 INFO: Processing datasets for head 'Default'\n",
      "2025-07-24 02:48:47.675 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-07-24 02:48:48.146 INFO: Head 'Default' training dataset size: 6306\n",
      "2025-07-24 02:48:48.146 INFO: Computing average number of neighbors\n",
      "2025-07-24 02:48:48.324 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-07-24 02:48:48.795 INFO: Head 'Default' training dataset size: 6306\n",
      "2025-07-24 02:48:48.795 INFO: Computing average number of neighbors\n",
      "2025-07-24 02:49:13.156 INFO: Average number of neighbors: 68.47692474069387\n",
      "2025-07-24 02:49:13.156 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-07-24 02:49:13.156 INFO: ===========MODEL DETAILS===========\n",
      "2025-07-24 02:49:14.363 INFO: Average number of neighbors: 68.47692474069387\n",
      "2025-07-24 02:49:14.364 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-07-24 02:49:14.364 INFO: ===========MODEL DETAILS===========\n",
      "2025-07-24 02:49:24.686 INFO: Loading FOUNDATION model\n",
      "2025-07-24 02:49:24.688 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-07-24 02:49:24.688 INFO: Model configuration extracted from foundation model\n",
      "2025-07-24 02:49:24.688 INFO: Using weighted loss function for fine-tuning\n",
      "2025-07-24 02:49:24.688 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-07-24 02:49:24.688 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-07-24 02:49:24.688 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-07-24 02:49:24.688 INFO: Distance transform for radial basis functions: None\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "2025-07-24 02:49:25.603 INFO: Loading FOUNDATION model\n",
      "2025-07-24 02:49:25.605 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-07-24 02:49:25.605 INFO: Model configuration extracted from foundation model\n",
      "2025-07-24 02:49:25.605 INFO: Using weighted loss function for fine-tuning\n",
      "2025-07-24 02:49:25.605 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-07-24 02:49:25.606 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-07-24 02:49:25.606 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-07-24 02:49:25.606 INFO: Distance transform for radial basis functions: None\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "2025-07-24 02:49:25.863 INFO: Total number of parameters: 894362\n",
      "2025-07-24 02:49:25.863 INFO: \n",
      "2025-07-24 02:49:25.863 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-07-24 02:49:25.863 INFO: Using ADAM as parameter optimizer\n",
      "2025-07-24 02:49:25.863 INFO: Batch size: 1\n",
      "2025-07-24 02:49:25.863 INFO: Number of gradient updates: 126120\n",
      "2025-07-24 02:49:25.863 INFO: Learning rate: 0.01, weight decay: 5e-07\n",
      "2025-07-24 02:49:25.863 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-07-24 02:49:25.874 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-07-24 02:49:25.874 INFO: \n",
      "2025-07-24 02:49:25.874 INFO: ===========TRAINING===========\n",
      "2025-07-24 02:49:25.874 INFO: Started training, reporting errors on validation set\n",
      "2025-07-24 02:49:25.874 INFO: Loss metrics on validation set\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Using reduced CG: False\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "2025-07-24 02:49:26.768 INFO: Total number of parameters: 894362\n",
      "2025-07-24 02:49:26.768 INFO: \n",
      "2025-07-24 02:49:26.768 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-07-24 02:49:26.768 INFO: Using ADAM as parameter optimizer\n",
      "2025-07-24 02:49:26.768 INFO: Batch size: 1\n",
      "2025-07-24 02:49:26.768 INFO: Number of gradient updates: 126120\n",
      "2025-07-24 02:49:26.768 INFO: Learning rate: 0.01, weight decay: 5e-07\n",
      "2025-07-24 02:49:26.768 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-07-24 02:49:26.778 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-07-24 02:49:26.778 INFO: \n",
      "2025-07-24 02:49:26.778 INFO: ===========TRAINING===========\n",
      "2025-07-24 02:49:26.778 INFO: Started training, reporting errors on validation set\n",
      "2025-07-24 02:49:26.778 INFO: Loss metrics on validation set\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "2025-07-24 02:49:40.764 INFO: Initial: head: Default, loss=83458.08978851, RMSE_E_per_atom=288641.89 meV, RMSE_F= 1267.25 meV / A\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "2025-07-24 02:49:42.076 INFO: Initial: head: Default, loss=83458.08978851, RMSE_E_per_atom=288641.89 meV, RMSE_F= 1267.25 meV / A\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "[rank0]:     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "[rank0]:     \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m767\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "[rank0]:     \u001b[31mtools.train\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<23 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m222\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "[rank0]:     \u001b[31mtrain_one_epoch\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<11 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m370\u001b[0m, in \u001b[35mtrain_one_epoch\u001b[0m\n",
      "[rank0]:     _, opt_metrics = \u001b[31mtake_step\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:                      \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model_to_train,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<6 lines>...\n",
      "[rank0]:         \u001b[1;31mdevice=device,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m416\u001b[0m, in \u001b[35mtake_step\u001b[0m\n",
      "[rank0]:     loss = closure()\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m410\u001b[0m, in \u001b[35mclosure\u001b[0m\n",
      "[rank0]:     \u001b[31mloss.backward\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/_tensor.py\"\u001b[0m, line \u001b[35m648\u001b[0m, in \u001b[35mbackward\u001b[0m\n",
      "[rank0]:     \u001b[31mtorch.autograd.backward\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mself, gradient, retain_graph, create_graph, inputs=inputs\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35mbackward\u001b[0m\n",
      "[rank0]:     \u001b[31m_engine_run_backward\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mtensors,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<5 lines>...\n",
      "[rank0]:         \u001b[1;31maccumulate_grad=True,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py\"\u001b[0m, line \u001b[35m824\u001b[0m, in \u001b[35m_engine_run_backward\u001b[0m\n",
      "[rank0]:     return \u001b[31mVariable._execution_engine.run_backward\u001b[0m\u001b[1;31m(  # Calls into the C++ engine to run the backward pass\u001b[0m\n",
      "[rank0]:            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mt_outputs, *args, **kwargs\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m  # Calls into the C++ engine to run the backward pass\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]: \u001b[1;35mtorch.OutOfMemoryError\u001b[0m: \u001b[35mCUDA out of memory. Tried to allocate 434.00 MiB. GPU 0 has a total capacity of 69.50 GiB of which 86.31 MiB is free. Process 1500391 has 35.34 GiB memory in use. Including non-PyTorch memory, this process has 34.01 GiB memory in use. Of the allocated memory 32.70 GiB is allocated by PyTorch, and 341.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\u001b[0m\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "[rank0]:     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "[rank0]:     \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m767\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "[rank0]:     \u001b[31mtools.train\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<23 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m222\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "[rank0]:     \u001b[31mtrain_one_epoch\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<11 lines>...\n",
      "[rank0]:         \u001b[1;31mrank=rank,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m370\u001b[0m, in \u001b[35mtrain_one_epoch\u001b[0m\n",
      "[rank0]:     _, opt_metrics = \u001b[31mtake_step\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:                      \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mmodel=model_to_train,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<6 lines>...\n",
      "[rank0]:         \u001b[1;31mdevice=device,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m416\u001b[0m, in \u001b[35mtake_step\u001b[0m\n",
      "[rank0]:     loss = closure()\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m410\u001b[0m, in \u001b[35mclosure\u001b[0m\n",
      "[rank0]:     \u001b[31mloss.backward\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/_tensor.py\"\u001b[0m, line \u001b[35m648\u001b[0m, in \u001b[35mbackward\u001b[0m\n",
      "[rank0]:     \u001b[31mtorch.autograd.backward\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mself, gradient, retain_graph, create_graph, inputs=inputs\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py\"\u001b[0m, line \u001b[35m353\u001b[0m, in \u001b[35mbackward\u001b[0m\n",
      "[rank0]:     \u001b[31m_engine_run_backward\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "[rank0]:     \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mtensors,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^\u001b[0m\n",
      "[rank0]:     ...<5 lines>...\n",
      "[rank0]:         \u001b[1;31maccumulate_grad=True,\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]:   File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py\"\u001b[0m, line \u001b[35m824\u001b[0m, in \u001b[35m_engine_run_backward\u001b[0m\n",
      "[rank0]:     return \u001b[31mVariable._execution_engine.run_backward\u001b[0m\u001b[1;31m(  # Calls into the C++ engine to run the backward pass\u001b[0m\n",
      "[rank0]:            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:         \u001b[1;31mt_outputs, *args, **kwargs\u001b[0m\n",
      "[rank0]:         \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "[rank0]:     \u001b[1;31m)\u001b[0m  # Calls into the C++ engine to run the backward pass\n",
      "[rank0]:     \u001b[1;31m^\u001b[0m\n",
      "[rank0]: \u001b[1;35mtorch.OutOfMemoryError\u001b[0m: \u001b[35mCUDA out of memory. Tried to allocate 1.27 GiB. GPU 0 has a total capacity of 69.50 GiB of which 232.31 MiB is free. Including non-PyTorch memory, this process has 35.20 GiB memory in use. Process 1500392 has 34.01 GiB memory in use. Of the allocated memory 33.84 GiB is allocated by PyTorch, and 397.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\u001b[0m\n",
      "[rank0]:[W724 02:49:52.676488240 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "[rank0]:[W724 02:49:53.217311117 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "W0724 02:49:54.975000 1500316 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1500391 closing signal SIGTERM\n",
      "E0724 02:49:55.641000 1500316 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 1 (pid: 1500392) of binary: /home/phanim/harshitrawat/miniconda3/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/bin/torchrun\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\"\u001b[0m, line \u001b[35m355\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return f(*args, **kwargs)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m892\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m883\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31melastic_launch(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mconfig=config,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~\u001b[0m\n",
      "        \u001b[31mentrypoint=cmd,\u001b[0m\n",
      "        \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n",
      "    \u001b[31m)\u001b[0m\u001b[1;31m(*cmd_args)\u001b[0m\n",
      "    \u001b[31m~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m139\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m270\u001b[0m, in \u001b[35mlaunch_agent\u001b[0m\n",
      "    raise ChildFailedError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "\u001b[1;35mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError\u001b[0m: \u001b[35m\n",
      "============================================================\n",
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-07-24_02:49:54\n",
      "  host      : cn10.cds.iisc.in\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 1500392)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Only expose ONE MIG device to torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"MIG-98d798d2-0f1a-500f-acdd-1a3ae2bc68d3, MIG-f3bd2aad-c585-52fe-b36a-42380cc6cfc6\"  # or \"3\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# PYTHONPATH optional if already in sys.path\n",
    "os.environ[\"PYTHONPATH\"] = \"/home/phanim/harshitrawat/mace/mace\"\n",
    "!torchrun --standalone \\\n",
    "  --nnodes=1 \\\n",
    "  --nproc_per_node=2 \\\n",
    "  /home/phanim/harshitrawat/mace/mace/cli/run_train.py \\\n",
    "  --name=\"mace_T1_finetune_a\" \\\n",
    "  --model=\"MACE\" \\\n",
    "  --num_interactions=1 \\\n",
    "  --foundation_model=\"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\" \\\n",
    "  --foundation_model_readout \\\n",
    "  --hidden_irreps=\"64x0e+64x1o+64x2e\" \\\n",
    "  --train_file=\"/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz\" \\\n",
    "  --test_file=\"/home/phanim/harshitrawat/summer/final_work/T2_chgnet_labeled.extxyz\" \\\n",
    "  --batch_size=1 \\\n",
    "  --valid_batch_size=1 \\\n",
    "  --valid_fraction=0.005 \\\n",
    "  --device=\"cuda\" \\\n",
    "  --forces_weight=100.0 \\\n",
    "  --energy_weight=1.0 \\\n",
    "  --max_num_epochs=20 \\\n",
    "  --r_max=5.0 \\\n",
    "  --E0s=\"{3:-201.7093,8:-431.6112,40:-1275.9529,57:-857.6754}\" \\\n",
    "  --seed=42 \\\n",
    "  --distributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "335ad365-3136-4223-a727-cb10b13eedae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy (eV): None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Atoms object has no calculator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m atoms \u001b[38;5;241m=\u001b[39m read(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnergy (eV):\u001b[39m\u001b[38;5;124m\"\u001b[39m, atoms\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForces (eV/Å):\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43matoms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_forces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mace_0.3.8/lib/python3.10/site-packages/ase/atoms.py:828\u001b[0m, in \u001b[0;36mAtoms.get_forces\u001b[0;34m(self, apply_constraint, md)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calculate atomic forces.\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \n\u001b[1;32m    816\u001b[0m \u001b[38;5;124;03mAsk the attached calculator to calculate the forces and apply\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03mconstraints (required for molecular dynamics with this type of\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;124;03mconstraints).\"\"\"\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAtoms object has no calculator.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    829\u001b[0m forces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc\u001b[38;5;241m.\u001b[39mget_forces(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apply_constraint:\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;66;03m# We need a special md flag here because for MD we want\u001b[39;00m\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;66;03m# to skip real constraints but include special \"constraints\"\u001b[39;00m\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;66;03m# Like Hookean.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Atoms object has no calculator."
     ]
    }
   ],
   "source": [
    "from ase.io import read\n",
    "atoms = read(\"/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz\", index=0)\n",
    "print(\"Energy (eV):\", atoms.info.get(\"energy\"))\n",
    "print(\"Forces (eV/Å):\", atoms.get_forces())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e34b601-b784-45aa-88b3-8df3a50e56dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully read 6337 structures\n"
     ]
    }
   ],
   "source": [
    "from ase.io import read\n",
    "from ase.io.extxyz import read_extxyz\n",
    "import traceback\n",
    "\n",
    "extxyz_path = \"/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz\"\n",
    "\n",
    "try:\n",
    "    atoms_list = list(read(extxyz_path, index=\":\"))\n",
    "    print(f\"✅ Successfully read {len(atoms_list)} structures\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to read with ASE:\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49933410-dd34-4479-8d65-1d5e1353ec4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'read_from_json' from 'mace.tools.utils' (/home/phanim/harshitrawat/mace/mace/tools/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MACE\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_from_json\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random_split, DataLoader\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'read_from_json' from 'mace.tools.utils' (/home/phanim/harshitrawat/mace/mace/tools/utils.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from mace.modules import MACE\n",
    "from mace.tools.utils import read_from_json\n",
    "from mace.data.utils import load_dataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from mace.training.trainer import Trainer\n",
    "from mace.training.loss_functions import EnergyForcesLoss\n",
    "from mace.training.utils import setup_logger\n",
    "\n",
    "# ==== Config ====\n",
    "pretrained_model_path = \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\"\n",
    "train_path = \"/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz\"\n",
    "valid_fraction = 0.005\n",
    "save_dir = \"mace_t1_finetuned_large\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 2\n",
    "max_epochs = 20\n",
    "r_max = 5.0\n",
    "seed = 42\n",
    "\n",
    "# ==== Reproducibility ====\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# ==== Load pretrained model config ====\n",
    "config = read_from_json(pretrained_model_path + \".config.json\")\n",
    "model = MACE(**config).to(device)\n",
    "model.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n",
    "\n",
    "# ==== Load dataset ====\n",
    "data = load_dataset(train_path, r_max=r_max)\n",
    "num_valid = max(1, int(valid_fraction * len(data)))\n",
    "num_train = len(data) - num_valid\n",
    "train_set, valid_set = random_split(data, [num_train, num_valid])\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=1)\n",
    "\n",
    "# ==== Define Loss and Optimizer ====\n",
    "loss_fn = EnergyForcesLoss(forces_weight=100.0, energy_weight=1.0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ==== Trainer Setup ====\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "logger = setup_logger(save_dir)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    save_dir=save_dir,\n",
    "    max_num_epochs=max_epochs,\n",
    "    device=device,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# ==== Train ====\n",
    "trainer.train()\n",
    "\n",
    "# ==== Save Final Model ====\n",
    "torch.save(model.state_dict(), os.path.join(save_dir, \"final_model.pt\"))\n",
    "print(\"✅ Training complete. Model saved at:\", save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b417a1-9a33-4b8b-8c8e-d775c2764b44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: mace_run_train \\\n",
      "    --name \\\n",
      "    mace_T1_finetune_scripted \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/final_work/T2_chgnet_labeled.extxyz \\\n",
      "    --batch_size \\\n",
      "    4 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --valid_fraction \\\n",
      "    0.1 \\\n",
      "    --ema_decay \\\n",
      "    0.99999 \\\n",
      "    --lr \\\n",
      "    0.0001 \\\n",
      "    --num_samples_pt \\\n",
      "    100000 \\\n",
      "    --forces_weight \\\n",
      "    10 \\\n",
      "    --energy_weight \\\n",
      "    1 \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --loss \\\n",
      "    universal \\\n",
      "    --max_num_epochs \\\n",
      "    20 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --enable_cueq \\\n",
      "    True \\\n",
      "    --restart_latest \\\n",
      "    --E0s \\\n",
      "    average \\\n",
      "    --seed \\\n",
      "    42\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 01:11:50.960 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-07-25 01:11:50.960 INFO: MACE version: 0.3.13\n",
      "2025-07-25 01:11:51.622 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py:157: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 01:11:52.299 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-07-25 01:11:52.302 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-07-25 01:11:52.302 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-07-25 01:11:52.302 INFO: Using heads: ['Default']\n",
      "2025-07-25 01:11:52.302 INFO: Using the key specifications to parse data:\n",
      "2025-07-25 01:11:52.302 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-07-25 01:11:52.302 INFO: =============    Processing head Default     ===========\n",
      "2025-07-25 01:12:13.297 INFO: Training set 1/1 [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, forces: 6337, charges: 0]\n",
      "2025-07-25 01:12:13.315 INFO: Total Training set [energy: 6337, stress: 0, virials: 0, dipole components: 0, head: 6337, forces: 6337, charges: 0]\n",
      "2025-07-25 01:12:15.913 INFO: Validation set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, forces: 705, charges: 0]\n",
      "2025-07-25 01:12:15.915 INFO: Total Validation set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, forces: 705, charges: 0]\n",
      "2025-07-25 01:12:15.915 INFO: Total number of configurations: train=6337, valid=705, tests=[],\n",
      "2025-07-25 01:12:16.218 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-07-25 01:12:16.218 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-07-25 01:12:16.218 INFO: Computing average Atomic Energies using least squares regression\n",
      "2025-07-25 01:12:16.257 INFO: Atomic Energies used (z: eV) for head Default: {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\n",
      "2025-07-25 01:12:16.257 INFO: Processing datasets for head 'Default'\n",
      "2025-07-25 01:13:39.274 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-07-25 01:13:48.735 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-07-25 01:13:48.736 INFO: Combined validation datasets for Default\n",
      "2025-07-25 01:13:48.736 INFO: Head 'Default' training dataset size: 6337\n",
      "2025-07-25 01:13:48.736 INFO: Computing average number of neighbors\n",
      "2025-07-25 01:13:54.613 INFO: Average number of neighbors: 68.51557639403025\n",
      "2025-07-25 01:13:54.614 INFO: During training the following quantities will be reported: energy, forces, stress\n",
      "2025-07-25 01:13:54.614 INFO: ===========MODEL DETAILS===========\n",
      "2025-07-25 01:14:00.188 INFO: Loading FOUNDATION model\n",
      "2025-07-25 01:14:00.189 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-07-25 01:14:00.189 INFO: Model configuration extracted from foundation model\n",
      "2025-07-25 01:14:00.190 INFO: Using universal loss function for fine-tuning\n",
      "2025-07-25 01:14:00.190 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-07-25 01:14:00.190 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-07-25 01:14:00.190 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-07-25 01:14:00.190 INFO: Distance transform for radial basis functions: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/modules/models.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"atomic_numbers\", torch.tensor(atomic_numbers, dtype=torch.int64)\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 01:14:01.606 INFO: Total number of parameters: 894362\n",
      "2025-07-25 01:14:01.606 INFO: \n",
      "2025-07-25 01:14:01.606 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-07-25 01:14:01.606 INFO: Using ADAM as parameter optimizer\n",
      "2025-07-25 01:14:01.606 INFO: Batch size: 4\n",
      "2025-07-25 01:14:01.606 INFO: Number of gradient updates: 31685\n",
      "2025-07-25 01:14:01.606 INFO: Learning rate: 0.0001, weight decay: 5e-07\n",
      "2025-07-25 01:14:01.606 INFO: UniversalLoss(energy_weight=1.000, forces_weight=10.000, stress_weight=1.000)\n",
      "2025-07-25 01:14:01.606 INFO: Converting model to CUEQ for accelerated training\n",
      "2025-07-25 01:14:01.684 INFO: Creating new model with cuequivariance settings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/bin/mace_run_train\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m75\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m668\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    model = run_e3nn_to_cueq(deepcopy(model), device=device)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/convert_e3nn_cueq.py\"\u001b[0m, line \u001b[35m166\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mtransfer_weights\u001b[0m\u001b[1;31m(source_model, target_model, max_L, correlation, num_layers)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/convert_e3nn_cueq.py\"\u001b[0m, line \u001b[35m125\u001b[0m, in \u001b[35mtransfer_weights\u001b[0m\n",
      "    \u001b[31mtarget_model.load_state_dict\u001b[0m\u001b[1;31m(target_dict)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m2593\u001b[0m, in \u001b[35mload_state_dict\u001b[0m\n",
      "    raise RuntimeError(\n",
      "    ...<3 lines>...\n",
      "    )\n",
      "\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mError(s) in loading state_dict for ScaleShiftMACE:\n",
      "\tUnexpected key(s) in state_dict: \"products.0.symmetric_contractions.weight\", \"products.1.symmetric_contractions.weight\". \n",
      "\tsize mismatch for node_embedding.linear.weight: copying a param with shape torch.Size([1, 512]) from checkpoint, the shape in current model is torch.Size([512]).\n",
      "\tsize mismatch for interactions.0.linear_up.weight: copying a param with shape torch.Size([1, 16384]) from checkpoint, the shape in current model is torch.Size([16384]).\n",
      "\tsize mismatch for interactions.0.linear.weight: copying a param with shape torch.Size([1, 65536]) from checkpoint, the shape in current model is torch.Size([65536]).\n",
      "\tsize mismatch for interactions.0.skip_tp.weight: copying a param with shape torch.Size([1, 65536]) from checkpoint, the shape in current model is torch.Size([65536]).\n",
      "\tsize mismatch for interactions.1.linear_up.weight: copying a param with shape torch.Size([1, 49152]) from checkpoint, the shape in current model is torch.Size([49152]).\n",
      "\tsize mismatch for interactions.1.linear.weight: copying a param with shape torch.Size([1, 278528]) from checkpoint, the shape in current model is torch.Size([278528]).\n",
      "\tsize mismatch for interactions.1.skip_tp.weight: copying a param with shape torch.Size([1, 65536]) from checkpoint, the shape in current model is torch.Size([65536]).\n",
      "\tsize mismatch for products.0.linear.weight: copying a param with shape torch.Size([1, 49152]) from checkpoint, the shape in current model is torch.Size([49152]).\n",
      "\tsize mismatch for products.1.linear.weight: copying a param with shape torch.Size([1, 16384]) from checkpoint, the shape in current model is torch.Size([16384]).\n",
      "\tsize mismatch for readouts.0.linear.weight: copying a param with shape torch.Size([1, 128]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "\tsize mismatch for readouts.1.linear_1.weight: copying a param with shape torch.Size([1, 2048]) from checkpoint, the shape in current model is torch.Size([2048]).\n",
      "\tsize mismatch for readouts.1.linear_2.weight: copying a param with shape torch.Size([1, 16]) from checkpoint, the shape in current model is torch.Size([16]).\u001b[0m\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['mace_run_train', '--name', 'mace_T1_finetune_scripted', '--model', 'MACE', '--num_interactions', '2', '--foundation_model', '/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model', '--foundation_model_readout', '--train_file', '/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz', '--valid_file', '/home/phanim/harshitrawat/summer/final_work/T2_chgnet_labeled.extxyz', '--batch_size', '4', '--valid_batch_size', '1', '--valid_fraction', '0.1', '--ema_decay', '0.99999', '--lr', '0.0001', '--num_samples_pt', '100000', '--forces_weight', '10', '--energy_weight', '1', '--device', 'cuda', '--loss', 'universal', '--max_num_epochs', '20', '--r_max', '5.0', '--enable_cueq', 'True', '--restart_latest', '--E0s', 'average', '--seed', '42']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mrun(cmd, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 47\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cmd), file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# execute\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mace_0.3.8/lib/python3.10/subprocess.py:526\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 526\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    527\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['mace_run_train', '--name', 'mace_T1_finetune_scripted', '--model', 'MACE', '--num_interactions', '2', '--foundation_model', '/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model', '--foundation_model_readout', '--train_file', '/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz', '--valid_file', '/home/phanim/harshitrawat/summer/final_work/T2_chgnet_labeled.extxyz', '--batch_size', '4', '--valid_batch_size', '1', '--valid_fraction', '0.1', '--ema_decay', '0.99999', '--lr', '0.0001', '--num_samples_pt', '100000', '--forces_weight', '10', '--energy_weight', '1', '--device', 'cuda', '--loss', 'universal', '--max_num_epochs', '20', '--r_max', '5.0', '--enable_cueq', 'True', '--restart_latest', '--E0s', 'average', '--seed', '42']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    # expose exactly the two MIG slices you want\n",
    "    # reduce fragmentation\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    # so that `mace` imports point to your local clone\n",
    "    os.environ[\"PYTHONPATH\"] = \"/home/phanim/harshitrawat/mace/mace\"\n",
    "\n",
    "    # ——— Compose the torchrun command ———\n",
    "    cmd = [\n",
    "        \"mace_run_train\",\n",
    "        \"--name\",              \"mace_T1_finetune_scripted\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "        \"--train_file\",        \"/home/phanim/harshitrawat/summer/final_work/T1_chgnet_labeled.extxyz\",\n",
    "        \"--valid_file\",        \"/home/phanim/harshitrawat/summer/final_work/T2_chgnet_labeled.extxyz\",\n",
    "        \"--batch_size\",        \"4\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "        \"--valid_fraction\",    \"0.1\",\n",
    "        \"--ema_decay\",         \"0.99999\",\n",
    "        \"--lr\",                \"0.0001\",\n",
    "        \"--num_samples_pt\",    \"100000\",\n",
    "        \"--forces_weight\",     \"10\",\n",
    "        \"--energy_weight\",     \"1\",\n",
    "        \"--device\",            \"cuda\",\n",
    "        \"--loss\",              \"universal\",\n",
    "        \"--max_num_epochs\",    \"20\",\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--enable_cueq\",       \"True\",\n",
    "        \"--restart_latest\",  # ✅ add this\n",
    "        \"--E0s\",               \"average\",\n",
    "        \"--seed\",              \"42\",\n",
    "    ]\n",
    "\n",
    "    # echo it so you can audit\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    # execute\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9991dc5-2ee5-47ce-b55c-37dbab2e3905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA H200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56257dc-08b3-446a-9c64-58cbfca75a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU mace_0.3.8)",
   "language": "python",
   "name": "mace_0.3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
