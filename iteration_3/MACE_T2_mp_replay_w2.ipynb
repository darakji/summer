{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74803bd4-0874-4a95-bcd2-702000a67339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: mace_run_train \\\n",
      "    --name \\\n",
      "    mace_T2_including_replay_w2 \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --pt_train_file \\\n",
      "    /home/phanim/harshitrawat/summer/replay_data/mp_finetuning-mace_T2_mp_replay_run-42.xyz \\\n",
      "    --atomic_numbers \\\n",
      "    [3,8,40,57] \\\n",
      "    --multiheads_finetuning \\\n",
      "    True \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz \\\n",
      "    --batch_size \\\n",
      "    2 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --forces_weight \\\n",
      "    0 \\\n",
      "    --energy_weight \\\n",
      "    10 \\\n",
      "    --stress_weight \\\n",
      "    0 \\\n",
      "    --lr \\\n",
      "    0.006 \\\n",
      "    --scheduler_patience \\\n",
      "    4 \\\n",
      "    --clip_grad \\\n",
      "    1 \\\n",
      "    --weight_decay \\\n",
      "    1e-8 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    130 \\\n",
      "    --E0s \\\n",
      "    {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549} \\\n",
      "    --seed \\\n",
      "    84 \\\n",
      "    --patience \\\n",
      "    8 \\\n",
      "    --restart_latest\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-21 10:16:49.590 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-08-21 10:16:49.591 INFO: MACE version: 0.3.14\n",
      "2025-08-21 10:16:50.231 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/cli/run_train.py:152: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-21 10:16:50.719 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-08-21 10:16:50.721 INFO: Multihead finetuning mode, setting learning rate to 0.0001 and EMA to True. To use a different learning rate, set --force_mh_ft_lr=True.\n",
      "2025-08-21 10:16:50.721 INFO: Using multiheads finetuning mode, setting learning rate to 0.0001 and EMA to True\n",
      "2025-08-21 10:16:50.721 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-08-21 10:16:50.721 INFO: Using heads: ['Default', 'pt_head']\n",
      "2025-08-21 10:16:50.721 INFO: Using the key specifications to parse data:\n",
      "2025-08-21 10:16:50.721 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-21 10:16:50.721 INFO: pt_head: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'polarizability': 'polarizability', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-21 10:16:50.721 INFO: =============    Processing head Default     ===========\n",
      "2025-08-21 10:16:56.090 INFO: Training set 1/1 [energy: 1612, stress: 0, virials: 0, dipole components: 0, head: 1612, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1612, charges: 0]\n",
      "2025-08-21 10:16:56.094 INFO: Total Training set [energy: 1612, stress: 0, virials: 0, dipole components: 0, head: 1612, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1612, charges: 0]\n",
      "2025-08-21 10:16:58.401 INFO: Validation set 1/1 [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-08-21 10:16:58.403 INFO: Total Validation set [energy: 705, stress: 0, virials: 0, dipole components: 0, head: 705, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 705, charges: 0]\n",
      "2025-08-21 10:16:58.403 INFO: Total number of configurations: train=1612, valid=705, tests=[],\n",
      "2025-08-21 10:16:58.403 INFO: =============    Processing head pt_head     ===========\n",
      "2025-08-21 10:17:01.415 INFO: Training set 1/1 [energy: 10000, stress: 10000, virials: 0, dipole components: 0, head: 10000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 10000, charges: 0]\n",
      "2025-08-21 10:17:01.443 INFO: Total Training set [energy: 10000, stress: 10000, virials: 0, dipole components: 0, head: 10000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 10000, charges: 0]\n",
      "2025-08-21 10:17:01.443 INFO: No validation set provided, splitting training data instead.\n",
      "2025-08-21 10:17:01.447 INFO: Using random 10% of training set for validation with indices saved in: ./valid_indices_84.txt\n",
      "2025-08-21 10:17:01.477 INFO: Random Split Training set [energy: 9000, stress: 9000, virials: 0, dipole components: 0, head: 9000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 9000, charges: 0]\n",
      "2025-08-21 10:17:01.480 INFO: Random Split Validation set [energy: 1000, stress: 1000, virials: 0, dipole components: 0, head: 1000, elec_temp: 0, total_charge: 0, polarizability: 0, total_spin: 0, forces: 1000, charges: 0]\n",
      "2025-08-21 10:17:01.480 INFO: Total number of configurations: train=9000, valid=1000, tests=[],\n",
      "2025-08-21 10:17:01.481 INFO: ==================Using multiheads finetuning mode==================\n",
      "2025-08-21 10:17:01.481 INFO: Total number of configurations in pretraining: train=9000, valid=1000\n",
      "2025-08-21 10:17:01.481 INFO: Using atomic numbers from command line argument\n",
      "2025-08-21 10:17:01.506 INFO: Atomic Numbers used: [1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91, 92, 93, 94]\n",
      "2025-08-21 10:17:01.506 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-08-21 10:17:01.510 INFO: Atomic Energies used (z: eV) for head Default: {3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\n",
      "2025-08-21 10:17:01.510 INFO: Atomic Energies used (z: eV) for head pt_head: {1: -3.667168021358939, 3: -3.482100566595956, 4: -4.736697230897597, 5: -7.724935420523256, 6: -8.405573550273285, 7: -7.360100452662763, 8: -7.28459863421322, 9: -4.896490881731322, 11: -2.7593613569762425, 12: -2.814047612069227, 13: -4.846881245288104, 14: -7.694793133351899, 15: -6.9632957911820235, 16: -4.672630400190884, 17: -2.8116892814008096, 19: -2.6176454856894793, 20: -5.390461060484104, 21: -7.8857952163517675, 22: -10.268392986214433, 23: -8.665147785496703, 24: -9.233050763772013, 25: -8.304951520770791, 26: -7.0489865771593765, 27: -5.577439766222147, 28: -5.172747618813715, 29: -3.2520726958619472, 30: -1.2901611618726314, 31: -3.527082192997912, 32: -4.70845955030298, 33: -3.9765109025623238, 34: -3.886231055836541, 35: -2.5184940099633986, 36: 6.766947645687137, 37: -2.5634958965928316, 38: -4.938005211501922, 39: -10.149818838085771, 40: -11.846857579882572, 41: -12.138896361658485, 42: -8.791678800595722, 43: -8.78694939675911, 44: -7.78093221529871, 45: -6.850021409115055, 46: -4.891019073240479, 47: -2.0634296773864045, 48: -0.6395695518943755, 49: -2.7887442084286693, 50: -3.818604275441892, 51: -3.587068329278862, 52: -2.8804045971118897, 53: -1.6355986842433357, 54: 9.846723842807721, 55: -2.765284507132287, 56: -4.990956432167774, 57: -8.933684809576345, 58: -8.735591176647514, 59: -8.018966025544966, 60: -8.251491970213372, 61: -7.591719594359237, 62: -8.169659881166858, 63: -13.592664636171698, 64: -18.517523458456985, 65: -7.647396572993602, 66: -8.122981037851925, 67: -7.607787319678067, 68: -6.85029094445494, 69: -7.8268821327130365, 70: -3.584786591677161, 71: -7.455406192077973, 72: -12.796283502572146, 73: -14.108127281277586, 74: -9.354916969477486, 75: -11.387537567890853, 76: -9.621909492152557, 77: -7.324393429417677, 78: -5.3046964808341945, 79: -2.380092582080244, 80: 0.24948924158195362, 81: -2.3239789120665026, 82: -3.730042357127322, 83: -3.438792347649683, 89: -5.062878214511315, 90: -11.02462566385297, 91: -12.265613551943261, 92: -13.855648206100362, 93: -14.933092020258243, 94: -15.282826131998245}\n",
      "2025-08-21 10:17:01.510 INFO: Processing datasets for head 'Default'\n",
      "2025-08-21 10:17:21.318 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-08-21 10:17:30.254 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-08-21 10:17:30.254 INFO: Combined validation datasets for Default\n",
      "2025-08-21 10:17:30.254 INFO: Head 'Default' training dataset size: 1612\n",
      "2025-08-21 10:17:30.254 INFO: Processing datasets for head 'pt_head'\n",
      "2025-08-21 10:17:38.565 INFO: Combining 1 list datasets for head 'pt_head'\n",
      "2025-08-21 10:17:39.480 INFO: Head 'pt_head' training dataset size: 9000\n",
      "2025-08-21 10:17:39.480 INFO: Average number of neighbors: 61.964672446250916\n",
      "2025-08-21 10:17:39.480 INFO: During training the following quantities will be reported: energy, forces, stress\n",
      "2025-08-21 10:17:39.480 INFO: ===========MODEL DETAILS===========\n",
      "2025-08-21 10:17:46.266 INFO: Loading FOUNDATION model\n",
      "2025-08-21 10:17:46.267 INFO: Using filtered elements: [1, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91, 92, 93, 94]\n",
      "2025-08-21 10:17:46.268 INFO: Model configuration extracted from foundation model\n",
      "2025-08-21 10:17:46.268 INFO: Using universal loss function for fine-tuning\n",
      "2025-08-21 10:17:46.268 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-08-21 10:17:46.268 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-08-21 10:17:46.268 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-08-21 10:17:46.268 INFO: Distance transform for radial basis functions: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/mace/tools/checkpoint.py:187: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-21 10:17:47.714 INFO: Total number of parameters: 5556810\n",
      "2025-08-21 10:17:47.714 INFO: \n",
      "2025-08-21 10:17:47.714 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-08-21 10:17:47.714 INFO: Using ADAM as parameter optimizer\n",
      "2025-08-21 10:17:47.714 INFO: Batch size: 2\n",
      "2025-08-21 10:17:47.714 INFO: Using Exponential Moving Average with decay: 0.99999\n",
      "2025-08-21 10:17:47.714 INFO: Number of gradient updates: 689780\n",
      "2025-08-21 10:17:47.714 INFO: Learning rate: 0.0001, weight decay: 1e-08\n",
      "2025-08-21 10:17:47.714 INFO: UniversalLoss(energy_weight=10.000, forces_weight=0.000, stress_weight=0.000)\n",
      "2025-08-21 10:17:47.725 WARNING: No SWA checkpoint found, while SWA is enabled. Compare the swa_start parameter and the latest checkpoint.\n",
      "2025-08-21 10:17:47.726 INFO: Loading checkpoint: ./checkpoints/mace_T2_including_replay_w2_run-84_epoch-59.pt\n",
      "2025-08-21 10:17:47.817 INFO: Using gradient clipping with tolerance=1.000\n",
      "2025-08-21 10:17:47.817 INFO: \n",
      "2025-08-21 10:17:47.817 INFO: ===========TRAINING===========\n",
      "2025-08-21 10:17:47.817 INFO: Started training, reporting errors on validation set\n",
      "2025-08-21 10:17:47.817 INFO: Loss metrics on validation set\n"
     ]
    }
   ],
   "source": [
    "# Now universal MACE finetuning on T2\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    cmd = [\n",
    "        \"mace_run_train\",\n",
    "        # === General settings ===\n",
    "        \"--name\",              \"mace_T2_including_replay_w2\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "    # --- MP replay (pretraining head) ---\n",
    "        \"--pt_train_file\",\"/home/phanim/harshitrawat/summer/replay_data/mp_finetuning-mace_T2_mp_replay_run-42.xyz\",              # <- MP replay shortcut\n",
    "        \"--atomic_numbers\",\"[3,8,40,57]\",    # Li, O, Zr, La\n",
    "        \"--multiheads_finetuning\",\"True\",\n",
    "\n",
    "        \"--train_file\",\"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T3_chgnet_labeled.extxyz\",\n",
    "        \"--valid_file\",\"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_chgnet_labeled.extxyz\",\n",
    "\n",
    "        \"--batch_size\",        \"2\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "\n",
    "        \"--device\",            \"cuda\",\n",
    "\n",
    "        # === Loss function weights ===\n",
    "        \"--forces_weight\",     \"0\",         # Increased force weight to balance energy better\n",
    "        \"--energy_weight\",     \"10\",   \n",
    "        \"--stress_weight\", \"0\",             # Reduced from 100 → avoid dominance + stabilize energy RMSE\n",
    "\n",
    "        # === Learning setup ===\n",
    "        \"--lr\",                \"0.006\",      # Explicit learning rate (0.0001 is too low → stagnation)\n",
    "        \"--scheduler_patience\",\"4\",          # Reduce LR if val loss doesn’t improve in 3 epochs\n",
    "        \"--clip_grad\",         \"1\",        # Avoid exploding gradients — essential when energy_weight is high\n",
    "        \"--weight_decay\",      \"1e-8\",       # Mild regularization to prevent overfitting\n",
    "\n",
    "        # === EMA helps smooth loss curve ===\n",
    "        #\"--ema_decay\",         \"0.999\",     # Smooths validation loss and helps final convergence\n",
    "\n",
    "        # === Domain + training settings ===\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"130\",\n",
    "        \"--E0s\",               \"{3: -1.2302615750354944, 8: -23.049110738413006, 40: 23.367646191010394, 57: 15.192898072498549}\",    # Still allowed — could optionally be replaced by manual E0s\n",
    "        \"--seed\",              \"84\",\n",
    "        \"--patience\",     \"8\",\n",
    "\n",
    "        \"--restart_latest\",                   # Resumes from checkpoint if available\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda62619-66cf-4057-a0bd-f54f8d36312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Keys renamed to REF_* and saved to:\n",
      "/home/phanim/harshitrawat/summer/replay_data/mp_finetuning-mace_T2_mp_replay_run-42_REFkeys.xyz\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd803b6-71b5-4fab-974f-2ea3ba19f232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Header updated:\n",
      "→ /home/phanim/harshitrawat/summer/replay_data/mp_finetuning-mace_T2_mp_replay_run-42_REFkeys.xyz\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa615d-2325-4419-80a0-3d680335b9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU mace_0.3.8)",
   "language": "python",
   "name": "mace_0.3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
