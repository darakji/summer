{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02747524-ebab-41ba-97d5-67a94ec53224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: torchrun \\\n",
      "    --standalone \\\n",
      "    --nnodes=1 \\\n",
      "    --nproc_per_node=1 \\\n",
      "    /home/phanim/harshitrawat/mace/mace/cli/run_train.py \\\n",
      "    --name \\\n",
      "    mace_it_2_T2_universal_loss_fcut200_avg \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_it_2_fcut_200.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/val_it2.extxyz \\\n",
      "    --batch_size \\\n",
      "    2 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --forces_weight \\\n",
      "    40 \\\n",
      "    --energy_weight \\\n",
      "    100 \\\n",
      "    --lr \\\n",
      "    0.01 \\\n",
      "    --scheduler_patience \\\n",
      "    4 \\\n",
      "    --clip_grad \\\n",
      "    2 \\\n",
      "    --weight_decay \\\n",
      "    1e-8 \\\n",
      "    --loss \\\n",
      "    universal \\\n",
      "    --ema_decay \\\n",
      "    0.999 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    130 \\\n",
      "    --E0s \\\n",
      "    average \\\n",
      "    --seed \\\n",
      "    42 \\\n",
      "    --patience \\\n",
      "    8 \\\n",
      "    --restart_latest\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-18 15:05:04.949 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-08-18 15:05:04.950 INFO: MACE version: 0.3.14\n",
      "2025-08-18 15:05:05.518 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py:146: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-18 15:05:05.990 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-08-18 15:05:05.991 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-08-18 15:05:05.991 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-08-18 15:05:05.991 INFO: Using heads: ['Default']\n",
      "2025-08-18 15:05:05.991 INFO: Using the key specifications to parse data:\n",
      "2025-08-18 15:05:05.991 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-18 15:05:05.991 INFO: =============    Processing head Default     ===========\n",
      "2025-08-18 15:05:11.622 INFO: Training set 1/1 [energy: 2773, stress: 0, virials: 0, dipole components: 0, head: 2773, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 2773, charges: 0]\n",
      "2025-08-18 15:05:11.629 INFO: Total Training set [energy: 2773, stress: 0, virials: 0, dipole components: 0, head: 2773, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 2773, charges: 0]\n",
      "2025-08-18 15:05:14.302 INFO: Validation set 1/1 [energy: 2505, stress: 0, virials: 0, dipole components: 0, head: 2505, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 2505, charges: 0]\n",
      "2025-08-18 15:05:14.309 INFO: Total Validation set [energy: 2505, stress: 0, virials: 0, dipole components: 0, head: 2505, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 2505, charges: 0]\n",
      "2025-08-18 15:05:14.309 INFO: Total number of configurations: train=2773, valid=2505, tests=[],\n",
      "2025-08-18 15:05:14.409 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-08-18 15:05:14.409 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-08-18 15:05:14.409 INFO: Computing average Atomic Energies using least squares regression\n",
      "2025-08-18 15:05:14.424 INFO: Atomic Energies used (z: eV) for head Default: {3: -1.2785249224810222, 8: -11.223045987492995, 40: 5.193197603746068, 57: -19.225222662804924}\n",
      "2025-08-18 15:05:14.424 INFO: Processing datasets for head 'Default'\n",
      "2025-08-18 15:05:35.981 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-08-18 15:05:45.255 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-08-18 15:05:45.255 INFO: Combined validation datasets for Default\n",
      "2025-08-18 15:05:45.255 INFO: Head 'Default' training dataset size: 2773\n",
      "2025-08-18 15:05:45.255 INFO: Computing average number of neighbors\n",
      "2025-08-18 15:05:48.101 INFO: Average number of neighbors: 67.32635519731062\n",
      "2025-08-18 15:05:48.102 INFO: During training the following quantities will be reported: energy, forces, stress\n",
      "2025-08-18 15:05:48.102 INFO: ===========MODEL DETAILS===========\n",
      "2025-08-18 15:05:50.294 INFO: Loading FOUNDATION model\n",
      "2025-08-18 15:05:50.295 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-08-18 15:05:50.295 INFO: Model configuration extracted from foundation model\n",
      "2025-08-18 15:05:50.295 INFO: Using universal loss function for fine-tuning\n",
      "2025-08-18 15:05:50.295 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-08-18 15:05:50.295 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-08-18 15:05:50.295 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-08-18 15:05:50.295 INFO: Distance transform for radial basis functions: None\n",
      "Using reduced CG: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using reduced CG: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-18 15:05:51.564 INFO: Total number of parameters: 894362\n",
      "2025-08-18 15:05:51.564 INFO: \n",
      "2025-08-18 15:05:51.564 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-08-18 15:05:51.564 INFO: Using ADAM as parameter optimizer\n",
      "2025-08-18 15:05:51.564 INFO: Batch size: 2\n",
      "2025-08-18 15:05:51.564 INFO: Number of gradient updates: 180245\n",
      "2025-08-18 15:05:51.564 INFO: Learning rate: 0.01, weight decay: 1e-08\n",
      "2025-08-18 15:05:51.564 INFO: UniversalLoss(energy_weight=100.000, forces_weight=40.000, stress_weight=1.000)\n",
      "2025-08-18 15:05:51.575 WARNING: Cannot find checkpoint with tag 'mace_it_2_T2_universal_loss_fcut200_avg_run-42' in './checkpoints'\n",
      "2025-08-18 15:05:51.575 INFO: Using gradient clipping with tolerance=2.000\n",
      "2025-08-18 15:05:51.575 INFO: \n",
      "2025-08-18 15:05:51.575 INFO: ===========TRAINING===========\n",
      "2025-08-18 15:05:51.575 INFO: Started training, reporting errors on validation set\n",
      "2025-08-18 15:05:51.575 INFO: Loss metrics on validation set\n"
     ]
    }
   ],
   "source": [
    "# Now universal MACE finetuning on T2\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    os.environ[\"PYTHONPATH\"] = \"/home/phanim/harshitrawat/mace/mace\"\n",
    "\n",
    "    cmd = [\n",
    "        \"torchrun\",\n",
    "        \"--standalone\", \n",
    "        \"--nnodes=1\", \n",
    "        \"--nproc_per_node=1\", \n",
    "        \"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\",\n",
    "        \"--name\",              \"mace_it_2_T2_universal_loss_fcut200_avg\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "\n",
    "        \"--train_file\",\"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_it_2_fcut_200.extxyz\",\n",
    "        \"--valid_file\",\"/home/phanim/harshitrawat/summer/T1_T2_T3_data/val_it2.extxyz\",\n",
    "\n",
    "\n",
    "        \"--batch_size\",        \"2\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "\n",
    "        \"--device\",            \"cuda\",\n",
    "\n",
    "        # === Loss function weights ===\n",
    "        \"--forces_weight\",     \"40\",         # Increased force weight to balance energy better\n",
    "        \"--energy_weight\",     \"100\",         # Reduced from 100 → avoid dominance + stabilize energy RMSE\n",
    "\n",
    "        # === Learning setup ===\n",
    "        \"--lr\",                \"0.01\",      # Explicit learning rate (0.0001 is too low → stagnation)\n",
    "        \"--scheduler_patience\",\"4\",          # Reduce LR if val loss doesn’t improve in 3 epochs\n",
    "        \"--clip_grad\",         \"2\",        # Avoid exploding gradients — essential when energy_weight is high\n",
    "        \"--weight_decay\",      \"1e-8\",       # Mild regularization to prevent overfitting\n",
    "        \"--loss\",                \"universal\",\n",
    "        # === EMA helps smooth loss curve ===\n",
    "        \"--ema_decay\",         \"0.999\",     # Smooths validation loss and helps final convergence\n",
    "\n",
    "        # === Domain + training settings ===\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"130\",\n",
    "        \"--E0s\",               \"average\",    # Still allowed — could optionally be replaced by manual E0s\n",
    "        \"--seed\",              \"42\",\n",
    "        \"--patience\",     \"8\",\n",
    "\n",
    "        \"--restart_latest\",                   # Resumes from checkpoint if available\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b99b41e6-27f4-4bed-8771-ef40e3633777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading frames from /home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_it_2.extxyz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering frames: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2812/2812 [00:00<00:00, 115926.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 2773 / 2812 frames with max force ≤ 200.0 eV/Å\n",
      "Filtered data written to /home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_it_2_fcut_200.extxyz\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from ase.io import read, write\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Config ===\n",
    "INPUT_FILE = \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_it_2.extxyz\"\n",
    "OUTPUT_FILE = \"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_it_2_fcut_200.extxyz\"\n",
    "FORCE_CUTOFF = 200.0  # eV/Å\n",
    "\n",
    "# === Load and filter ===\n",
    "print(f\"Reading frames from {INPUT_FILE}\")\n",
    "frames = read(INPUT_FILE, index=\":\")\n",
    "\n",
    "filtered_frames = []\n",
    "for atoms in tqdm(frames, desc=\"Filtering frames\"):\n",
    "    forces = atoms.arrays[\"REF_forces\"]\n",
    "    max_force = np.linalg.norm(forces, axis=1).max()\n",
    "    if max_force <= FORCE_CUTOFF:\n",
    "        filtered_frames.append(atoms)\n",
    "\n",
    "print(f\"Kept {len(filtered_frames)} / {len(frames)} frames with max force ≤ {FORCE_CUTOFF} eV/Å\")\n",
    "\n",
    "# === Write to file ===\n",
    "write(OUTPUT_FILE, filtered_frames)\n",
    "print(f\"Filtered data written to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a599c46-20be-401a-95c9-0a3c2a737bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: torchrun \\\n",
      "    --standalone \\\n",
      "    --nnodes=1 \\\n",
      "    --nproc_per_node=1 \\\n",
      "    /home/phanim/harshitrawat/mace/mace/cli/run_train.py \\\n",
      "    --name \\\n",
      "    mace_T3_finetune_h200_cn10 \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/T2_wo_binaries_isolated/T2_wo_bin_iso.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/val_it2.extxyz \\\n",
      "    --batch_size \\\n",
      "    2 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --forces_weight \\\n",
      "    40 \\\n",
      "    --energy_weight \\\n",
      "    10 \\\n",
      "    --lr \\\n",
      "    0.001 \\\n",
      "    --scheduler_patience \\\n",
      "    4 \\\n",
      "    --clip_grad \\\n",
      "    2 \\\n",
      "    --weight_decay \\\n",
      "    1e-8 \\\n",
      "    --ema_decay \\\n",
      "    0.999 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    130 \\\n",
      "    --E0s \\\n",
      "    {3: -1.9089228666666667, 8: -4.947961005 , 40: -8.54770063 , 57: -4.936007105} \\\n",
      "    --seed \\\n",
      "    42 \\\n",
      "    --patience \\\n",
      "    8 \\\n",
      "    --restart_latest\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-18 00:10:19.512 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-08-18 00:10:19.512 INFO: MACE version: 0.3.14\n",
      "2025-08-18 00:10:20.061 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py:146: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-18 00:10:20.585 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-08-18 00:10:20.586 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-08-18 00:10:20.586 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-08-18 00:10:20.586 INFO: Using heads: ['Default']\n",
      "2025-08-18 00:10:20.586 INFO: Using the key specifications to parse data:\n",
      "2025-08-18 00:10:20.586 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-18 00:10:20.586 INFO: =============    Processing head Default     ===========\n",
      "2025-08-18 00:10:23.702 INFO: Using isolated atom energies from training file\n",
      "2025-08-18 00:10:23.806 INFO: Training set 1/1 [energy: 855, stress: 0, virials: 0, dipole components: 0, head: 855, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 855, charges: 0]\n",
      "2025-08-18 00:10:23.808 INFO: Total Training set [energy: 855, stress: 0, virials: 0, dipole components: 0, head: 855, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 855, charges: 0]\n",
      "2025-08-18 00:10:26.470 INFO: Validation set 1/1 [energy: 2505, stress: 0, virials: 0, dipole components: 0, head: 2505, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 2505, charges: 0]\n",
      "2025-08-18 00:10:26.476 INFO: Total Validation set [energy: 2505, stress: 0, virials: 0, dipole components: 0, head: 2505, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 2505, charges: 0]\n",
      "2025-08-18 00:10:26.477 INFO: Total number of configurations: train=855, valid=2505, tests=[],\n",
      "2025-08-18 00:10:26.546 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-08-18 00:10:26.546 INFO: Atomic Energies used (z: eV) for head Default: {3: -1.9089228666666667, 8: -4.947961005, 40: -8.54770063, 57: -4.936007105}\n",
      "2025-08-18 00:10:26.546 INFO: Processing datasets for head 'Default'\n",
      "2025-08-18 00:10:37.024 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-08-18 00:10:46.297 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-08-18 00:10:46.297 INFO: Combined validation datasets for Default\n",
      "2025-08-18 00:10:46.297 INFO: Head 'Default' training dataset size: 855\n",
      "2025-08-18 00:10:46.298 INFO: Computing average number of neighbors\n",
      "2025-08-18 00:10:47.378 INFO: Average number of neighbors: 67.84491909346121\n",
      "2025-08-18 00:10:47.378 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-08-18 00:10:47.378 INFO: ===========MODEL DETAILS===========\n",
      "2025-08-18 00:10:48.290 INFO: Loading FOUNDATION model\n",
      "2025-08-18 00:10:48.290 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-08-18 00:10:48.291 INFO: Model configuration extracted from foundation model\n",
      "2025-08-18 00:10:48.291 INFO: Using weighted loss function for fine-tuning\n",
      "2025-08-18 00:10:48.291 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-08-18 00:10:48.291 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-08-18 00:10:48.291 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-08-18 00:10:48.291 INFO: Distance transform for radial basis functions: None\n",
      "Using reduced CG: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using reduced CG: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/mace/mace/tools/checkpoint.py:187: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-18 00:10:49.641 INFO: Total number of parameters: 894362\n",
      "2025-08-18 00:10:49.641 INFO: \n",
      "2025-08-18 00:10:49.641 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-08-18 00:10:49.641 INFO: Using ADAM as parameter optimizer\n",
      "2025-08-18 00:10:49.641 INFO: Batch size: 2\n",
      "2025-08-18 00:10:49.641 INFO: Number of gradient updates: 55575\n",
      "2025-08-18 00:10:49.641 INFO: Learning rate: 0.001, weight decay: 1e-08\n",
      "2025-08-18 00:10:49.641 INFO: WeightedEnergyForcesLoss(energy_weight=10.000, forces_weight=40.000)\n",
      "2025-08-18 00:10:49.650 WARNING: No SWA checkpoint found, while SWA is enabled. Compare the swa_start parameter and the latest checkpoint.\n",
      "2025-08-18 00:10:49.651 INFO: Loading checkpoint: ./checkpoints/mace_T3_finetune_h200_cn10_run-42_epoch-0.pt\n",
      "2025-08-18 00:10:49.685 INFO: Using gradient clipping with tolerance=2.000\n",
      "2025-08-18 00:10:49.685 INFO: \n",
      "2025-08-18 00:10:49.685 INFO: ===========TRAINING===========\n",
      "2025-08-18 00:10:49.685 INFO: Started training, reporting errors on validation set\n",
      "2025-08-18 00:10:49.685 INFO: Loss metrics on validation set\n",
      "2025-08-18 00:13:58.331 INFO: Initial: head: Default, loss=425096977301510.43750000, RMSE_E_per_atom=171198627.24 meV, RMSE_F=840482452.54 meV / A\n",
      "2025-08-18 00:21:09.725 INFO: Epoch 0: head: Default, loss=709584618009209.87500000, RMSE_E_per_atom=221546298.50 meV, RMSE_F=1083243736.35 meV / A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 00:22:33.849000 355918 site-packages/torch/distributed/elastic/agent/server/api.py:719] Received 2 death signal, shutting down workers\n",
      "W0818 00:22:33.852000 355918 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 355933 closing signal SIGINT\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m767\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mtools.train\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mmodel=model,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "    ...<23 lines>...\n",
      "        \u001b[1;31mrank=rank,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m222\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "    \u001b[31mtrain_one_epoch\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mmodel=model,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "    ...<11 lines>...\n",
      "        \u001b[1;31mrank=rank,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m370\u001b[0m, in \u001b[35mtrain_one_epoch\u001b[0m\n",
      "    _, opt_metrics = \u001b[31mtake_step\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                     \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mmodel=model_to_train,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<6 lines>...\n",
      "        \u001b[1;31mdevice=device,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m416\u001b[0m, in \u001b[35mtake_step\u001b[0m\n",
      "    loss = closure()\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m409\u001b[0m, in \u001b[35mclosure\u001b[0m\n",
      "    loss = loss_fn(pred=output, ref=batch)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m1751\u001b[0m, in \u001b[35m_wrapped_call_impl\u001b[0m\n",
      "    return \u001b[31mself._call_impl\u001b[0m\u001b[1;31m(*args, **kwargs)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py\"\u001b[0m, line \u001b[35m1762\u001b[0m, in \u001b[35m_call_impl\u001b[0m\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/modules/loss.py\"\u001b[0m, line \u001b[35m241\u001b[0m, in \u001b[35mforward\u001b[0m\n",
      "    loss_forces = mean_squared_error_forces(ref, pred, ddp)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/modules/loss.py\"\u001b[0m, line \u001b[35m124\u001b[0m, in \u001b[35mmean_squared_error_forces\u001b[0m\n",
      "    configs_weight = \u001b[31mtorch.repeat_interleave\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                     \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mref.weight, ref.ptr[1:] - ref.ptr[:-1]\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m.unsqueeze(-1)\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "\u001b[1;35mKeyboardInterrupt\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     57\u001b[39m     subprocess.run(cmd, check=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m cmd = [\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtorchrun\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m--standalone\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m--restart_latest\u001b[39m\u001b[33m\"\u001b[39m,                   \u001b[38;5;66;03m# Resumes from checkpoint if available\u001b[39;00m\n\u001b[32m     54\u001b[39m ]\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m.join(cmd), file=sys.stderr)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:558\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    560\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:1213\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1211\u001b[39m         stderr = \u001b[38;5;28mself\u001b[39m.stderr.read()\n\u001b[32m   1212\u001b[39m         \u001b[38;5;28mself\u001b[39m.stderr.close()\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1215\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:1276\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1274\u001b[39m     endtime = _time() + timeout\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1278\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1279\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1280\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:2068\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.returncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2067\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2068\u001b[39m (pid, sts) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[32m   2071\u001b[39m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[32m   2072\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pid == \u001b[38;5;28mself\u001b[39m.pid:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:2026\u001b[39m, in \u001b[36mPopen._try_wait\u001b[39m\u001b[34m(self, wait_flags)\u001b[39m\n\u001b[32m   2024\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[32m   2025\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2026\u001b[39m     (pid, sts) = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[32m   2028\u001b[39m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[32m   2029\u001b[39m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[32m   2030\u001b[39m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[32m   2031\u001b[39m     pid = \u001b[38;5;28mself\u001b[39m.pid\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Now universal MACE finetuning on T2\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    os.environ[\"PYTHONPATH\"] = \"/home/phanim/harshitrawat/mace/mace\"\n",
    "\n",
    "    cmd = [\n",
    "        \"torchrun\",\n",
    "        \"--standalone\", \n",
    "        \"--nnodes=1\", \n",
    "        \"--nproc_per_node=1\", \n",
    "        \"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\",\n",
    "        \"--name\",              \"mace_T3_finetune_h200_cn10\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "\n",
    "        \"--train_file\",\"/home/phanim/harshitrawat/summer/T2_wo_binaries_isolated/T2_wo_bin_iso.extxyz\",\n",
    "        \"--valid_file\",\"/home/phanim/harshitrawat/summer/T1_T2_T3_data/val_it2.extxyz\",\n",
    "\n",
    "\n",
    "        \"--batch_size\",        \"2\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "\n",
    "        \"--device\",            \"cuda\",\n",
    "\n",
    "        # === Loss function weights ===\n",
    "        \"--forces_weight\",     \"40\",         # Increased force weight to balance energy better\n",
    "        \"--energy_weight\",     \"10\",         # Reduced from 100 → avoid dominance + stabilize energy RMSE\n",
    "\n",
    "        # === Learning setup ===\n",
    "        \"--lr\",                \"0.001\",      # Explicit learning rate (0.0001 is too low → stagnation)\n",
    "        \"--scheduler_patience\",\"4\",          # Reduce LR if val loss doesn’t improve in 3 epochs\n",
    "        \"--clip_grad\",         \"2\",        # Avoid exploding gradients — essential when energy_weight is high\n",
    "        \"--weight_decay\",      \"1e-8\",       # Mild regularization to prevent overfitting\n",
    "\n",
    "        # === EMA helps smooth loss curve ===\n",
    "        \"--ema_decay\",         \"0.999\",     # Smooths validation loss and helps final convergence\n",
    "\n",
    "        # === Domain + training settings ===\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"130\",\n",
    "        \"--E0s\",               \"{3: -1.9089228666666667, 8: -4.947961005 , 40: -8.54770063 , 57: -4.936007105}\",    # Still allowed — could optionally be replaced by manual E0s\n",
    "        \"--seed\",              \"42\",\n",
    "        \"--patience\",     \"8\",\n",
    "\n",
    "        \"--restart_latest\",                   # Resumes from checkpoint if available\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6adb43f9-25b1-4560-94b4-9d3dbe80aa68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running: torchrun \\\n",
      "    --standalone \\\n",
      "    --nnodes=1 \\\n",
      "    --nproc_per_node=1 \\\n",
      "    /home/phanim/harshitrawat/mace/mace/cli/run_train.py \\\n",
      "    --name \\\n",
      "    mace_iteration_2_T2 \\\n",
      "    --model \\\n",
      "    MACE \\\n",
      "    --num_interactions \\\n",
      "    2 \\\n",
      "    --foundation_model \\\n",
      "    /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model \\\n",
      "    --foundation_model_readout \\\n",
      "    --train_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_it_2.extxyz \\\n",
      "    --valid_file \\\n",
      "    /home/phanim/harshitrawat/summer/T1_T2_T3_data/val_it2.extxyz \\\n",
      "    --batch_size \\\n",
      "    2 \\\n",
      "    --valid_batch_size \\\n",
      "    1 \\\n",
      "    --valid_fraction \\\n",
      "    0.1 \\\n",
      "    --device \\\n",
      "    cuda \\\n",
      "    --forces_weight \\\n",
      "    40 \\\n",
      "    --energy_weight \\\n",
      "    100 \\\n",
      "    --lr \\\n",
      "    0.005 \\\n",
      "    --scheduler_patience \\\n",
      "    4 \\\n",
      "    --clip_grad \\\n",
      "    10 \\\n",
      "    --weight_decay \\\n",
      "    1e-8 \\\n",
      "    --ema_decay \\\n",
      "    0.999 \\\n",
      "    --r_max \\\n",
      "    5.0 \\\n",
      "    --max_num_epochs \\\n",
      "    130 \\\n",
      "    --E0s \\\n",
      "    average \\\n",
      "    --seed \\\n",
      "    42 \\\n",
      "    --patience \\\n",
      "    8 \\\n",
      "    --restart_latest\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-15 23:56:00.517 INFO: ===========VERIFYING SETTINGS===========\n",
      "2025-08-15 23:56:00.517 INFO: MACE version: 0.3.14\n",
      "2025-08-15 23:56:01.061 INFO: CUDA version: 12.6, CUDA device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/mace/mace/cli/run_train.py:146: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  model_foundation = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-15 23:56:01.677 INFO: Using foundation model /home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model as initial checkpoint.\n",
      "2025-08-15 23:56:01.679 WARNING: Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\n",
      "2025-08-15 23:56:01.679 INFO: ===========LOADING INPUT DATA===========\n",
      "2025-08-15 23:56:01.679 INFO: Using heads: ['Default']\n",
      "2025-08-15 23:56:01.679 INFO: Using the key specifications to parse data:\n",
      "2025-08-15 23:56:01.679 INFO: Default: KeySpecification(info_keys={'energy': 'REF_energy', 'stress': 'REF_stress', 'virials': 'REF_virials', 'dipole': 'dipole', 'head': 'head', 'elec_temp': 'elec_temp', 'total_charge': 'total_charge', 'total_spin': 'total_spin'}, arrays_keys={'forces': 'REF_forces', 'charges': 'REF_charges'})\n",
      "2025-08-15 23:56:01.679 INFO: =============    Processing head Default     ===========\n",
      "2025-08-15 23:56:07.316 INFO: Training set 1/1 [energy: 2812, stress: 0, virials: 0, dipole components: 0, head: 2812, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 2812, charges: 0]\n",
      "2025-08-15 23:56:07.324 INFO: Total Training set [energy: 2812, stress: 0, virials: 0, dipole components: 0, head: 2812, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 2812, charges: 0]\n",
      "2025-08-15 23:56:09.932 INFO: Validation set 1/1 [energy: 2505, stress: 0, virials: 0, dipole components: 0, head: 2505, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 2505, charges: 0]\n",
      "2025-08-15 23:56:09.939 INFO: Total Validation set [energy: 2505, stress: 0, virials: 0, dipole components: 0, head: 2505, elec_temp: 0, total_charge: 0, total_spin: 0, forces: 2505, charges: 0]\n",
      "2025-08-15 23:56:09.939 INFO: Total number of configurations: train=2812, valid=2505, tests=[],\n",
      "2025-08-15 23:56:10.037 INFO: Atomic Numbers used: [3, 8, 40, 57]\n",
      "2025-08-15 23:56:10.037 INFO: Isolated Atomic Energies (E0s) not in training file, using command line argument\n",
      "2025-08-15 23:56:10.037 INFO: Computing average Atomic Energies using least squares regression\n",
      "2025-08-15 23:56:10.052 INFO: Atomic Energies used (z: eV) for head Default: {3: -1.2795325543974942, 8: -11.037578373588586, 40: 5.191448526830094, 57: -19.9359375070913}\n",
      "2025-08-15 23:56:10.052 INFO: Processing datasets for head 'Default'\n",
      "2025-08-15 23:56:30.109 INFO: Combining 1 list datasets for head 'Default'\n",
      "2025-08-15 23:56:39.369 INFO: Combining 1 list datasets for head 'Default_valid'\n",
      "2025-08-15 23:56:39.369 INFO: Combined validation datasets for Default\n",
      "2025-08-15 23:56:39.369 INFO: Head 'Default' training dataset size: 2812\n",
      "2025-08-15 23:56:39.370 INFO: Computing average number of neighbors\n",
      "2025-08-15 23:56:42.201 INFO: Average number of neighbors: 67.31953374096544\n",
      "2025-08-15 23:56:42.201 INFO: During training the following quantities will be reported: energy, forces\n",
      "2025-08-15 23:56:42.201 INFO: ===========MODEL DETAILS===========\n",
      "2025-08-15 23:56:44.390 INFO: Loading FOUNDATION model\n",
      "2025-08-15 23:56:44.391 INFO: Using filtered elements: [3, 8, 40, 57]\n",
      "2025-08-15 23:56:44.391 INFO: Model configuration extracted from foundation model\n",
      "2025-08-15 23:56:44.391 INFO: Using weighted loss function for fine-tuning\n",
      "2025-08-15 23:56:44.391 INFO: Message passing with hidden irreps 128x0e+128x1o+128x2e)\n",
      "2025-08-15 23:56:44.391 INFO: 2 layers, each with correlation order: 3 (body order: 4) and spherical harmonics up to: l=3\n",
      "2025-08-15 23:56:44.391 INFO: Radial cutoff: 6.0 A (total receptive field for each atom: 12.0 A)\n",
      "2025-08-15 23:56:44.391 INFO: Distance transform for radial basis functions: None\n",
      "Using reduced CG: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using reduced CG: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/phanim/harshitrawat/mace/mace/tools/checkpoint.py:187: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-15 23:56:45.620 INFO: Total number of parameters: 894362\n",
      "2025-08-15 23:56:45.620 INFO: \n",
      "2025-08-15 23:56:45.620 INFO: ===========OPTIMIZER INFORMATION===========\n",
      "2025-08-15 23:56:45.620 INFO: Using ADAM as parameter optimizer\n",
      "2025-08-15 23:56:45.620 INFO: Batch size: 2\n",
      "2025-08-15 23:56:45.620 INFO: Number of gradient updates: 182780\n",
      "2025-08-15 23:56:45.620 INFO: Learning rate: 0.005, weight decay: 1e-08\n",
      "2025-08-15 23:56:45.620 INFO: WeightedEnergyForcesLoss(energy_weight=100.000, forces_weight=40.000)\n",
      "2025-08-15 23:56:45.652 WARNING: No SWA checkpoint found, while SWA is enabled. Compare the swa_start parameter and the latest checkpoint.\n",
      "2025-08-15 23:56:45.653 INFO: Loading checkpoint: ./checkpoints/mace_iteration_2_T2_run-42_epoch-21.pt\n",
      "2025-08-15 23:56:45.687 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-08-15 23:56:45.688 INFO: \n",
      "2025-08-15 23:56:45.688 INFO: ===========TRAINING===========\n",
      "2025-08-15 23:56:45.688 INFO: Started training, reporting errors on validation set\n",
      "2025-08-15 23:56:45.688 INFO: Loss metrics on validation set\n",
      "2025-08-15 23:59:55.084 INFO: Initial: head: Default, loss=5100.22512230, RMSE_E_per_atom=  540.85 meV, RMSE_F= 2268.14 meV / A\n",
      "2025-08-16 00:11:02.802 INFO: Epoch 21: head: Default, loss=5386.33421397, RMSE_E_per_atom=  713.12 meV, RMSE_F= 2233.41 meV / A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0816 00:15:59.898000 952576 site-packages/torch/distributed/elastic/agent/server/api.py:719] Received 2 death signal, shutting down workers\n",
      "W0816 00:15:59.899000 952576 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 952732 closing signal SIGINT\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m996\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m77\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\"\u001b[0m, line \u001b[35m767\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mtools.train\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mmodel=model,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "    ...<23 lines>...\n",
      "        \u001b[1;31mrank=rank,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m222\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "    \u001b[31mtrain_one_epoch\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mmodel=model,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "    ...<11 lines>...\n",
      "        \u001b[1;31mrank=rank,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m370\u001b[0m, in \u001b[35mtrain_one_epoch\u001b[0m\n",
      "    _, opt_metrics = \u001b[31mtake_step\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                     \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mmodel=model_to_train,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<6 lines>...\n",
      "        \u001b[1;31mdevice=device,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/mace/mace/tools/train.py\"\u001b[0m, line \u001b[35m417\u001b[0m, in \u001b[35mtake_step\u001b[0m\n",
      "    \u001b[31moptimizer.step\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/optim/optimizer.py\"\u001b[0m, line \u001b[35m485\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    out = func(*args, **kwargs)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/optim/optimizer.py\"\u001b[0m, line \u001b[35m79\u001b[0m, in \u001b[35m_use_grad\u001b[0m\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/optim/adam.py\"\u001b[0m, line \u001b[35m246\u001b[0m, in \u001b[35mstep\u001b[0m\n",
      "    \u001b[31madam\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mparams_with_grad,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<19 lines>...\n",
      "        \u001b[1;31mdecoupled_weight_decay=group[\"decoupled_weight_decay\"],\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/optim/optimizer.py\"\u001b[0m, line \u001b[35m147\u001b[0m, in \u001b[35mmaybe_fallback\u001b[0m\n",
      "    return func(*args, **kwargs)\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/optim/adam.py\"\u001b[0m, line \u001b[35m933\u001b[0m, in \u001b[35madam\u001b[0m\n",
      "    \u001b[31mfunc\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mparams,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^\u001b[0m\n",
      "    ...<17 lines>...\n",
      "        \u001b[1;31mdecoupled_weight_decay=decoupled_weight_decay,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/optim/adam.py\"\u001b[0m, line \u001b[35m739\u001b[0m, in \u001b[35m_multi_tensor_adam\u001b[0m\n",
      "    1 - beta1 ** \u001b[31m_get_value\u001b[0m\u001b[1;31m(step)\u001b[0m for step in device_state_steps\n",
      "                 \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/phanim/harshitrawat/miniconda3/lib/python3.13/site-packages/torch/optim/optimizer.py\"\u001b[0m, line \u001b[35m94\u001b[0m, in \u001b[35m_get_value\u001b[0m\n",
      "    return \u001b[31mx.item\u001b[0m\u001b[1;31m()\u001b[0m if isinstance(x, torch.Tensor) else x\n",
      "           \u001b[31m~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mKeyboardInterrupt\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     58\u001b[39m     subprocess.run(cmd, check=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m cmd = [\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtorchrun\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m--standalone\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m--restart_latest\u001b[39m\u001b[33m\"\u001b[39m,                   \u001b[38;5;66;03m# Resumes from checkpoint if available\u001b[39;00m\n\u001b[32m     55\u001b[39m ]\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m.join(cmd), file=sys.stderr)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:558\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    560\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:1213\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1211\u001b[39m         stderr = \u001b[38;5;28mself\u001b[39m.stderr.read()\n\u001b[32m   1212\u001b[39m         \u001b[38;5;28mself\u001b[39m.stderr.close()\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1215\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:1276\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1274\u001b[39m     endtime = _time() + timeout\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1278\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1279\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1280\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:2068\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.returncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2067\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2068\u001b[39m (pid, sts) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[32m   2071\u001b[39m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[32m   2072\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pid == \u001b[38;5;28mself\u001b[39m.pid:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/subprocess.py:2026\u001b[39m, in \u001b[36mPopen._try_wait\u001b[39m\u001b[34m(self, wait_flags)\u001b[39m\n\u001b[32m   2024\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[32m   2025\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2026\u001b[39m     (pid, sts) = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[32m   2028\u001b[39m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[32m   2029\u001b[39m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[32m   2030\u001b[39m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[32m   2031\u001b[39m     pid = \u001b[38;5;28mself\u001b[39m.pid\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Now universal MACE finetuning on T2\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # ——— Environment setup ———\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    os.environ[\"PYTHONPATH\"] = \"/home/phanim/harshitrawat/mace/mace\"\n",
    "\n",
    "    cmd = [\n",
    "        \"torchrun\",\n",
    "        \"--standalone\", \n",
    "        \"--nnodes=1\", \n",
    "        \"--nproc_per_node=1\", \n",
    "        \"/home/phanim/harshitrawat/mace/mace/cli/run_train.py\",\n",
    "        \"--name\",              \"mace_iteration_2_T2\",\n",
    "        \"--model\",             \"MACE\",\n",
    "        \"--num_interactions\",  \"2\",\n",
    "        \"--foundation_model\",  \"/home/phanim/harshitrawat/summer/mace_models/universal/2024-01-07-mace-128-L2_epoch-199.model\",\n",
    "        \"--foundation_model_readout\",\n",
    "\n",
    "        \"--train_file\",\"/home/phanim/harshitrawat/summer/T1_T2_T3_data/T2_it_2.extxyz\",\n",
    "        \"--valid_file\",\"/home/phanim/harshitrawat/summer/T1_T2_T3_data/val_it2.extxyz\",\n",
    "\n",
    "\n",
    "        \"--batch_size\",        \"2\",\n",
    "        \"--valid_batch_size\",  \"1\",\n",
    "        \"--valid_fraction\",    \"0.1\",\n",
    "\n",
    "        \"--device\",            \"cuda\",\n",
    "\n",
    "        # === Loss function weights ===\n",
    "        \"--forces_weight\",     \"40\",         # Increased force weight to balance energy better\n",
    "        \"--energy_weight\",     \"100\",         # Reduced from 100 → avoid dominance + stabilize energy RMSE\n",
    "\n",
    "        # === Learning setup ===\n",
    "        \"--lr\",                \"0.005\",      # Explicit learning rate (0.0001 is too low → stagnation)\n",
    "        \"--scheduler_patience\",\"4\",          # Reduce LR if val loss doesn’t improve in 3 epochs\n",
    "        \"--clip_grad\",         \"10\",        # Avoid exploding gradients — essential when energy_weight is high\n",
    "        \"--weight_decay\",      \"1e-8\",       # Mild regularization to prevent overfitting\n",
    "\n",
    "        # === EMA helps smooth loss curve ===\n",
    "        \"--ema_decay\",         \"0.999\",     # Smooths validation loss and helps final convergence\n",
    "\n",
    "        # === Domain + training settings ===\n",
    "        \"--r_max\",             \"5.0\",\n",
    "        \"--max_num_epochs\",    \"130\",\n",
    "        \"--E0s\",               \"average\",    # Still allowed — could optionally be replaced by manual E0s\n",
    "        \"--seed\",              \"42\",\n",
    "        \"--patience\",     \"8\",\n",
    "\n",
    "        \"--restart_latest\",                   # Resumes from checkpoint if available\n",
    "    ]\n",
    "\n",
    "    print(\"Running:\", \" \\\\\\n    \".join(cmd), file=sys.stderr)\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252349a-161e-48db-b0dc-5dd23fe10e13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
